{
    "collab_server" : "",
    "contents" : "# Optimal\ncl.tabl = as.data.frame(\n  matrix(, nrow = , ncol = 15))\nclu.f = floor(sqrt(dim(wanted)[1]/2))\nfor (clu in clu.f:(clu.f+1)){\n  par(mar=c(2,2,2,2))\n  png(filename=paste0(dir,\"/plots/\",texty[pv],\n                      \"/optimal_f\",clu,\".png\"),\n      w = 800, h = 600)\n  max_clu = get(paste(\"maxsos\",clu,sep=\"\"))\n  colclu = match(c(\"lat\",max_clu[2,c(2:3)]),colnames(kdata.sca))\n  clusters = pam(kdata.sca[,colclu], clu)\n  \n  {\n    mappoint= cbind(dis_ang[as.numeric(rownames(kdata)),c(9,10)],\n                    clusters$clustering)\n    newmap <- getMap(resolution = \"low\")\n    # windows(800, 600, pointsize = 12)\n    plot(newmap, xlim = c(-80, -65), ylim = c(25, 45), asp = 1)\n    legend(-63,30,legend = c(1:clu,\"Discordant\"),\n           bg = \"white\",col=c(colors[1:clu],\"black\"), \n           bty = \"n\", pch = c(rep(20,clu),1), \n           pt.cex = c(rep(3,clu),6), \n           pt.lwd = c(rep(1,clu),3),\n           ncol = 1, cex = 1.5)\n    ll = 1\n    while(ll<length(colnames(clusters$medoids))+1){\n      if(ll==1){\n        text(-63,42.5-ll*1.5,\"Lat\",\n             cex = 2.5)\n      }\n      text(-63,42.5-ll*1.5,\n           capitalize(colnames(clusters$medoids)[ll]),\n           cex = 2.5)\n      ll = ll + 1\n    }\n    text(-63,35,\n         paste0(\"Si: \",summary(silhouette(clusters))$si.summary[4]),\n         cex = 2.5)\n    \n    cl = 1\n    if(clu!=clu.f){\n      dimc = dim(cl.tabl)[1]\n    }else{\n      dimc = 0\n    }\n    while(cl<length(unique(clusters$clustering))+1){\n      cl.index = which(as.numeric(clusters$clustering)==cl)\n      dm.table = cbind(kdata[cl.index,1],\n                       wanted[cl.index,14],\n                       cov[cl.index,3],\n                       lmom[cl.index,8:11])\n      colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n      dm.test = regtst(dm.table[,1:6],5000)\n      is.dm.ex = \n        length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n      n.good.fit = length(which(abs(dm.test$Z)<1.64))\n      \n      if(is.dm.ex>0){\n        dm.sta = \n          dm.table[which(as.numeric(dm.test$D)>dm.test$Dcrit[1]),1]\n        dm.point = cbind(dis_ang[match(dm.sta,dis_ang[,2]),c(9,10)],cl)\n        points(dm.point$LON,dm.point$LAT,\n               col=colors[cl], cex=6, pch = 1,\n               lwd = 3)\n      }\n      if(clu==clu.f){\n        cl.tabl[cl,1] = clu\n        cl.tabl[cl,2] = cl\n        cl.tabl[cl,3] = length(dm.test$D)\n        cl.tabl[cl,4] = \n          length(which(dm.test$D>dm.test$Dcrit[1]))\n        cl.tabl[cl,5] = dm.test$H[1]\n        if(n.good.fit!=0){\n          for(n.g in 1:n.good.fit){\n            cl.tabl[cl,14+(2*n.g)-1] = sort(abs(dm.test$Z))[n.g]\n            cl.tabl[cl,14+(2*n.g)] = attributes(sort(abs(dm.test$Z))[n.g])\n          }\n        }else{\n          cl.tabl[cl,15] = sort(abs(abs(dm.test$Z)-1.64))[1] + 1.64\n          cl.tabl[cl,16] = attributes(sort(abs(abs(dm.test$Z)-1.64))[1])\n        }\n      }else{\n        cl.tabl[dimc + cl,1] = clu\n        cl.tabl[dimc + cl,2] = cl\n        cl.tabl[dimc + cl,3] = length(dm.test$D)\n        cl.tabl[dimc + cl,4] = \n          length(which(dm.test$D>dm.test$Dcrit[1]))\n        cl.tabl[dimc + cl,5] = dm.test$H[1]\n        if(n.good.fit!=0){\n          for(n.g in 1:n.good.fit){\n            cl.tabl[dimc + cl,14+(2*n.g)-1] = sort(abs(dm.test$Z))[n.g]\n            cl.tabl[dimc + cl,14+(2*n.g)] = attributes(sort(abs(dm.test$Z))[n.g])\n          }\n        }else{\n          cl.tabl[dimc + cl,15] = sort(abs(abs(dm.test$Z)-1.64))[1] + 1.64\n          cl.tabl[dimc + cl,16] = attributes(sort(abs(abs(dm.test$Z)-1.64))[1])\n        }\n      }\n      f = 1\n      \n      ## Obtain desired data\n      file = paste0(\"station_matrix_\",\n                    kdata.sca[which(clusters$clustering==cl),1],\n                    \"_update.xlsx\")\n      if(is.dm.ex>0){\n        file_r = paste0(\"station_matrix_\",\n                        dm.sta,\n                        \"_update.xlsx\")\n        file = file[which(!(file %in% file_r))]\n      }\n      yearr = as.data.frame(\n        matrix(, nrow = , ncol = 2))\n      \n      ## Identify beginning and ending years\n      while(f<length(file)+1){\n        data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n        \n        \n        # Delete unnecessary rows until first data\n        data_gg = data_g[(grep(\"^Date\", \n                               data_g[,1])+1):length(data_g[,1]),]\n        \n        # assign data_gg column names as data_g's row 6 \n        # because it has column names aligned correctly\n        colnames(data_gg) <- data_g[6,]\n        \n        # Date/Time Format originally MM/DD/YYYY HH:MM\n        # Distorted to Excel Serial Date (start from 1900-01-01)\n        # numbers while loading. Hence, these are  \n        # converted to YYYY-MM-DD HH:MM format\n        data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                                 + as.POSIXct(\"1899-12-30 00:00\"))\n        \n        # Convert strings into numbers\n        data_gg[,2] = as.numeric(data_gg[,2])\n        data_gg[,3] = as.numeric(data_gg[,3])\n        \n        # Read storm type\n        # Read storm type\n        \n        if(pv==2){\n          # Non-thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n        }\n        if(pv==3){\n          # Thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n        }\n        if(pv==4){\n          # Tropical Storm\n          data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n        }\n        \n        # Obtain minimum and maximum years of each station\n        yearr[f,1] = min(unique(year(data_gg[,1])))\n        yearr[f,2] = max(unique(year(data_gg[,1])))\n        \n        f = f + 1\n      }\n      \n      super_data = as.data.frame(\n        matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n               ncol = 2*length(file)))\n      super_data[,seq(2,length(file)*2,2)] = \n        min(yearr[,1]):max(yearr[,2])\n      \n      f = 1\n      \n      # Place max accordingly\n      while(f<length(file)+1){\n        data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n        \n        # Delete unnecessary rows until first data\n        data_gg = data_g[(grep(\"^Date\", \n                               data_g[,1])+1):length(data_g[,1]),]\n        \n        # assign data_gg column names as data_g's row 6 \n        # because it has column names aligned correctly\n        colnames(data_gg) <- data_g[6,]\n        \n        # Date/Time Format originally MM/DD/YYYY HH:MM\n        # Distorted to Excel Serial Date (start from 1900-01-01)\n        # numbers while loading. Hence, these are  \n        # converted to YYYY-MM-DD HH:MM format\n        data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                                 + as.POSIXct(\"1899-12-30 00:00\"))\n        \n        # Convert strings into numbers\n        data_gg[,2] = as.numeric(data_gg[,2])\n        data_gg[,3] = as.numeric(data_gg[,3])\n        \n        # Read storm type\n        if(pv==2){\n          # Non-thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n        }\n        if(pv==3){\n          # Thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n        }\n        if(pv==4){\n          # Tropical Storm\n          data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n        }\n        \n        # Annual\n        {\n          i = 1\n          j = 1\n          k = 1\n          # no_YEARS: how many data points are there?\n          no_YEARS = length(unique(year(data_gg[,1])))\n          year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n          data_gg[,14] = year(data_gg[,1])\n          while(k<(no_YEARS)){\n            # Identify which data row is the last one\n            # from all of the year n\n            while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n              j = j + 1\n            }\n            j = i + j - 1\n            \n            if(m == 1){\n              # Mean\n              year[k] = sum(data_gg[i:j,3])/length(i:j)\n              type = \"Mean\"\n            }else if(m == 2){\n              # Median\n              year[k] = median(data_gg[i:j,3]) \n              type = \"Median\"\n            }else if(m == 3){\n              # Coefficient of Variation\n              year[k] = sd(data_gg[i:j,3])/\n                (sum(data_gg[i:j,3])/length(i:j)) \n              type = \"Coefficient of Variation\"\n            }else if(m == 4){\n              # Maximum\n              year_a[k,1] = max(data_gg[i:j,3])\n              year_a[k,2] = as.character(data_gg[i,1])\n              year_a[k,3] = year(data_gg[i,1])\n              type = \"Maximum\"\n            }else if(m == 5){\n              # Frequency\n              year[k] = length(i:j)\n              type = \"Frequency\"\n            }else{\n              # L-Moments\n              L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n                type = \"L-CV\"\n              }else if(n == 2){\n                year[k] = lsk\n                type = \"L-Skewness\"\n              }else{\n                year[k] = lku\n                type = \"L-Kurtosis\"\n              }\n            }\n            \n            # i variable is added by 1\n            # to move on to the next year\n            i = j + 1\n            j = 1\n            k = k + 1\n            if(k == (no_YEARS)){\n              last = length(data_gg[,3])\n              if(m == 1){\n                year[k] = sum(data_gg[i:last,3])/length(i:last)\n              }else if(m == 2){\n                year[k] = median(data_gg[i:last,3])\n              }else if(m == 3){\n                year[k] = sd(data_gg[i:last,3])/\n                  (sum(data_gg[i:last,3])/length(i:last)) \n              }else if(m == 4){\n                year_a[k,1] = max(data_gg[i:last,3])\n                year_a[k,2] = as.character(data_gg[last,1])\n                year_a[k,3] = year(data_gg[last,1])\n              }else if(m == 5){\n                year[k] = length(i:last)\n              }else{\n                L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n                lcv = L[2]/L[1] # L-cv\n                lsk = L[3]/L[2] # L-Skew\n                lku = L[4]/L[2] # L-Kurtosis\n                \n                l1 = fit$mle[1]/(1+fit$mle[2])\n                l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n                \n                t2[k,1] = l2/l1\n                t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n                \n                if(n == 1){\n                  year[k] = lcv\n                }else if(n == 2){\n                  year[k] = lsk\n                }else{\n                  year[k] = lku\n                }\n              }\n              k = k + 1\n            }\n          }\n          used_data = year_a[,1]\n          used_date = year_a[,2]\n        }\n        \n        # Obtain desired data\n        super_data[,2*(f-1)+1] = \n          ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n                 year_a[,1],NA)\n        \n        f = f + 1\n      }\n      \n      # Obtain max from every row, ignoring NA\n      cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n      vector.is.empty <- function(x) return(length(x) ==0 )\n      \n      dim_sup = dim(super_data[,seq(1,length(file)*2-1,2)])[2]\n      # Superstation values\n      if(vector.is.empty(dim_sup)){\n        used_data = super_data[,seq(1,length(file)*2-1,2)]\n        nas = which(is.na(used_data))\n        used_data = used_data[!is.na(used_data)]\n        yearr_t = min(yearr[,1]):max(yearr[,2])\n        yearr_t[nas] = NA\n        yearr_t = yearr_t[!is.na(yearr_t)]\n        \n        # Remove data points that are NA (not recorded)\n        # by indexing together\n        t = yearr_t - min(yearr_t)\n        t[which(is.na(used_data))] = NA\n        used_data <- used_data[!is.na(used_data)]\n        t <- t[!is.na(t)]\n      }else if(dim_sup>1){\n        used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n        yearr_t = min(yearr[,1]):max(yearr[,2])\n        \n        # Remove data points that are NA (not recorded)\n        # by indexing together\n        t = yearr_t - min(yearr_t)\n        t[which(is.na(used_data))] = NA\n        used_data <- used_data[!is.na(used_data)]\n        t <- t[!is.na(t)]\n      }\n      \n      used_data = log(used_data)\n      \n      # Significance\n      # Linear Regression Model\n      sta.lm = lm(used_data~t)\n      \n      # P-Values for Intercept and Trend Coefficients\n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[2,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      \n      # Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk = 2*(1-pnorm(abs(mk_z)))\n      \n      if(clu==clu.f){\n        cl.tabl[cl,6] = format(round(\n          summary(sta.lm)$coefficients[2,1], 5), nsmall = 5)\n        cl.tabl[cl,7] = format(round(\n          summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n        cl.tabl[cl,8] = format(round(alp,5), nsmall = 5)\n        cl.tabl[cl,9] = format(round(\n          pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n        cl.tabl[cl,10] = mk\n        cl.tabl[cl,12:14] = colnames(clusters$medoids)\n      }else{\n        cl.tabl[dimc + cl,6] = format(round(\n          summary(sta.lm)$coefficients[2,1], 5), nsmall = 5)\n        cl.tabl[dimc + cl,7] = format(round(\n          summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n        cl.tabl[dimc + cl,8] = format(round(alp,5), nsmall = 5)\n        cl.tabl[dimc + cl,9] = format(round(\n          pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n        cl.tabl[dimc + cl,10] = mk\n        cl.tabl[dimc + cl,12:14] = colnames(clusters$medoids)\n      }\n      \n      cl = cl + 1\n    }\n    \n    mm = 1\n    while(mm < max(clusters$clustering)+1){\n      mmm = which(clusters$clustering %in% mm)\n      points(mappoint$LON[mmm],mappoint$LAT[mmm],\n             col=colors[mm], cex=4, pch = 20)\n      mm = mm + 1\n    }\n    \n    # savePlot(\"clipboard\", type=\"wmf\")\n    dev.off()\n  }\n  \n  # Plotting\n  cl = 1\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    \n    file = paste0(\"station_matrix_\",\n                  kdata.sca[cl.index,1],\n                  \"_update.xlsx\")\n    \n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    n.good.fit = length(which(abs(dm.test$Z)<1.64))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    f = 1\n    ## Identify beginning and ending years\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      # Read storm type\n      \n      if(pv==1){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==2){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==3){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Obtain minimum and maximum years of each station\n      yearr[f,1] = min(unique(year(data_gg[,1])))\n      yearr[f,2] = max(unique(year(data_gg[,1])))\n      \n      f = f + 1\n    }\n    \n    super_data = as.data.frame(\n      matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n             ncol = 2*length(file)))\n    super_data[,seq(2,length(file)*2,2)] = \n      min(yearr[,1]):max(yearr[,2])\n    \n    f = 1\n    \n    # Place max accordingly\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,2]\n      }\n      \n      \n      # Obtain desired data\n      super_data[,2*(f-1)+1] = \n        ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n               year_a[,1],NA)\n      \n      f = f + 1\n    }\n    \n    # Obtain max from every row, ignoring NA\n    cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n    \n    if(dim(super_data)[2]<3){\n      super_data = super_data[which(!is.na(super_data[,1])),]\n      yearr_t = super_data[,2]\n      used_data = super_data[,1]\n    }else{\n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n    }\n    \n    # Remove data points that are NA (not recorded)\n    # by indexing together\n    t = yearr_t\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n    # Significance\n    # Linear Regression Model\n    sta.lm = lm(log(used_data)~t)\n    \n    # P-Values for Intercept and Trend Coefficients\n    t1 = sta.lm$coefficients[2]/\n      summary(sta.lm)$coefficients[2,2]\n    \n    alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n    \n    t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n      1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n    \n    # Log transformation and linear regression\n    ln_used_data = log(used_data)\n    lm.year = lm(ln_used_data~t)\n    m.year = lm(used_data~t)\n    sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t\n    rho = cor(ln_used_data,t)\n    \n    # Conditional Moments of y\n    mu_y.w = lm.year$coefficients[1] + \n      lm.year$coefficients[2]*t\n    vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)\n    sk_y.w = skewness(ln_used_data) - \n      skewness(t)*(lm.year$coefficients[2]^3)\n    \n    # Conditional Moments of x\n    mu_x.w = exp(mu_y.w + (vr_y.w)/2)\n    vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)\n    cv_x.w = sqrt(exp(vr_y.w)-1)\n    sk_x.w = 3*cv_x.w + cv_x.w^3\n    \n    # Stationary\n    mean.sta = mean(ln_used_data)\n    sd.sta = sqrt(sum((ln_used_data - \n                         mean(ln_used_data))^2)/\n                    length(ln_used_data))\n    \n    YR = c(10,20,30)\n    LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))\n    \n    {\n      # PPCC \n      b_coef = cor(ln_used_data, t)*sd(ln_used_data)/sd(t)\n      mu_y.pp = mean.sta + b_coef*(t-mean(t))\n      sd_y.pp = sqrt(var(ln_used_data)*(1-cor(ln_used_data, t)^2))\n      z_i.s = sort((ln_used_data - mean.sta)/sd.sta)\n      z_i.n = sort((ln_used_data - mu_y.pp)/sd_y.pp)\n      rank_i.s = rank(z_i.s)\n      rank_i.n = rank(z_i.n)\n      p_pos.s = (rank_i.s - 0.375)/(length(ln_used_data) + 0.25)\n      p_pos.n = (rank_i.n - 0.375)/(length(ln_used_data) + 0.25)\n      \n      z_ii.s = qnorm(p_pos.s)\n      z_ii.n = qnorm(p_pos.n)\n      if(cl==1){\n        if(clu==2){\n          png(filename=paste0(dir,\"/plots/\",texty[pv],\n                              \"/pp_opt\",clu,\".png\"),\n              w = 800, h = 400)\n          par(mar=c(8,4.5,2,2))\n          par(mfrow=c(1,2))\n          txt = 1\n          txt2 = 1\n        }else if(clu==3){\n          png(filename=paste0(dir,\"/plots/\",texty[pv],\n                              \"/pp_opt\",clu,\".png\"),\n              w = 900, h = 300)\n          par(mar=c(8,4.5,2,2))\n          par(mfrow=c(1,3))\n          txt = 1.5\n          txt2 = txt\n        }else if(clu==4){\n          png(filename=paste0(dir,\"/plots/\",texty[pv],\n                              \"/pp_opt\",clu,\".png\"),\n              w = 800, h = 800)\n          par(mar=c(8,4.5,2,2))\n          par(mfrow=c(2,2))\n          txt = 1.5\n          txt2 = txt\n        }\n      }\n      plot(z_ii.s,z_i.s, xlab = \"Normal Quantiles\",\n           ylab = \"Ordered Standardized Observations\",\n           type = \"n\", tck = 0.02,\n           xlim = c(-3,3), ylim = c(-3,3),\n           cex.lab = txt, xaxt= \"n\", yaxt =\"n\")\n      axis(1, cex.axis = txt)\n      axis(2, cex.axis = txt)\n      \n      points(z_ii.n,z_i.n, pch = 21, col = \"gray\",\n             bg = \"gray\", cex = 2)\n      points(z_ii.s,z_i.s, pch = 20)\n      abline(0,1, lwd = 1)\n      \n      corst = round(cor(z_ii.s,z_i.s),4)\n      corns = round(cor(z_ii.n,z_i.n),4)\n      \n      leg_s = bquote(PPCC[s] == .(corst))\n      leg_n = bquote(PPCC[ns] == .(corns))\n      text(-3,2.75,leg_s, pos = 4,\n           bty = \"n\",\n           cex = txt2)\n      text(-3,2.25,leg_n, pos = 4,\n           bty = \"n\",\n           cex = txt2)\n      text(3.25,-2.25, \n           paste0(clu,\"-Means\"), pos = 2,\n           cex = txt2)\n      text(3.25,-1.75, \n           text.storm, pos = 2,\n           cex = txt2)\n      text(3.25,-2.25, \n           paste0(clu,\"-Means\"), pos = 2,\n           cex = txt2)\n      text(3.25,-2.75, \n           paste(\"Cluster\",cl), pos = 2,\n           cex = txt2)\n      if(cl==length(unique(clusters$clustering))){\n        add_legend(\"bottom\", \n                   legend = c(\"Stationary\", \"Nonstationary\"),\n                   ncol = 2, pch = c(20, 21),\n                   col = c(\"black\", \"gray\"), pt.cex = c(1,2),\n                   pt.bg = c(NA, \"gray\"), bty =\"n\",\n                   cex = 2)\n        dev.off()\n      }\n    }\n    \n    if(dis==\"pe3\"){\n      # LP3\n      n_lp = length(ln_used_data)\n      sk_y.w.lp = (1+6/n_lp)*\n        ((n_lp*sum((ln_used_data-mean(ln_used_data))^3)))/\n        ((n_lp-1)*(n_lp-2)*sd(ln_used_data)^3)\n      a.lp3 = max(2/sk_y.w.lp,0.4)\n      b.lp3 = 1 + 0.0144*((max(0,sk_y.w.lp-2.25))^2)\n      f.lp3 = sk_y.w.lp - 0.063*(max(0,sk_y.w.lp-1)^1.85)\n      h.lp3 = (b.lp3 - 2/(sk_y.w.lp*a.lp3))^(1/3)\n      k.p_max2 = 1-((f.lp3/6)^2) + qnorm(1-(1/rp))*((f.lp3/6))\n      kp_i = 1\n      k.p = as.data.frame(\n        matrix(, nrow = length(k.p_max2), \n               ncol = 1))\n      for(kp_i in 1:length(k.p_max2)){\n        k.p[kp_i,] = max(h.lp3,k.p_max2[kp_i])\n      }\n      k.p = a.lp3*(k.p^3) - b.lp3\n      \n      ppp_sta = exp(mean.sta + k.p*sd.sta)\n      ppp_hom = exp(mean.hom + k.p*sd.hom)\n      ppp_het = exp(mean.het + k.p*sd.het)\n    }\n    if(dis==\"gno\"){\n      # LN2\n      ppp_sta.ln2 = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n      ppp_hom.ln2 = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n      ppp_het.ln2 = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n      \n      # LN3\n      # u = ln(x - tau)\n      # Stationarity\n      vr_x = sum((used_data-mean(used_data))^2)/\n        (length(used_data)-1)\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      \n      B = .5*(-g_x + sqrt(g_x^2 + 4))\n      the = (1 - B^(2/3))/(B^(1/3))\n      ome = the^2 + 1\n      \n      mu_u = .5*log(vr_x/(ome*(ome-1)))\n      vr_u = log(ome)\n      ta_x = mean(used_data) - sqrt(vr_x)/the\n      ppp_sta.ln3 = ta_x + exp(mu_u + qnorm(1-(1/rp))*sqrt(vr_u))\n      \n      # Nonstationarity\n      # Homogeneous\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      B_non = .5*(-sk_x.w.hom + sqrt(sk_x.w.hom^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.hom/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.hom - sqrt(vr_x.w.hom)/the_non\n      ppp_hom.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      # Heterogeneous\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      B_non = .5*(-sk_x.w.het + sqrt(sk_x.w.het^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.het/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.het - sqrt(vr_x.w.het)/the_non\n      ppp_het.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      ln.com = c(ppp_sta.ln2, ppp_hom.ln2, ppp_het.ln2,\n                 ppp_sta.ln3, ppp_hom.ln3, ppp_het.ln3)\n      ln.max = signif(ceiling(max(ln.com)), digits = 1)\n      ln.min = signif(round_any(min(ln.com), 10, f = ceiling), digits = 1)\n      plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(ln.min,ln.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.ln2, col = colors[1], lwd = 2)\n      lines(rp_gev, ppp_hom.ln2, col = colors[1], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln2, col = colors[1], lwd = 2, lty = 3)\n      lines(rp_gev, ppp_sta.ln3, col = colors[2], lwd = 2)\n      lines(rp_gev, ppp_hom.ln3, col = colors[2], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln3, col = colors[2], lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n      \n      \n    }\n    if(dis==\"gev\"){\n      dis = \"GEV\"\n      # Stationarity\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      if((g_x<=1.15)&(g_x>=-0.7)){\n        k_sta = 0.0087*g_x^3 + 0.0582^2 - 0.32*g_x + 0.2778\n      }else{\n        k_sta = -.31158*(1-exp(-.4556*(g_x - 0.97134)))\n      }\n      \n      a_sta = sign(k_sta)*k_sta*sqrt(vr_x)/\n        sqrt(gamma(1+2*k_sta)-(gamma(1+2*k_sta))^2)\n      e_sta = mean(used_data) - a_sta*(gamma(1+k_sta)-1)/k_sta\n      \n      pp = seq(0.01,1, length.out = 10001)\n      rp_gev = (1-pp)^-1\n      ppp_sta.gev = e_sta + (a_sta/k_sta)*(1-(-log(pp))^k_sta)\n      \n      # Nonstationarity\n      # Homogeneous\n      if((sk_x.w<=1.15)&(sk_x.w>=-0.7)){\n        k_non.hom = 0.0087*sk_x.w^3 + 0.0582^2 - 0.32*sk_x.w + 0.2778\n      }else{\n        k_non.hom = -.31158*(1-exp(-.4556*(sk_x.w - 0.97134)))\n      }\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      a_non.hom = sign(k_non.hom)*k_non.hom*sqrt(vr_x.w.hom)/\n        sqrt(gamma(1+2*k_non.hom)-(gamma(1+2*k_non.hom))^2)\n      e_non.hom = mean(used_data) - a_non.hom*(gamma(1+k_non.hom)-1)/k_non.hom\n      ppp_hom.gev = e_non.hom + (a_non.hom/k_non.hom)*(1-(-log(pp))^k_non.hom)\n      \n      # Heterogeneous\n      k_non.het = k_non.hom\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      a_non.het = sign(k_non.het)*k_non.het*sqrt(vr_x.w.het)/\n        sqrt(gamma(1+2*k_non.het)-(gamma(1+2*k_non.het))^2)\n      e_non.het = mean(used_data) - a_x*(gamma(1+k_non.het)-1)/k_non.het\n      ppp_het.gev = e_non.het + (a_non.het/k_non.het)*(1-(-log(pp))^k_non.het)\n      \n      last1700 = tail(which(rp_gev<1701),1)\n      gev.com = c(ppp_sta.gev, ppp_hom.gev, ppp_het.gev)[1:last1700]\n      gev.max = signif(ceiling(max(gev.com)), digits = 1)\n      gev.min = signif(round_any(min(gev.com), 10, f = ceiling), digits = 1)\n      plot(rp_gev,ppp_sta.gev, log=\"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(gev.min,gev.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.gev, lwd = 2)\n      lines(rp_gev, ppp_hom.gev, lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.gev, lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n    }\n    cl = cl + 1\n  }\n}\nfor (clu in clu.f:(clu.f+1)){\n  max_clu = get(paste(\"maxsos\",clu,sep=\"\"))\n  colclu = match(c(\"lat\",max_clu[2,c(2:3)]),colnames(kdata.sca))\n  clusters = pam(kdata.sca[,colclu], clu)\n  \n  \n  # Plotting\n  cl = 1\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    \n    file = paste0(\"station_matrix_\",\n                  kdata.sca[cl.index,1],\n                  \"_update.xlsx\")\n    \n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    n.good.fit = length(which(abs(dm.test$Z)<1.64))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    f = 1\n    ## Identify beginning and ending years\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      # Read storm type\n      \n      if(pv==1){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==2){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==3){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Obtain minimum and maximum years of each station\n      yearr[f,1] = min(unique(year(data_gg[,1])))\n      yearr[f,2] = max(unique(year(data_gg[,1])))\n      \n      f = f + 1\n    }\n    \n    super_data = as.data.frame(\n      matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n             ncol = 2*length(file)))\n    super_data[,seq(2,length(file)*2,2)] = \n      min(yearr[,1]):max(yearr[,2])\n    \n    f = 1\n    \n    # Place max accordingly\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,2]\n      }\n      \n      \n      # Obtain desired data\n      super_data[,2*(f-1)+1] = \n        ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n               year_a[,1],NA)\n      \n      f = f + 1\n    }\n    \n    # Obtain max from every row, ignoring NA\n    cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n    \n    if(dim(super_data)[2]<3){\n      super_data = super_data[which(!is.na(super_data[,1])),]\n      yearr_t = super_data[,2]\n      used_data = super_data[,1]\n    }else{\n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n    }\n    \n    # Remove data points that are NA (not recorded)\n    # by indexing together\n    t = yearr_t\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n    # Significance\n    # Linear Regression Model\n    sta.lm = lm(log(used_data)~t)\n    \n    # P-Values for Intercept and Trend Coefficients\n    t1 = sta.lm$coefficients[2]/\n      summary(sta.lm)$coefficients[2,2]\n    \n    alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n    \n    t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n      1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n    \n    # Log transformation and linear regression\n    ln_used_data = log(used_data)\n    lm.year = lm(ln_used_data~t)\n    m.year = lm(used_data~t)\n    sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t\n    rho = cor(ln_used_data,t)\n    \n    # Conditional Moments of y\n    mu_y.w = lm.year$coefficients[1] + \n      lm.year$coefficients[2]*t\n    vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)\n    sk_y.w = skewness(ln_used_data) - \n      skewness(t)*(lm.year$coefficients[2]^3)\n    \n    # Conditional Moments of x\n    mu_x.w = exp(mu_y.w + (vr_y.w)/2)\n    vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)\n    cv_x.w = sqrt(exp(vr_y.w)-1)\n    sk_x.w = 3*cv_x.w + cv_x.w^3\n    \n    # Stationary\n    mean.sta = mean(ln_used_data)\n    sd.sta = sqrt(sum((ln_used_data - \n                         mean(ln_used_data))^2)/\n                    length(ln_used_data))\n    \n    YR = c(10,20,30)\n    LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))\n    \n    t_i = 1\n    while(t_i<4){\n      t_non = c(tail(t,1),2020,2030)\n      # Nonstationary - Mean\n      mean.hom = unname(lm.year$coefficients[1] + \n                          lm.year$coefficients[2]*t_non[t_i])\n      sd.hom = unname(sqrt(var(ln_used_data)-\n                             lm.year$coefficients[2]^2*\n                             var(t)))\n      \n      # Nonstationary - Mean + Cv\n      res = (((lm.year$residuals)^(2))^(1/3))\n      lm.res = lm(res~t)\n      mean.het = mean.hom\n      sd.het = sqrt((lm.res$coefficients[1]+\n                       lm.res$coefficients[2]*tail(t,1))^3 +\n                      3*(summary(lm.res)$sigma^2)*\n                      (lm.res$coefficients[1]+\n                         lm.res$coefficients[2]*t_non[t_i]))\n      \n      # Generate return period data points\n      rp = seq(1.01,1700, length.out = 10001)\n      \n      ppp_sta.ln2 = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n      ppp_hom.ln2 = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n      ppp_het.ln2 = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n      \n      \n      if(t_i==1){\n        png(filename=paste0(dir,\"/plots/\",texty[pv],\n                            \"/cOPT\",clu,\"_\",cl,\".png\"),\n            w = 800, h = 600)\n        par(mfrow=c(1,2))\n        plot(t,ln_used_data, xlab = \"Year\",\n             ylab = \"ln (Peak Wind Gust)\",,\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.1)\n        axis(side=1, \n             at = c(1980,1990,2000,2010),\n             cex.axis = 1.5)\n        axis(side=2, \n             cex.axis = 1.5)\n        abline(lm(ln_used_data~t))\n        \n        plot(t, res, xlab = \"Year\",\n             ylab = expression(epsilon^{2/3}),\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.1)\n        abline(lm(res~t))\n        axis(side=1, \n             at = c(1980,1990,2000,2010),\n             cex.axis = 1.5)\n        axis(side=2, \n             cex.axis = 1.5)\n        dev.off()\n        \n        max.mean = unname(lm.year$coefficients[1] + \n                            lm.year$coefficients[2]*2030)\n        max.sd = sqrt((lm.res$coefficients[1]+\n                         lm.res$coefficients[2]*2030)^3 +\n                        3*(summary(lm.res)$sigma^2)*\n                        (lm.res$coefficients[1]+\n                           lm.res$coefficients[2]*2030))\n        max.hom = exp(max.mean + qnorm(1-(1/rp))*sd.hom)\n        max.het = exp(max.mean + qnorm(1-(1/rp))*max.sd)\n        ln2.max = round_any(max(max.hom,max.het,ppp_sta.ln2),\n                            10, \n                            f = ceiling)\n        if((pv==4)&(cl==2)){\n          ln2.max = 180\n        }\n        ln2.min = round_any(min(c(ppp_sta.ln2,ppp_hom.ln2,ppp_het.ln2)),\n                            10, f = floor)\n        \n        \n        par(mar = c(4,4,2,2))\n        png(filename=paste0(dir,\"/plots/\",texty[pv],\n                            \"/opt\",clu,\"_\",cl,\".png\"),\n            w = 800, h = 600)\n        plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n             xlim = c(1,3500), ylim = c(ln2.min,ln2.max),\n             xlab = \"Return Period\",\n             ylab = \"Peak wind gust (mph)\",\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.5)\n        axis(side=1, labels = T, \n             at=c(1, 10, 50, 100,300,700,1700),\n             cex.axis = 1.5)\n        axis(side=2, labels = T, \n             at=seq(40,240,10),\n             cex.axis = 1)\n        abline(v=c(1, 10, 50, 100, 300,700, 1700), \n               h=seq(40,240,10), \n               col=\"gray\", lty=3)\n        legend(\"bottomright\", \n               c(\"Stationary\", \n                 \"Trend in Mean\",\n                 \"Trend in Mean + Cv\"), \n               lty = c(1,2,3),\n               lwd = 2,\n               cex = 1.5)\n        lines(rp, ppp_sta.ln2, col = colors[cl], lwd = 2)\n      }\n      \n      \n      lines(rp, ppp_hom.ln2, col = colors[cl], lwd = 1+t_i, lty = 2)\n      lines(rp, ppp_het.ln2, col = colors[cl], lwd = 1+t_i, lty = 3)\n      text(x= 1700, y= max(ppp_hom.ln2), cex = 0.75,\n           pos = 4, labels = paste(t_non[t_i], \"HOM\"))\n      text(x= 1700, y= max(ppp_het.ln2), cex = 0.75,\n           pos = 4, labels = paste(t_non[t_i], \"HET\"))\n      \n      LLL[1,t_i] = ppp_sta.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i] = ppp_sta.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i] = ppp_sta.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i] = ppp_sta.ln2[which.min(abs(rp-1700))]\n      LLL[1,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-1700))]\n      LLL[1,t_i + 6] = ppp_het.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i + 6] = ppp_het.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i + 6] = ppp_het.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i + 6] = ppp_het.ln2[which.min(abs(rp-1700))]\n      \n      if(t_i==3){\n        assign(paste(\"LLL\",clu,cl,YR[t_i], sep = \"_\"),\n               LLL)\n      }\n      t_i = t_i + 1\n    }\n    \n    dev.off()\n    \n    if(dis==\"pe3\"){\n      source(\"pe3.R\")\n    }\n    if(dis==\"gno\"){\n      source(\"gno.R\")\n    }\n    if(dis==\"gev\"){\n      source(\"gev.R\")\n    }\n    cl = cl + 1\n  }\n}\ncl.tabl[which(cl.tabl[,5]<1),11] = \"AHom\"\ncl.tabl[which(cl.tabl[,5]>2),11] = \"DHet\"\ncl.tabl[which((cl.tabl[,5]<2)&(cl.tabl[,5]>1)),11] = \"PHet\"\ncolnames(cl.tabl) = c(\"Cluster\",\"Cluster ID\", \"N\", \"N_d\", \n                      \"H\",\"SlopeC\",\"SlopeP\",\n                      \"TypeI\",\"TypeII\",\"MK\",\"Type\",\n                      \"Var1\",\"Var2\",\"Var3\")\nemptycol = as.data.frame(\n  matrix(\".\", nrow = dim(cl.tabl)[1], ncol = 1))\ncl.tabl = cbind(lll.cl.tabl,emptycol,cl.tabl)\nwrite.csv(cl.tabl,file = paste0(dir,\"/plots/\",\n                                texty[pv],\n                                \"/table11f.csv\"))\n\n# Slope Coefficient Plot\n# 3 Pval, 4 Type I, 5 Type II, 6 MK\nsig_tar = 4\nclu.f = floor(sqrt(dim(wanted)[1]/2))\nfor(clu in clu.f:(clu.f+1)){\n  \n  ref_st = read.csv(paste0(dir,\"/plots/table.csv\"), header = F)\n  \n  if(pv==1){\n    colnames(ref_st) = sapply(ref_st[2,], as.character)\n    ref_st = ref_st[3:9,]\n    ref_st = ref_st[,(which(ref_st ==\".\", arr.ind= T)[2,2]+1):dim(ref_st)[2]]\n  }else if(pv==2){\n    colnames(ref_st) = sapply(ref_st[12,], as.character)\n    ref_st = ref_st[13:19,]\n    ref_st = ref_st[,(which(ref_st ==\".\", arr.ind= T)[2,2]+1):dim(ref_st)[2]]\n  }else if(pv==3){\n    colnames(ref_st) = sapply(ref_st[22,], as.character)\n    ref_st = ref_st[23:27,]\n    ref_st = ref_st[,(which(ref_st ==\".\", arr.ind= T)[2,2]+1):dim(ref_st)[2]]\n    \n  }else if(pv == 4){\n    colnames(ref_st) = sapply(ref_st[30,], as.character)\n    ref_st = ref_st[31:35,]\n    ref_st = ref_st[,(which(ref_st ==\".\", arr.ind= T)[2,2]+1):dim(ref_st)[2]]\n    \n  }\n  \n  clusters = pam(kdata.sca[,colclu], clu)\n  das = c(1:clu)\n  cl = 1\n  \n  # Go to every cluster...\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      dm.sta = \n        dm.table[which(as.numeric(dm.test$D)>dm.test$Dcrit[1]),1]\n      dm.point = cbind(dis_ang[match(dm.sta,dis_ang[,2]),c(9,10)],cl)\n    }\n    f = 1\n    \n    ## Obtain desired data and remove discordant stations\n    file = paste0(\"station_matrix_\",\n                  kdata.sca[which(clusters$clustering==cl),1],\n                  \"_update.xlsx\")\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    sig_stat = as.data.frame(\n      matrix(, nrow = length(file), \n             ncol = 6))\n    colnames(sig_stat) = c(\"id\", \"Pval\", \"TypeI\",\n                           \"TypeII\", \"MK\", \"SlopeC\")\n    \n    f = 1\n    \n    # Obtain statistics\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,3]\n      }\n      \n      t = used_date - min(used_date)\n      \n      # Significance\n      # Linear Regression Model\n      sta.lm = lm(log(used_data)~t)\n      \n      # P-Values for Intercept and Trend Coefficients\n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[2,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      ## Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk = 2*(1-pnorm(abs(mk_z)))\n      \n      \n      colnames(sig_stat) = c(\"id\", \"SlopeC\", \"Pval\", \n                             \"TypeI\", \"TypeII\", \"MK\")\n      sig_stat[f,1] = as.numeric(substr(file[f],16,21))\n      sig_stat[f,2] = format(round(\n        summary(sta.lm)$coefficients[2,1], 5), nsmall = 5)\n      sig_stat[f,3] = format(round(\n        summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n      sig_stat[f,4] = format(round(alp,5), nsmall = 5)\n      sig_stat[f,5] = format(round(\n        pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n      sig_stat[f,6] = mk\n      \n      f = f + 1\n    }\n    \n    \n    if(cl==1){\n      par(mar = c(4,4,2,2))\n      png(filename=paste0(dir,\"/plots/\",texty[pv],\n                          \"/slopeoptC\",clu,\".png\"),\n          w = 800, h = 600)\n      plot(das,\n           seq(-.03,.03, length.out=clu),\n           type = \"n\",\n           xlab = \"Clusters\",\n           ylab = \"Slope Coefficient\",\n           xaxt = \"n\",\n           xlim = c(0.5,clu+0.5),\n           ylim = c(-0.03,0.03),\n           cex.lab = 1.5)\n    }\n    \n    if(as.numeric(sapply((\n      subset(ref_st,ref_st[,1]==clu&ref_st[,2]==cl)[sig_tar+4]),\n      as.character))>0.05){\n      rect(cl-.5,-1,cl+.5,1,col = rgb(0.5,0.5,0.5,1/4),\n           border = NA)\n    }\n    # Significant points\n    cl_si = which(sig_stat[,sig_tar]<0.05)\n    points(seq(cl,cl,length.out=length(cl_si)),\n           sig_stat[cl_si,2],\n           pch = 3,cex = 2)\n    # Insignificant points\n    cl_insi = which(sig_stat[,sig_tar]>0.050001)\n    points(seq(cl,cl,length.out=length(cl_insi)),\n           sig_stat[cl_insi,2],\n           pch = 19,cex = 0.5)\n    # Superstation point\n    points(cl,\n           subset(ref_st,ref_st[,1]==clu.f&ref_st[,2]==cl)[6],\n           pch= 18, col = \"red\")\n    \n    abline(h = 0, col = \"gray\", lty = \"dotted\")\n    axis(1, at=cl, labels = cl, tick = T)\n    cl = cl + 1\n  }\n  legend(\"bottomleft\",\n         legend=c(\"Insignificant\",\n                  \"Significant\",\n                  \"Cluster Superstation\"),\n         col=c(\"black\",\"black\",\"red\"),\n         pch=c(19,3,18),\n         pt.cex=c(0.5,2,1))\n  dads = as.numeric(sapply(ref_st[which(ref_st[,1]==clu.f),6],\n                           as.character))\n  points(das,dads[das], pch= 18, col = \"red\")\n  dev.off\n}",
    "created" : 1506493952678.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "4|29|1011|0|\n1142|7|1251|6|\n1352|17|1429|6|\n",
    "hash" : "1275013291",
    "id" : "2F59B315",
    "lastKnownWriteTime" : 1506494069,
    "last_content_update" : 1506494069699,
    "path" : "C:/Users/Jai/Box Sync/Wind/Florida/optimal.R",
    "project_path" : "optimal.R",
    "properties" : {
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}