{
    "collab_server" : "",
    "contents" : "# Lat, Lon, L-CV\ncl.tabl = as.data.frame(\n  matrix(, nrow = , ncol = 12))\nclu.f = floor(sqrt(length(stations_dup[,1])/2))\nfor (clu in clu.f:(clu.f+1)){\n  clusters = pam(kdata.sca[,c(2,3,7)], clu)\n  {\n    par(mar=c(2,2,2,2))\n    png(file=paste0(dir,\"/plots/\",texty[pv],\n                    \"/latlonlcv_fff\",clu,\".png\"),\n        w = 800, h = 600)\n    mappoint= cbind(kdata[,2:3],\n                    clusters$clustering)\n    newmap <- getMap(resolution = \"low\")\n    # windows(800, 600, pointsize = 12)\n    plot(newmap, xlim = c(-80, -65), ylim = c(25, 45), asp = 1)\n    legend(-63,36,legend = c(1:clu,\"Discordant\"),\n           bg = \"white\",col=c(colors[1:clu],\"black\"), \n           bty = \"n\", pch = c(rep(20,clu),1), \n           pt.cex = c(rep(3,clu),4), \n           pt.lwd = c(rep(1,clu),3),\n           ncol = 1, cex = 1.25)\n    \n    ll = 1\n    while(ll<length(colnames(clusters$medoids))+1){\n      text(-63,42.5-ll*1.5,\n           capitalize(colnames(clusters$medoids)[ll]),\n           cex = 2)\n      ll = ll + 1\n    }\n    \n    text(-63,36.5,\n         paste0(\"Si: \",summary(silhouette(clusters))$si.summary[4]),\n         cex = 2)\n    \n    cl = 1\n    if(clu!=clu.f){\n      dimc = dim(cl.tabl)[1]\n    }else{\n      dimc = 0\n    }\n    \n    # Identify\n    while(cl<length(unique(clusters$clustering))+1){\n      cl.index = which(as.numeric(clusters$clustering)==cl)\n      dm.table = cbind(kdata[cl.index,1],\n                       k_n[cl.index],\n                       cov[cl.index,3],\n                       lmom[cl.index,8:11])\n      colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n      dm.test = regtst(dm.table[,1:6],5000)\n      is.dm.ex = \n        length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n      n.good.fit = length(which(abs(dm.test$Z)<1.64))\n      \n      # Check if there are discordant stations\n      if(is.dm.ex>0){\n        dm.sta = \n          dm.table[which(as.numeric(dm.test$D)>dm.test$Dcrit[1]),1]\n        dm.point = cbind(kdata[match(dm.sta,kdata[,1]),2:3],cl)\n        points(dm.point$LON,dm.point$LAT,\n               col=colors[cl], cex=6, pch = 1,\n               lwd = 3)\n      }\n      \n      if(clu==clu.f){\n        cl.tabl[cl,1] = clu\n        cl.tabl[cl,2] = cl\n        cl.tabl[cl,3] = length(dm.test$D)\n        cl.tabl[cl,4] = \n          length(which(dm.test$D>dm.test$Dcrit[1]))\n        cl.tabl[cl,5] = dm.test$H[1]\n        n.g = 1\n        if(n.good.fit!=0){\n          for(n.g in 1:n.good.fit){\n            cl.tabl[cl,11+(2*n.g)-1] = sort(abs(dm.test$Z))[n.g]\n            cl.tabl[cl,11+(2*n.g)] = attributes(sort(abs(dm.test$Z))[n.g])\n          }\n        }else{\n          cl.tabl[cl,12] = sort(abs(abs(dm.test$Z)-1.64))[1] + 1.64\n          cl.tabl[cl,13] = attributes(sort(abs(abs(dm.test$Z)-1.64))[1])\n        }\n      }else{\n        cl.tabl[dimc + cl,1] = clu\n        cl.tabl[dimc + cl,2] = cl\n        cl.tabl[dimc + cl,3] = length(dm.test$D)\n        cl.tabl[dimc + cl,4] = \n          length(which(dm.test$D>dm.test$Dcrit[1]))\n        cl.tabl[dimc + cl,5] = dm.test$H[1]\n        n.g = 1\n        if(n.good.fit!=0){\n          for(n.g in 1:n.good.fit){\n            cl.tabl[dimc + cl,11+(2*n.g)-1] = sort(abs(dm.test$Z))[n.g]\n            cl.tabl[dimc + cl,11+(2*n.g)] = attributes(sort(abs(dm.test$Z))[n.g])\n          }\n        }else{\n          cl.tabl[dimc + cl,12] = sort(abs(abs(dm.test$Z)-1.64))[1] + 1.64\n          cl.tabl[dimc + cl,13] = attributes(sort(abs(abs(dm.test$Z)-1.64))[1])\n        }\n      }\n      \n      f = 1\n      \n      ## Obtain desired data\n      file = paste0(\"station_matrix_\",\n                    kdata.sca[which(clusters$clustering==cl),1],\n                    \"_update.xlsx\")\n      if(is.dm.ex>0){\n        file_r = paste0(\"station_matrix_\",\n                        dm.sta,\n                        \"_update.xlsx\")\n        file = file[which(!(file %in% file_r))]\n      }\n      yearr = as.data.frame(\n        matrix(, nrow = , ncol = 2))\n      \n      ## Identify beginning and ending years\n      while(f<length(file)+1){\n        data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n        \n        \n        # Delete unnecessary rows until first data\n        data_gg = data_g[(grep(\"^Date\", \n                               data_g[,1])+1):length(data_g[,1]),]\n        \n        # assign data_gg column names as data_g's row 6 \n        # because it has column names aligned correctly\n        colnames(data_gg) <- data_g[6,]\n        \n        # Date/Time Format originally MM/DD/YYYY HH:MM\n        # Distorted to Excel Serial Date (start from 1900-01-01)\n        # numbers while loading. Hence, these are  \n        # converted to YYYY-MM-DD HH:MM format\n        data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                                 + as.POSIXct(\"1899-12-30 00:00\"))\n        \n        # Convert strings into numbers\n        data_gg[,2] = as.numeric(data_gg[,2])\n        data_gg[,3] = as.numeric(data_gg[,3])\n        \n        # Read storm type\n        # Read storm type\n        \n        if(pv==2){\n          # Non-thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n        }\n        if(pv==3){\n          # Thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n        }\n        if(pv==4){\n          # Tropical Storm\n          data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n        }\n        \n        # Obtain minimum and maximum years of each station\n        yearr[f,1] = min(unique(year(data_gg[,1])))\n        yearr[f,2] = max(unique(year(data_gg[,1])))\n        \n        f = f + 1\n      }\n      \n      super_data = as.data.frame(\n        matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n               ncol = 2*length(file)))\n      super_data[,seq(2,length(file)*2,2)] = \n        min(yearr[,1]):max(yearr[,2])\n      \n      f = 1\n      \n      # Place max accordingly\n      while(f<length(file)+1){\n        data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n        \n        # Delete unnecessary rows until first data\n        data_gg = data_g[(grep(\"^Date\", \n                               data_g[,1])+1):length(data_g[,1]),]\n        \n        # assign data_gg column names as data_g's row 6 \n        # because it has column names aligned correctly\n        colnames(data_gg) <- data_g[6,]\n        \n        # Date/Time Format originally MM/DD/YYYY HH:MM\n        # Distorted to Excel Serial Date (start from 1900-01-01)\n        # numbers while loading. Hence, these are  \n        # converted to YYYY-MM-DD HH:MM format\n        data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                                 + as.POSIXct(\"1899-12-30 00:00\"))\n        \n        # Convert strings into numbers\n        data_gg[,2] = as.numeric(data_gg[,2])\n        data_gg[,3] = as.numeric(data_gg[,3])\n        \n        # Read storm type\n        if(pv==2){\n          # Non-thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n        }\n        if(pv==3){\n          # Thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n        }\n        if(pv==4){\n          # Tropical Storm\n          data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n        }\n        \n        source(paste0(dir_fl,\"annmax.R\"))\n        \n        # Obtain desired data\n        super_data[,2*(f-1)+1] = \n          ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n                 year_a[,1],NA)\n        \n        f = f + 1\n      }\n      \n      # Obtain max from every row, ignoring NA\n      cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n      \n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      used_data = log(used_data)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n      \n      # Remove data points that are NA (not recorded)\n      # by indexing together\n      t = yearr_t - min(yearr_t)\n      t[which(is.na(used_data))] = NA\n      used_data <- used_data[!is.na(used_data)]\n      t <- t[!is.na(t)]\n      \n      # Significance\n      # Linear Regression Model\n      sta.lm = lm(used_data~t)\n      \n      # P-Values for Intercept and Trend Coefficients\n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[2,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      \n      # Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk = 2*(1-pnorm(abs(mk_z)))\n      \n      if(clu==clu.f){\n        cl.tabl[cl,6] = format(round(\n          summary(sta.lm)$coefficients[2,1], 6), nsmall = 5)\n        cl.tabl[cl,7] = format(round(\n          summary(sta.lm)$coefficients[2,4], 6), nsmall = 5)\n        cl.tabl[cl,8] = format(round(alp,5), nsmall = 5)\n        cl.tabl[cl,9] = format(round(\n          pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n        cl.tabl[cl,10] = mk\n      }else{\n        cl.tabl[dimc + cl,6] = format(round(\n          summary(sta.lm)$coefficients[2,1], 6), nsmall = 5)\n        cl.tabl[dimc + cl,7] = format(round(\n          summary(sta.lm)$coefficients[2,4], 6), nsmall = 5)\n        cl.tabl[dimc + cl,8] = format(round(alp,5), nsmall = 6)\n        cl.tabl[dimc + cl,9] = format(round(\n          pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n        cl.tabl[dimc + cl,10] = mk\n      }\n      \n      cl = cl + 1\n    }\n    \n    # Plot clusters!\n    for(mm in 1:max(clusters$clustering)){\n      mmm = which(clusters$clustering %in% mm)\n      points(mappoint$lon[mmm],mappoint$lat[mmm],\n             col=colors[mm], cex=4, pch = 20)\n      mm = mm + 1\n    }\n    \n    # savePlot(\"clipboard\", type=\"wmf\")\n    \n  }\n  \n  dev.off()\n  # Plotting\n  cl = 1\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    \n    file = paste0(\"station_matrix_\",\n                  kdata.sca[cl.index,1],\n                  \"_update.xlsx\")\n    \n    dm.table = cbind(kdata[cl.index,1],\n                     k_n[cl.index],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    n.good.fit = length(which(abs(dm.test$Z)<1.64))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    f = 1\n    \n    ## Identify beginning and ending years\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      # Read storm type\n      \n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Obtain minimum and maximum years of each station\n      yearr[f,1] = min(unique(year(data_gg[,1])))\n      yearr[f,2] = max(unique(year(data_gg[,1])))\n      \n      f = f + 1\n    }\n    \n    super_data = as.data.frame(\n      matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n             ncol = 2*length(file)))\n    super_data[,seq(2,length(file)*2,2)] = \n      min(yearr[,1]):max(yearr[,2])\n    \n    f = 1\n    \n    # Place max accordingly\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,2]\n      }\n      \n      \n      # Obtain desired data\n      super_data[,2*(f-1)+1] = \n        ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n               year_a[,1],NA)\n      \n      f = f + 1\n    }\n    \n    # Obtain max from every row, ignoring NA\n    cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n    \n    \n    if(dim(super_data)[2]<3){\n      super_data = super_data[which(!is.na(super_data[,1])),]\n      yearr_t = super_data[,2]\n      used_data = super_data[,1]\n    }else{\n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n    }\n    \n    # Remove data points that are NA (not recorded)\n    # by indexing together\n    t = yearr_t\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n    # Significance\n    # Linear Regression Model\n    sta.lm = lm(log(used_data)~t)\n    \n    # P-Values for Intercept and Trend Coefficients\n    t1 = sta.lm$coefficients[2]/\n      summary(sta.lm)$coefficients[2,2]\n    \n    alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n    \n    t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n      1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n    \n    # Log transformation and linear regression\n    ln_used_data = log(used_data)\n    lm.year = lm(ln_used_data~t)\n    m.year = lm(used_data~t)\n    sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t\n    rho = cor(ln_used_data,t)\n    \n    # Conditional Moments of y\n    mu_y.w = lm.year$coefficients[1] + \n      lm.year$coefficients[2]*t\n    vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)\n    sk_y.w = skewness(ln_used_data) - \n      skewness(t)*(lm.year$coefficients[2]^3)\n    \n    # Conditional Moments of x\n    mu_x.w = exp(mu_y.w + (vr_y.w)/2)\n    vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)\n    cv_x.w = sqrt(exp(vr_y.w)-1)\n    sk_x.w = 3*cv_x.w + cv_x.w^3\n    \n    # Stationary\n    mean.sta = mean(ln_used_data)\n    sd.sta = sqrt(sum((ln_used_data - \n                         mean(ln_used_data))^2)/\n                    length(ln_used_data))\n    YR = c(10,20,30)\n    LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))\n    t_i = 1\n    \n    {\n      # PPCC \n      b_coef = cor(ln_used_data, t)*sd(ln_used_data)/sd(t)\n      mu_y.pp = mean.sta + b_coef*(t-mean(t))\n      sd_y.pp = sqrt(var(ln_used_data)*(1-cor(ln_used_data, t)^2))\n      z_i.s = sort((ln_used_data - mean.sta)/sd.sta)\n      z_i.n = sort((ln_used_data - mu_y.pp)/sd_y.pp)\n      rank_i.s = rank(z_i.s)\n      rank_i.n = rank(z_i.n)\n      p_pos.s = (rank_i.s - 0.375)/(length(ln_used_data) + 0.25)\n      p_pos.n = (rank_i.n - 0.375)/(length(ln_used_data) + 0.25)\n      \n      z_ii.s = qnorm(p_pos.s)\n      z_ii.n = qnorm(p_pos.n)\n      \n      corst = round(cor(z_ii.s,z_i.s),4)\n      corns = round(cor(z_ii.n,z_i.n),4)\n      \n      \n      }\n    \n    cl = cl + 1\n    }\n    \n  \n  }\nfor (clu in clu.f:(clu.f+1)){\n  clusters = pam(kdata.sca[,c(2,3,7)], clu)\n  # Plotting\n  cl = 1\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    \n    file = paste0(\"station_matrix_\",\n                  kdata.sca[cl.index,1],\n                  \"_update.xlsx\")\n    \n    dm.table = cbind(kdata[cl.index,1],\n                     k_n[cl.index],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    n.good.fit = length(which(abs(dm.test$Z)<1.64))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    f = 1\n    \n    ## Identify beginning and ending years\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      # Read storm type\n      \n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Obtain minimum and maximum years of each station\n      yearr[f,1] = min(unique(year(data_gg[,1])))\n      yearr[f,2] = max(unique(year(data_gg[,1])))\n      \n      f = f + 1\n    }\n    \n    super_data = as.data.frame(\n      matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n             ncol = 2*length(file)))\n    super_data[,seq(2,length(file)*2,2)] = \n      min(yearr[,1]):max(yearr[,2])\n    \n    f = 1\n    \n    # Place max accordingly\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      source(paste0(dir_fl,\"annmax.r\"))\n      \n      # Obtain desired data\n      super_data[,2*(f-1)+1] = \n        ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n               year_a[,1],NA)\n      \n      f = f + 1\n    }\n    \n    if(dim(super_data)[2]<3){\n      super_data = super_data[which(!is.na(super_data[,1])),]\n      yearr_t = super_data[,2]\n      used_data = super_data[,1]\n    }else{\n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n    }\n    \n    # Remove data points that are NA (not recorded)\n    # by indexing together\n    t = yearr_t\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n    # Significance\n    # Linear Regression Model\n    sta.lm = lm(log(used_data)~t)\n    \n    # P-Values for Intercept and Trend Coefficients\n    t1 = sta.lm$coefficients[2]/\n      summary(sta.lm)$coefficients[2,2]\n    \n    alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n    \n    t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n      1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n    \n    # Log transformation and linear regression\n    ln_used_data = log(used_data)\n    lm.year = lm(ln_used_data~t)\n    m.year = lm(used_data~t)\n    sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t\n    rho = cor(ln_used_data,t)\n    \n    # Conditional Moments of y\n    mu_y.w = lm.year$coefficients[1] + \n      lm.year$coefficients[2]*t\n    vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)\n    sk_y.w = skewness(ln_used_data) - \n      skewness(t)*(lm.year$coefficients[2]^3)\n    \n    # Conditional Moments of x\n    mu_x.w = exp(mu_y.w + (vr_y.w)/2)\n    vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)\n    cv_x.w = sqrt(exp(vr_y.w)-1)\n    sk_x.w = 3*cv_x.w + cv_x.w^3\n    \n    # Stationary\n    mean.sta = mean(ln_used_data)\n    sd.sta = sqrt(sum((ln_used_data - \n                         mean(ln_used_data))^2)/\n                    length(ln_used_data))\n    YR = c(10,20,30)\n    LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))\n    t_i = 1\n    \n    while(t_i<4){\n      \n      t_non = c(tail(t,1),2020,2030)\n      # Nonstationary - Mean\n      mean.hom = unname(lm.year$coefficients[1] + \n                          lm.year$coefficients[2]*t_non[t_i])\n      sd.hom = unname(sqrt(var(ln_used_data)-\n                             lm.year$coefficients[2]^2*\n                             var(t)))\n      \n      # Nonstationary - Mean + Cv\n      res = (((lm.year$residuals)^(2))^(1/3))\n      lm.res = lm(res~t)\n      mean.het = mean.hom\n      sd.het = sqrt((lm.res$coefficients[1]+\n                       lm.res$coefficients[2]*tail(t,1))^3 +\n                      3*(summary(lm.res)$sigma^2)*\n                      (lm.res$coefficients[1]+\n                         lm.res$coefficients[2]*t_non[t_i]))\n      \n      \n      \n      \n      # Generate return period data points\n      rp = seq(1.01,1700, length.out = 10001)\n      \n      if(dis==\"ln2\"){\n        ppp_sta = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n        ppp_hom = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n        ppp_het = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n      }\n      if(dis==\"gev\"){\n        source(paste0(dir_fl,\"gev.R\"))\n      }\n      if(dis==\"pe3\"){\n        source(paste0(dir_fl,\"pe3.R\"))\n      }\n      if(dis==\"gno\"){\n        source(paste0(dir_fl,\"gno.R\"))\n      }\n      \n      if(t_i==1){\n        png(filename=paste0(dir,\"/plots/\",texty[pv],\n                            \"/cLLL\",clu,\"_\",cl,\".png\"),\n            w = 800, h = 600)\n        par(mfrow=c(1,2))\n        plot(t,ln_used_data, xlab = \"Year\",\n             ylab = \"ln (Peak Wind Gust)\",\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.1)\n        abline(lm(ln_used_data~t))\n        axis(side=1, \n             at = c(1980,1990,2000,2010),\n             cex.axis = 1.5)\n        axis(side=2, \n             cex.axis = 1.5)\n        \n        plot(t, res, xlab = \"Year\",\n             ylab = expression(epsilon^{2/3}),\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.1)\n        abline(lm(res~t))\n        axis(side=1, \n             at = c(1980,1990,2000,2010),\n             cex.axis = 1.5)\n        axis(side=2, \n             cex.axis = 1.5)\n        dev.off()\n        \n        par(mar = c(4,4,2,2))\n        png(filename=paste0(dir,\"/plots/\",texty[pv],\n                            \"/lll\",clu,\"_\",cl,\".png\"),\n            w = 800, h = 600)\n        plot(rp,ppp_sta, log = \"x\", typ=\"n\", \n             xlim = c(1,3500), ylim = c(40,150),\n             xlab = \"Return Period\",\n             ylab = \"Peak wind gust (mph)\",\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.5)\n        axis(side=1, labels = T, \n             at=c(1, 10, 50, 100,300,700,1700),\n             cex.axis = 1.5)\n        axis(side=2, labels = T, \n             at=seq(40,240,10),\n             cex.axis = 1)\n        abline(v=c(1, 10, 50, 100, 300,700, 1700), \n               h=seq(40,240,10), \n               col=\"gray\", lty=3)\n        legend(\"bottomright\", \n               c(\"Stationary\", \n                 \"Trend in Mean\",\n                 \"Trend in Mean + CV\"), \n               lty = c(1,2,3),\n               lwd = 2,\n               cex = 1.5)\n        lines(rp, ppp_sta, col = colors[cl], lwd = 2)\n      }\n      \n      \n      lines(rp, ppp_hom, col = colors[cl], lwd = 1+t_i, lty = 2)\n      lines(rp, ppp_het, col = colors[cl], lwd = 1+t_i, lty = 3)\n      text(x= 1700, y= max(ppp_hom), cex = 1,\n           pos = 4, labels = paste(t_non[t_i], \"HOM\"))\n      text(x= 1700, y= max(ppp_het), cex = 1,\n           pos = 4, labels = paste(t_non[t_i], \"HET\"))\n      \n      LLL[1,t_i] = ppp_sta[which.min(abs(rp-100))]\n      LLL[2,t_i] = ppp_sta[which.min(abs(rp-300))]\n      LLL[3,t_i] = ppp_sta[which.min(abs(rp-700))]\n      LLL[4,t_i] = ppp_sta[which.min(abs(rp-1700))]\n      LLL[1,t_i + 3] = ppp_hom[which.min(abs(rp-100))]\n      LLL[2,t_i + 3] = ppp_hom[which.min(abs(rp-300))]\n      LLL[3,t_i + 3] = ppp_hom[which.min(abs(rp-700))]\n      LLL[4,t_i + 3] = ppp_hom[which.min(abs(rp-1700))]\n      LLL[1,t_i + 6] = ppp_het[which.min(abs(rp-100))]\n      LLL[2,t_i + 6] = ppp_het[which.min(abs(rp-300))]\n      LLL[3,t_i + 6] = ppp_het[which.min(abs(rp-700))]\n      LLL[4,t_i + 6] = ppp_het[which.min(abs(rp-1700))]\n      if(t_i==3){\n        assign(paste(\"LLL\",clu,cl,YR[t_i], sep = \"_\"),\n               LLL)\n      }\n      t_i = t_i + 1\n    }\n    par(fig = c(grconvertX(c(1, 10), from=\"user\", to=\"ndc\"),\n                grconvertY(c(110, 155), from=\"user\", to=\"ndc\")),\n        mar = c(1,0,1,0),\n        new = TRUE)\n    plot(newmap, xlim = c(-80, -65), ylim = c(25, 45), asp = 1)\n    clm = which(clusters$clustering %in% cl)\n    points(mappoint$lon[clm],mappoint$lat[clm],\n           col=colors[cl], cex=1.25, pch = 20)\n    dev.off()\n    \n    cl = cl + 1\n  }\n}\ncl.tabl[which(cl.tabl[,5]<1),11] = \"AHom\" \ncl.tabl[which(cl.tabl[,5]>2),11] = \"DHet\" \ncl.tabl[which((cl.tabl[,5]<2)&(cl.tabl[,5]>1)),11] = \"PHet\"\ncolnames(cl.tabl) = c(\"Cluster\", \"Cluster ID\", \"N\", \n                      \"N_d\", \"H\", \"SlopeC\",  \n                      \"SlopeP\", \"TypeI\", \"TypeII\", \n                      \"MK\", \"Type\")\nlll.cl.tabl = cl.tabl\n\n\n# Slope Coefficient Plot\n# 3 Pval, 4 Type I, 5 Type II, 6 MK\nsig_tar = 4\nclu.f = floor(sqrt(dim(wanted)[1]/2))\nfor(clu in clu.f:(clu.f+1)){\n  \n  ref_st = read.csv(paste0(dir,\"/plots/table.csv\"), header = F)\n  \n  if(pv==1){\n    colnames(ref_st) = sapply(ref_st[2,], as.character)\n    ref_st = ref_st[3:9,1:11]\n    \n  }else if(pv==2){\n    colnames(ref_st) = sapply(ref_st[12,], as.character)\n    ref_st = ref_st[13:19,1:11]\n    \n  }else if(pv==3){\n    colnames(ref_st) = sapply(ref_st[22,], as.character)\n    ref_st = ref_st[23:27,1:11]\n  }else if(pv == 4){\n    colnames(ref_st) = sapply(ref_st[30,], as.character)\n    ref_st = ref_st[31:35,1:11]\n  }\n  \n  clusters = pam(kdata.sca[,c(2,3,7)], clu)\n  das = c(1:clu)\n  cl = 1\n  \n  # Go to every cluster...\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    dm.table = cbind(kdata[cl.index,1],\n                     k_n[cl.index],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      dm.sta = \n        dm.table[which(as.numeric(dm.test$D)>dm.test$Dcrit[1]),1]\n      dm.point = cbind(dis_ang[match(dm.sta,dis_ang[,2]),c(9,10)],cl)\n    }\n    f = 1\n    \n    ## Obtain desired data and remove discordant stations\n    file = paste0(\"station_matrix_\",\n                  kdata.sca[which(clusters$clustering==cl),1],\n                  \"_update.xlsx\")\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    sig_stat = as.data.frame(\n      matrix(, nrow = length(file), \n             ncol = 6))\n    colnames(sig_stat) = c(\"id\", \"Pval\", \"TypeI\",\n                           \"TypeII\", \"MK\", \"SlopeC\")\n    \n    f = 1\n    \n    # Obtain statistics\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,3]\n      }\n      \n      t = used_date - min(used_date)\n      \n      # Significance\n      # Linear Regression Model\n      sta.lm = lm(log(used_data)~t)\n      \n      # P-Values for Intercept and Trend Coefficients\n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[2,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      ## Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk = 2*(1-pnorm(abs(mk_z)))\n      \n      \n      colnames(sig_stat) = c(\"id\", \"SlopeC\", \"Pval\", \n                             \"TypeI\", \"TypeII\", \"MK\")\n      sig_stat[f,1] = as.numeric(substr(file[f],16,21))\n      sig_stat[f,2] = format(round(\n        summary(sta.lm)$coefficients[2,1], 5), nsmall = 5)\n      sig_stat[f,3] = format(round(\n        summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n      sig_stat[f,4] = format(round(alp,5), nsmall = 5)\n      sig_stat[f,5] = format(round(\n        pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n      sig_stat[f,6] = mk\n      \n      f = f + 1\n    }\n    \n    \n    if(cl==1){\n      par(mar = c(4,4,2,2))\n      png(filename=paste0(dir,\"/plots/\",texty[pv],\n                          \"/slopelllC\",clu,\".png\"),\n          w = 800, h = 600)\n      plot(das,\n           seq(-.03,.03, length.out=clu),\n           type = \"n\",\n           xlab = \"Clusters\",\n           ylab = \"Slope Coefficient\",\n           xaxt = \"n\",\n           xlim = c(0.5,clu+0.5),\n           ylim = c(-0.03,0.03),\n           cex.lab = 1.5)\n    }\n    \n    if(as.numeric(sapply((\n      subset(ref_st,ref_st[,1]==clu&ref_st[,2]==cl)[sig_tar+4]),\n      as.character))>0.05){\n      rect(cl-.5,-1,cl+.5,1,col = rgb(0.5,0.5,0.5,1/4),\n           border = NA)\n    }\n    # Significant points\n    cl_si = which(sig_stat[,sig_tar]<0.05)\n    points(seq(cl,cl,length.out=length(cl_si)),\n           sig_stat[cl_si,2],\n           pch = 3,cex = 2)\n    # Insignificant points\n    cl_insi = which(sig_stat[,sig_tar]>0.050001)\n    points(seq(cl,cl,length.out=length(cl_insi)),\n           sig_stat[cl_insi,2],\n           pch = 19,cex = 0.5)\n    # Superstation point\n    points(cl,\n           subset(ref_st,ref_st[,1]==clu.f&ref_st[,2]==cl)[6],\n           pch= 18, col = \"red\")\n    \n    abline(h = 0, col = \"gray\", lty = \"dotted\")\n    axis(1, at=cl, labels = cl, tick = T)\n    cl = cl + 1\n  }\n  legend(\"bottomleft\",\n         legend=c(\"Insignificant\",\n                  \"Significant\",\n                  \"Cluster Superstation\"),\n         col=c(\"black\",\"black\",\"red\"),\n         pch=c(19,3,18),\n         pt.cex=c(0.5,2,1),\n         cex = 1.5)\n  dads = as.numeric(sapply(ref_st[which(ref_st[,1]==clu),6],\n                           as.character))\n  points(das,dads[das], pch= 18, col = \"red\")\n  dev.off()\n}",
    "created" : 1508435643189.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "4|29|645|2|\n24|51|29|4|\n36|19|38|4|\n38|10|40|4|\n43|52|297|4|\n117|30|161|6|\n172|30|216|6|\n345|28|389|4|\n400|28|555|4|\n620|5|639|6|\n646|29|981|0|\n681|28|725|4|\n736|28|781|4|\n",
    "hash" : "4018674284",
    "id" : "E2998B8F",
    "lastKnownWriteTime" : 1508729853,
    "last_content_update" : 1508730241520,
    "path" : "C:/Users/Jai/Box Sync/Wind/Florida/latlonlcv.R",
    "project_path" : "latlonlcv.R",
    "properties" : {
        "source_window_id" : ""
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}