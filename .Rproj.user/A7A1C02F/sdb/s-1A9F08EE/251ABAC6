{
    "collab_server" : "",
    "contents" : "# Remove all variables and data\nrm(list = ls())\n\n# Call packages that will be used\n{\n  library(xlsx)\n  library(plyr)\n  library(moments)\n  library(Hmisc)\n  library(rworldmap)\n  library(cluster)\n  library(stringr)\n  library(lubridate)\n  library(Lmoments)\n  library(gamlss)\n  library(extRemes)\n  library(fitdistrplus)\n  library(evd)\n  library(nleqslv)\n  library(XLConnect)\n  library(openxlsx)\n  library(trend)\n  library(Kendall)\n  library(lmomRFA)\n  \n}\n\n# Some useful functions and IATA codes\n{\n  cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n  \n  add_legend <- function(...) {\n    opar <- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n                mar=c(0, 0, 0, 0), new=TRUE)\n    on.exit(par(opar))\n    plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n    legend(...)\n  }\n  \n  rep.row<-function(x,n){\n    matrix(rep(x,each=n),nrow=n)\n  }\n  \n  pop.var <- function(x) var(x) * (length(x)-1) / length(x)\n  \n  pop.sd <- function(x) sqrt(pop.var(x))\n  \n  texty = c(\"comm\",\"nthu\",\"thun\",\"trop\")\n  \n  colors = c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \n             \"#984ea3\", \"#ff7f00\", \"#a65628\",\n             \"#f781bf\", \"#999999\")\n}\n\n# Load IATA codes\n{\n  OS <- Sys.info()['sysname']\n  login <- Sys.info()['login']\n  if(OS == \"Windows\"){\n    if(login == \"Jai\"){\n      # Windows/Home\n      setwd(\"C:/Users/Jai/Box Sync/Wind\") \n      \n    }else if(login != \"Jai\"){\n      # Windows/Office\n      setwd(\"C:/Users/jchung11/Box Sync/Wind\") \n    }\n  }else if(OS != \"Windows\"){\n    # Mac/Air\n    setwd(\"~/Box Sync/Wind\")\n  }\n  iata = read.table(\"codes.txt\",stringsAsFactors = F)\n}\n\n{\n  m <- readline(\"Input 4\n                  \")\n  m <- as.numeric(m)\n  {\n    mi <- readline(\"0 or 20?\n                  \")\n    mi <- as.numeric(mi)\n  }\n  if(mi == 0){\n    mi = \"\"\n  }else {\n    mi = paste(\"_\",mi,\"mi\",sep=\"\")\n  }\n}\n\n## Set data directory\nfor(filii in 1:2){\n  OS <- Sys.info()['sysname']\n  login <- Sys.info()['login']\n  if(OS == \"Windows\"){\n    if(login == \"Jai\"){\n      # Windows/Home\n      dir = setwd(\"C:/Users/Jai/Box Sync/Wind/gust_data\") \n      dir_fl = \"C:/Users/Jai/Box Sync/Wind/Florida/\"\n    }else if(login != \"Jai\"){\n      # Windows/Office\n      dir = setwd(\"C:/Users/jchung11/Box Sync/Wind/gust_data\") \n      dir_fl = \"C:/Users/jchung11/Box Sync/Wind/Florida/\"\n    }\n  }else if(OS != \"Windows\"){\n    # Mac/Air\n    dir = setwd(\"~/Box Sync/Wind/gust_data\")\n    dir_fl = (\"~/Box Sync/Wind/Florida/\")\n  }\n  file = list.files(\n    paste(dir,\"Lower_48\",sep=\"/\"),\n    pattern=paste(\"^station_matrix_\",sep=\"\"))\n}\n\nstations = read.xlsx(paste(\"stations_risk2\",mi,\".xlsx\",sep=\"\"))\nreq_stations = \n  which(as.numeric(substr(file,16,21)) %in% stations[,2])\nreq_file = file[req_stations]\n\n# Tons of empty DFs\n{\n  # DF Sign\n  d_sign = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 4))\n  \n  # DF COV + WIND ANGLE\n  cov = as.data.frame(\n    matrix(, nrow = , ncol = 5))\n  \n  # DF Duplicates\n  d_dup = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 1))\n  \n  # DF Time\n  time_rec = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = ))\n  \n  # DF Significance P-Values\n  sigval = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 8))\n  sigval_type = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 8))\n  sigval.sl = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 4))\n  \n  # DF Indices\n  indices = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 4))\n  \n  # DF MannKendall\n  mk = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 4))\n  mk1 = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 4))\n  \n  # DF SC\n  sc = as.data.frame(\n    matrix(, nrow = length(req_file), ncol =4))\n}\n\nfile_i = 1\ntype_w = 0\n\n# Statistical analysis on Annual Maximums\nwhile(file_i<length(req_file) + 1){ \n  data = read.xlsx(paste(dir,\"Lower_48\",req_file[file_i],sep=\"/\"))\n  colnames(data) <- data[6,]\n  \n  # Delete unnecessary rows until first data\n  data = data[(grep(\"^Date\", \n                    data[,1])+1):length(data[,1]),]\n  \n  data[,1] <-as.POSIXct(as.numeric(data[,1])*24*3600\n                        + as.POSIXct(\"1899-12-30 00:00\"))\n  \n  # Convert strings into numbers\n  data[,2] = as.numeric(data[,2])\n  data[,3] = as.numeric(data[,3])\n  data[,4] = as.numeric(data[,4])\n  \n  while(type_w<4){\n    # Read storm type\n    if(type_w==0){\n      # Commingled\n      data_u <- data\n    }\n    if(type_w==1){\n      # Non-thunderstorm\n      data_u <- data[-c(which(data[,5]!=\"1\")),]\n    }\n    if(type_w==2){\n      # Thunderstorm\n      data_u <- data[-c(which(data[,7]!=\"1\")),]\n    }\n    if(type_w==3){\n      # Tropical Storm\n      data_u <- data[-c(which(data[,10]!=\"1\")),]\n    }\n    \n    # no_YEARS: how many data points are there?\n    no_YEARS = length(unique(year(data_u[,1])))\n    \n    if(no_YEARS < 3){\n      sigval[file_i,2*(type_w)+1] = NaN\n      sigval[file_i,2*(type_w)+2] = NaN\n      \n      sigval_type[file_i,2*(type_w)+1] = NaN\n      sigval_type[file_i,2*(type_w)+2] = NaN\n      \n      # Mann Kendall P-Values\n      mk[file_i,type_w+1] = NaN\n      \n      d_sign[file_i,type_w+1] = NaN\n    }\n    else{\n      source(paste0(dir_fl,\"annmax.R\"))\n      \n      t = used_date - min(used_date)\n      t[which(is.na(used_data))] = NA\n      used_data <- used_data[!is.na(used_data)]\n      t <- t[!is.na(t)]\n      \n      time_rec[file_i,type_w+1] = length(t)\n      \n      # Linear Regression Model\n      sta.lm = lm(used_data~t)\n      \n      if(sta.lm$coefficients[2]>0){\n        d_sign[file_i,type_w+1] = \"P\"\n      }else{\n        d_sign[file_i,type_w+1] = \"N\"\n      }\n      \n      if(all((table(used_data)==1)==T)==F){      \n        d_dup[file_i] = 2\n      }\n      \n      # P-Values for Intercept and Trend Coefficients\n      sigval[file_i,2*(type_w)+1] = format(round(\n        summary(sta.lm)$coefficients[1,4], 4), nsmall = 5)\n      sigval[file_i,2*(type_w)+2] = format(round(\n        summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n      sigval.sl[file_i,type_w+1] = format(round(\n        summary(sta.lm)$coefficients[2], 5), nsmall = 5)\n      \n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[1,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      sigval_type[file_i,2*(type_w)+1] = format(round(alp,5), nsmall = 5)\n      sigval_type[file_i,2*(type_w)+2] = format(round(\n        pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n      \n      # Calculate Serial Correlation\n      nume = 0\n      deno = 0\n      used_n = length(used_data)\n      for(n in 1:(used_n-1)){\n        nume = nume + (used_data[n]-mean(used_data))*\n          (used_data[n+1]-mean(used_data))\n      }\n      for(n in 1:used_n){\n        deno = deno + (used_data[n]-mean(used_data))^2\n      }\n      sc[file_i,type_w+1] = (nume/(used_n-1))/(deno/used_n)\n      \n      # Mann Kendall P-Values\n      mk1[file_i,type_w+1] = format(round(\n        as.numeric(mk.test(ts(used_data, start = used_date[1],\n                              end = used_date[length(used_date)]))[5]),5),\n        nsmall = 5)\n      \n      # Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk[file_i,type_w+1] = 2*(1-pnorm(abs(mk_z)))\n    }\n    type_w = type_w + 1\n  }\n  type_w = 0\n  file_i = file_i + 1\n}\n\nstations_dup = stations[!duplicated(stations[,2]),]\n\n# Manually choose stations\nwanted = as.data.frame(\n  matrix(, nrow = , ncol = ))\n\na = 1\nb = 1\nwhile(b!=3){\n  \n  if(b==1){\n    no = c(30,30,30,7) # Default\n    \n    # Look at a group of stations\n    pv <- readline(\"Input type of data desired:\n                  ======================================\n                  1: Commingled\n                  2: Non-Thunderstorm\n                  3: Thunderstorm\n                  4: Tropical\n                  \")\n    pv <- as.numeric(pv)\n    \n    o <- readline(\"Input minimum number of years:\n                  \")\n    no[pv] = as.numeric(o)\n    \n  }\n  \n  b <- readline(\"Desired data:\n                  ======================================\n                  Input station number or \n                  type 0 to stop recording\n                  type 1 to get all significant stations\n                  type 2 to get ALL stations.\n                  \")\n  b <- as.numeric(b)\n  \n  if(b==0){\n    b = 3\n    super_stations = \n      stations_dup[match(wanted[,1],stations_dup[,1]),]\n  }else if(b==1){\n    b = 3\n    stations_dup = stations[!duplicated(stations[,2]),]\n    stations_dup = cbind(stations_dup,d_sign[,pv],\n                         time_rec[,pv],\n                         mk[,pv],\n                         sigval[,c((pv-1)+pv,2*pv)],\n                         sigval_type[,c((pv-1)+pv,2*pv)],\n                         sigval.sl[,pv],\n                         sc[,pv])\n    sigsta = \n      which((stations_dup[,14]>=no[pv])&(stations_dup[,15]<0.050001))\n    wanted = as.data.frame(stations_dup[sigsta,])\n    super_stations = wanted\n    \n  }else if(b==2){\n    b = 3\n    stations_dup = cbind(stations_dup,d_sign[,pv],\n                         time_rec[,pv],\n                         mk[,pv],\n                         sc[,pv])\n    stations_dup = stations_dup[\n      -c(which(is.na(stations_dup$`sc[, pv]`))), ]\n    stations_dup = stations_dup[\n      -c(which(stations_dup$`time_rec[, pv]`<no[pv])), ]\n    assign(\"lmom\",as.data.frame(\n      matrix(, nrow = length(stations_dup[,1]), ncol = 12)))\n    # stations_dup = cbind(stations_dup,d_sign[,pv],\n    #                      time_rec[,pv],\n    #                      mk[,pv],\n    #                      sigval[,c((pv-1)+pv,2*pv)],\n    #                      sigval_type[,c((pv-1)+pv,2*pv)],\n    #                      sigval.sl[,pv])\n    sc_ind_TF = (stations_dup$`sc[, pv]`<=\n                   (1.96/sqrt(stations_dup$`time_rec[, pv]`)))&\n      (stations_dup$`sc[, pv]`>=\n         (-1.96/sqrt(stations_dup$`time_rec[, pv]`)))\n    sc_ind = which((stations_dup$`sc[, pv]`<=\n                      (1.96/sqrt(stations_dup$`time_rec[, pv]`)))&\n                     (stations_dup$`sc[, pv]`>=\n                        (-1.96/sqrt(stations_dup$`time_rec[, pv]`))))\n    \n    sc_i = 1\n    for (sc_i in 1:length(sc_ind_TF)){\n      data = read.xlsx(\n        paste(dir,\"Lower_48\",\n              paste(\"station_matrix_\",stations_dup[sc_i,1],\n                    \"_update.xlsx\",sep=\"\"),\n              sep=\"/\"))\n      colnames(data) <- data[6,]\n      \n      # Delete unnecessary rows until first data\n      data = data[(grep(\"^Date\", \n                        data[,1])+1):length(data[,1]),]\n      \n      data[,1] <-as.POSIXct(as.numeric(data[,1])*24*3600\n                            + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data[,2] = as.numeric(data[,2])\n      data[,3] = as.numeric(data[,3])\n      \n      # Read storm type\n      if(pv==1){\n        # Commingled\n        data_u <- data\n        text.storm = \"Commingled\"\n      }\n      if(pv==2){\n        # Non-thunderstorm\n        data_u <- data[-c(which(data[,5]!=\"1\")),]\n        text.storm = \"Non-Thunderstorm\"\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_u <- data[-c(which(data[,7]!=\"1\")),]\n        text.storm = \"Thunderstorm\"\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_u <- data[-c(which(data[,10]!=\"1\")),]\n        text.storm = \"Tropical Storm\"\n      }\n      \n      # no_YEARS: how many data points are there?\n      no_YEARS = length(unique(year(data_u[,1])))\n      if(no_YEARS < 4){\n        # sigval[2*pv-1,file_i] = NaN\n        # sigval[2*pv,file_i] = NaN\n        # \n        # sigval_type[2*pv-1,file_i] = NaN\n        # sigval_type[2*pv,file_i] = NaN\n        # \n        # # Mann Kendall P-Values\n        # mk[pv,file_i] = NaN\n        # \n        # d_sign[pv,file_i] = NaN\n      }else{\n        source(paste0(dir_fl,\"annmax.R\"))\n        \n        \n        t = used_date - min(used_date)\n        t[which(is.na(used_data))] = NA\n        used_data <- used_data[!is.na(used_data)]\n        t <- t[!is.na(t)]\n        \n      }\n      # sc_acf = acf(used_data, plot=F)\n      # sc[type_w+1,file_i] = sc_acf$acf[2]\n      # \n      if(sc_ind_TF[sc_i]==T){\n        nume = 0\n        deno = 0\n        used_n = length(used_data)\n        for(n in 1:(used_n-1)){\n          nume = nume + (used_data[n]-mean(used_data))*\n            (used_data[n+1]-mean(used_data))\n        }\n        for(n in 1:used_n){\n          deno = deno + (used_data[n]-mean(used_data))^2\n        }\n        \n        sc.temp = (nume/(used_n-1))/(deno/used_n)\n        \n        # von Storch autocorrelation correction\n        mod_i = 2\n        for (mod_i in 2:length(used_data)){\n          used_data[mod_i] = used_data[mod_i] - \n            sc.temp*used_data[mod_i-1]\n        }\n        \n        # Manual MK\n        # S\n        mk_s = 0\n        mk_j = 1\n        while(mk_j<length(used_data)){\n          mk_i = mk_j + 1\n          while(mk_i<length(used_data)+1){\n            mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n            mk_i = mk_i + 1\n          }\n          mk_j = mk_j + 1\n        }\n        \n        # VARS\n        mk_n = length(used_data)\n        vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n        \n        # ZMK\n        if(vars>0){\n          mk_z = (mk_s-1)/(sqrt(vars))\n        }else if(vars<0){\n          mk_z = (mk_s+1)/(sqrt(vars))\n        }else if(vars==0){\n          mk_z = 0\n        }\n        \n        # Pval\n        stations_dup[sc_i,15] = 2*(1-pnorm(abs(mk_z)))\n      }\n      \n      lmom[sc_i,1] = \n        stations_dup[sc_i,1]\n      lmom[sc_i,2] = \n        length(used_data)\n      lmom[sc_i,3:7] = \n        Lmoments(used_data, rmax = 5)\n      cov[sc_i,1:2] =\n        as.data.frame(cbind(stations_dup[sc_i,1],\n                            mean(used_data)/pop.sd(used_data)))\n      cov[sc_i,3] = mean(used_data)\n      cov[sc_i,4] = mean(as.numeric(year[,4]))\n      cov[sc_i,5] = median(as.numeric(year[,4]))\n        \n    }\n    \n    }else if((b!=0)|(b!=1)|(b!=2)|(b!=3)){\n      if(nchar(b)==6){\n      wanted[a,] = as.matrix(b)\n      a = a + 1  \n    }else{\n      message(\"Stations number must be six digits\")\n    }\n  }\n}\n\ncolnames(stations_dup)[13:16] = c(\"d_sign\",\"timerec\",\"mk\",\"sc\")\n# \n# write.csv(wanted, file = \"trop_f.csv\")\n\nlmom[,8] = lmom[,4]/lmom[,3]\nlmom[,9] = lmom[,5]/lmom[,4]\nlmom[,10] = lmom[,6]/lmom[,4]\nlmom[,11] = lmom[,7]/lmom[,4]\ncolnames(lmom) <- c(\"Station\",\"n\",\"mean\",\"l2\",\n                    \"l3\",\"l4\",\"l5\",\"t\",\"t3\",\"t4\",\"t5\")\n\n# 3 if Mean, 4 if Median\nk_ang = 3\nkdata = as.data.frame(cbind(stations_dup[,1],\n                            stations_dup[,8:10],\n                            cov[,c(2,5)],\n                            lmom[,8:10]))\nk_n = stations_dup[,14]\ncolnames(kdata) = c(\"id\",\"lat\",\"lon\",\"elev\", \n                    \"cov\",\"ang\",\"L-CV\",\"L-Skewness\",\n                    \"L-Kurtosis\")\nkdata.sca = kdata\n\n# Feature Scaling\nfor(i in 2:dim(kdata)[2]){\n  kdata[,i] = as.numeric(kdata[,i])\n  kdata.sca[,i] = (kdata[,i]-min(kdata[,i]))/\n    (max(kdata[,i])-min(kdata[,i]))\n}\n\n# Group variables as string\nvari = colnames(kdata)[2:dim(kdata)[2]]\nwind_vari = vari[c(4,6,7,8)]\npiv_vari = vari[1]\nnpiv_vari = vari[2:8]\n\n# Pivoted Clustering\ncl = floor(sqrt(length(stations_dup[,1])/2))\ncl_m = ceiling(sqrt(length(stations_dup[,1])/2))+1\nwhile(cl<cl_m){\n  d = 1\n  assign(paste(\"maxsos\",cl,sep=\"\"), as.data.frame(\n    matrix(, nrow = 8, ncol = 9)))\n  while(d<length(npiv_vari)){\n    combvar = t(as.data.frame(combn(npiv_vari,d)))\n    assign(paste(\"sos\",d,\"c\",cl,sep=\"\"), as.data.frame(\n      matrix(, nrow = dim(combvar)[1], ncol = dim(combvar)[2]+1)))\n    maxsos = get(paste(\"maxsos\",cl,sep=\"\"))\n    dd = 1\n    while(dd<dim(combvar)[1]+1){\n      sos = get(paste(\"sos\",d,\"c\",cl,sep=\"\"))\n      \n      kmdd = pam(kdata.sca[c(piv_vari,combvar[dd,])],cl)\n      kmdd.sos = summary(silhouette(kmdd))$si.summary[4]\n      \n      sos[dd,] = c(kmdd.sos,\n                   combvar[dd,1:dim(combvar)[2]])\n      sos[,1] = as.numeric(sos[,1])\n      assign(paste(\"sos\",d,\"c\",cl,sep=\"\"),sos)\n      dd = dd + 1\n    }\n    maxsos[d,] = sos[which.max(sos[,1]),]\n    assign(paste(\"maxsos\",cl,sep=\"\"),maxsos)\n    d = d + 1\n  }\n  cl = cl + 1\n}\n\n\n# DF PPCC\nPPCC_ST = as.data.frame(\n  matrix(, nrow = 4, ncol = length(req_file)))\nPPCC_NS = as.data.frame(\n  matrix(, nrow = 4, ncol = length(req_file)))\n\n# Latitude longitude L-CV\nsource(paste0(dir_fl,\"latlonlcv.R\"))\n\n# LLL = rbind(LLL_2_1_30,LLL_2_2_30,\n#             LLL_3_1_30,LLL_3_2_30,LLL_3_3_30)\n# LLL = rbind(LLL_3_1_30,LLL_3_2_30,LLL_3_3_30, \n#             LLL_4_1_30,LLL_4_2_30,LLL_4_3_30,LLL_4_4_30)\n# write.table(LLL, \"clipboard\", sep=\"\\t\", row.names=FALSE, col.names=FALSE)\n\n# Optimal\nsource(paste0(dir_fl,\"optimal.R\"))\n\n\nif(pv==5){\n  \n  ## L-moment diagram \n  # LCV/LSkew\n  if(pv==1){\n    par(mfrow=c(2,2))\n  }\n  if(pv<3){\n    par(mar = c(7,4,2,1))\n  }else{\n    par(mar = c(9,4,0,1))\n  }\n  plot(lmom[,9],lmom[,8],\n       xlim = c(-0.5,0.5), ylim = c(0,0.5),\n       pch = 19, cex = 0.5,\n       xlab = \"L-Skewness\",\n       ylab = \"L-CV\", \n       cex.lab = 1.25,\n       cex.axis = 1.25,\n       type = \"n\")\n  sam.t3 = seq(-1,1,length.out = 100000)\n  lmrd.ln2 = 1.16008*sam.t3 - 0.05325*(sam.t3^2) -\n    0.10501*(sam.t3^4) - 0.00103*(sam.t3^6)\n  lmrd.ga2 = 1.74139*sam.t3 - 2.59736*(sam.t3^3) +\n    2.09911*(sam.t3^4) - 0.35948*(sam.t3^6)\n  lmrd.we2 = 0.17864 + 1.02381*sam.t3 - \n    0.17878*(sam.t3^2) - 0.00894*(sam.t3^4) - \n    0.01443*(sam.t3^6)\n  lmrd.gp2 = 0.33299 + 0.44559*sam.t3 - \n    0.16641*(sam.t3^2) + 0.09111*(sam.t3^5) - \n    0.03625*(sam.t3^7)\n  lines(sam.t3, lmrd.ln2, col = colors[1])\n  lines(sam.t3, lmrd.gp2, col = colors[4])\n  points(lmom[,9],lmom[,8],\n         pch = 19, cex = 0.5)\n  text(-.54,.48,text.storm, cex = 2, pos = 4)\n  \n  par(mfrow=c(1,1))\n  par(mar = c(0,4.1,0,1.1))\n  par(oma=c(0,4,17,4))\n  legend(\"topleft\", \n         c(\"Observations\", \"LN2 Theory\", \n           \"GA2 Theory\", \"WE2 Theory\", \n           \"GP2 Theory\"),\n         col = c(\"black\",colors[1:4]), \n         pch = c(19, NA, NA, NA, NA),\n         lty = c(NA, 1, 1, 1, 1),\n         pt.cex = c(0.25, NA, NA, NA, NA),\n         horiz = T,\n         bty = \"n\")\n  \n  # LKurtosis/LSkew\n  if(pv==1){\n    par(mfrow=c(2,2))\n  }\n  if(pv<3){\n    par(mar = c(7,4,2,1))\n  }else{\n    par(mar = c(9,4,0,1))\n  }\n  plot(lmom[,9],lmom[,10],\n       xlim = c(-0.5,0.5), ylim = c(-0.5,0.5),\n       pch = 19, cex = 0.5,\n       xlab = \"L-Kurtosis\",\n       ylab = \"L-Skewness\", \n       cex.lab = 1.25,\n       cex.axis = 1.25,\n       type = \"n\")\n  sam.t3 = seq(-1,1,length.out = 100000)\n  lmrd.gev = 0.10701 + 0.1109*sam.t3 + \n    0.84838*(sam.t3^2) - 0.06669*(sam.t3^3) + \n    0.00567*(sam.t3^4) - 0.04208*(sam.t3^5) + \n    0.03763*(sam.t3^6)\n  lmrd.ln3 = 0.12282 + 0.77518*(sam.t3^2) + \n    0.12279*(sam.t3^4) - 0.13638*(sam.t3^6) + \n    0.11368*(sam.t3^8)\n  lmrd.pt3 = 0.1224 + 0.30115*(sam.t3^2) + \n    0.95812*(sam.t3^4) - 0.57488*(sam.t3^6) + \n    0.19383*(sam.t3^8)\n  lmrd.glo = 0.16667 + 0.83333*(sam.t3^2)\n  lmrd.gpa = 0.20196*sam.t3 + 0.95924*(sam.t3^2) - \n    0.20096*(sam.t3^3) + 0.4061*(sam.t3^4)\n  lines(sam.t3, lmrd.gev, col = colors[1])\n  lines(sam.t3, lmrd.ln3, col = colors[2])\n  lines(sam.t3, lmrd.pt3, col = colors[3])\n  lines(sam.t3, lmrd.glo, col = colors[4])\n  lines(sam.t3, lmrd.gpa, col = colors[5])\n  points(lmom[,9],lmom[,10],\n         pch = 19, cex = 0.5)\n  text(0,.48,text.storm, cex = 2)\n  \n  par(mfrow=c(1,1))\n  par(mar = c(0,4.1,0,1.1))\n  par(oma=c(0,4,0,4))\n  legend(\"top\", \n         c(\"Observations\", \"GEV Theory\", \n           \"LN3 Theory\", \"PT3 Theory\", \n           \"GLO Theory\", \"GPA Theory\"),\n         col = c(\"black\",colors[1:5]), \n         pch = c(19, NA, NA, NA, NA, NA),\n         lty = c(NA, 1, 1, 1, 1, 1),\n         pt.cex = c(0.25, NA, NA, NA, NA, NA),\n         horiz = F,\n         bty = \"n\",\n         ncol = 3)\n  \n  \n}\n\n\nplot(X_ST,Y_ST, xlab = \"Normal Quantiles\",\n     ylab = \"Ordered Standardized Observations\",\n     type = \"n\", tck = 0.02,\n     xlim = c(-3,3), ylim = c(-3,3))\npoints(X_NS,Y_NS, pch = 21, col = \"gray\",\n       bg = \"gray\", cex = 2)\npoints(X_ST,Y_ST, pch = 20)\nabline(0,1, lwd = 2)\nleg_s = bquote(PPCC[s] == .(corst))\nleg_n = bquote(PPCC[ns] == .(corns))\ntext(-3,2.75,leg_s, pos = 4,\n     bty = \"n\",\n     cex = 1.25)\ntext(-3,2.25,leg_n, pos = 4,\n     bty = \"n\",\n     cex = 1.25)\ntext(3.25,-2.5, text_plot, pos = 2,\n     cex = 1.75)\n\ntype_w = type_w + 1\n\n\n## Real Space\n\nwhile(type_w < 4){\n  \n  ######################################\n  ######################################\n  ######################################\n  if(type_w==0){\n    par(mfrow=c(3,1)) \n    y_min = min(year[,3])\n  }\n\n  \n  lm.year = lm(used_data~t)\n  \n  # Conditional Moment\n  mean_y_w = unname(mean(used_data) + \n                      lm.year$coefficients[2]*(t-mean(t)))\n  \n  X_cor = lm.year$coefficients[2]*sd(t)/sd(ln_used_data)\n  sd_y_w = sqrt((1-X_cor^2)*var(ln_used_data))\n  skew_y_w = skewness(ln_used_data) - \n    lm.year$coefficients[2]^3*skewness(t)\n  \n  Y_ST = sort((ln_used_data - mean(ln_used_data))/sd(ln_used_data))\n  Y_NS = sort((ln_used_data - mean_y_w)/sd_y_w)\n  \n  t_w_ST = t[order((ln_used_data - mean(ln_used_data))/sd(ln_used_data))]\n  t_w_NS = t[order((ln_used_data - mean_y_w)/sd_y_w)]\n  \n  Y_ST = (Y_ST + mean(ln_used_data))/sd(ln_used_data) \n  Y_NS = (Y_NS + mean(ln_used_data) + \n            lm.year$coefficients[2]*(t_w_NS-mean(t)))/sd_y_w\n  p_i = (1:length(ln_used_data)-0.375)/(length(ln_used_data)+0.25)\n  X = qnorm(p_i)\n  X_ST = exp(mean(ln_used_data) + X*sd(ln_used_data))\n  X_NS = exp(mean(ln_used_data) + lm.year$coefficients[2]*(t_w_NS-mean(t)) \n             + X*sd(ln_used_data))\n  corst = round(cor(X_ST,Y_ST),4)\n  corns = round(cor(X_NS,Y_NS),4)\n  \n  if(type_w == 0){\n    par(mfrow=c(2,2), new=TRUE)\n  }\n  \n  plot(X_ST,Y_ST, xlab = \"Normal Quantiles\",\n       ylab = \"Ordered Observations\",\n       type = \"n\", tck = 0.02,\n       xlim = c(0,100), ylim = c(0,100))\n  points(X_NS,Y_NS, pch = 21, col = \"gray\",\n         bg = \"gray\", cex = 2)\n  points(X_ST,Y_ST, pch = 20)\n  abline(0,1, lwd = 2)\n  leg_s = bquote(PPCC[s] == .(corst))\n  leg_n = bquote(PPCC[ns] == .(corns))\n  text(0,95,leg_s, pos = 4,\n       bty = \"n\",\n       cex = 1.25)\n  text(0,80,leg_n, pos = 4,\n       bty = \"n\",\n       cex = 1.25)\n  text(105,5, text_plot, pos = 2,\n       cex = 1.75)\n  \n  type_w = type_w + 1\n}\n\nlegend(\"topleft\",\n       legend = c(\n         substitute(paste(\n           PPCC[st],\" = \",cors),\n           list(cors = corst)),\n         substitute(paste(\n           PPCC[ns],\" = \",corn),\n           list(corn = corns))),\n       bty = \"n\",\n       cex = 1.25)\n\nlegend(\"topleft\",\n       legend = c(\n         substitute(paste(\n           PPCC[st],\" = \",cors),\n           list(cors = corst)),\n         substitute(paste(\n           PPCC[ns],\" = \",corn),\n           list(corn = corns))),\n       bty = \"n\",\n       cex = 1.25)\n\nlegend('bottomright',\n       legend = c(\"Theoretical\",\n                  \"Nonstationary\",\n                  \"Stationary\"),\n       lwd = c(2,NA,NA),lty = c(1,NA,NA),\n       pch = c(NA,21,20),pt.cex = c(NA,2,1),\n       col = c(\"black\",\"gray\",\"black\"),\n       pt.bg = c(NA,\"gray\",\"black\"))",
    "created" : 1508734419827.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1515981103",
    "id" : "251ABAC6",
    "lastKnownWriteTime" : 1508726881,
    "last_content_update" : 1508726881,
    "path" : "C:/Users/Jai/Box Sync/Wind/Florida/main.R",
    "project_path" : "main.R",
    "properties" : {
    },
    "relative_order" : 14,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}