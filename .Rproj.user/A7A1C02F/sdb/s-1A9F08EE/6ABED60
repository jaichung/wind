{
    "collab_server" : "",
    "contents" : "# Remove all variables and data\nrm(list = ls())\n\n# Call packages that will be used\n{\n  library(xlsx)\n  library(plyr)\n  library(moments)\n  library(Hmisc)\n  library(rworldmap)\n  library(cluster)\n  library(stringr)\n  library(lubridate)\n  library(Lmoments)\n  library(gamlss)\n  library(extRemes)\n  library(fitdistrplus)\n  library(evd)\n  library(nleqslv)\n  library(XLConnect)\n  library(openxlsx)\n  library(trend)\n  library(Kendall)\n  library(lmomRFA)\n  \n  add_legend <- function(...) {\n    opar <- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), \n                mar=c(0, 0, 0, 0), new=TRUE)\n    on.exit(par(opar))\n    plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')\n    legend(...)\n  }\n  \n  rep.row<-function(x,n){\n    matrix(rep(x,each=n),nrow=n)\n  }\n  \n  pop.var <- function(x) var(x) * (length(x)-1) / length(x)\n  \n  pop.sd <- function(x) sqrt(pop.var(x))\n  \n  texty = c(\"comm\",\"nthu\",\"thun\",\"trop\")\n  \n  colors = c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \n             \"#984ea3\", \"#ff7f00\", \"#a65628\",\n             \"#f781bf\", \"#999999\")\n}\n\n# Load IATA codes\n{\n  OS <- Sys.info()['sysname']\n  login <- Sys.info()['login']\n  if(OS == \"Windows\"){\n    if(login == \"Jai\"){\n      # Windows/Home\n      setwd(\"C:/Users/Jai/Box Sync/Wind\") \n      \n    }else if(login != \"Jai\"){\n      # Windows/Office\n      setwd(\"C:/Users/jchung11/Box Sync/Wind\") \n    }\n  }else if(OS != \"Windows\"){\n    # Mac/Air\n    setwd(\"~/Box Sync/Wind\")\n  }\n  iata = read.table(\"codes.txt\",stringsAsFactors = F)\n}\n\n{\n  m <- readline(\"Input 4\n                  \")\n  m <- as.numeric(m)\n  {\n    mi <- readline(\"0 or 20?\n                  \")\n    mi <- as.numeric(mi)\n  }\n  if(mi == 0){\n    mi = \"\"\n  }else {\n    mi = paste(\"_\",mi,\"mi\",sep=\"\")\n  }\n}\n\n## Set data directory\nfor(filii in 1:2){\n  OS <- Sys.info()['sysname']\n  login <- Sys.info()['login']\n  if(OS == \"Windows\"){\n    if(login == \"Jai\"){\n      # Windows/Home\n      dir = setwd(\"C:/Users/Jai/Box Sync/Wind/gust_data\") \n    }else if(login != \"Jai\"){\n      # Windows/Office\n      dir = setwd(\"C:/Users/jchung11/Box Sync/Wind/gust_data\") \n    }\n  }else if(OS != \"Windows\"){\n    # Mac/Air\n    dir = setwd(\"~/Box Sync/Wind/gust_data\")\n  }\n  file = list.files(\n    paste(dir,\"Lower_48\",sep=\"/\"),\n    pattern=paste(\"^station_matrix_\",sep=\"\"))\n}\n\nstations = read.xlsx(paste(\"stations_risk2\",mi,\".xlsx\",sep=\"\"))\nreq_stations = \n  which(as.numeric(substr(file,16,21)) %in% stations[,2])\nreq_file = file[req_stations]\n\n# Tons of empty DFs\n{\n  # DF Sign\n  d_sign = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n  \n  # DF COV + WIND ANGLE\n  cov = as.data.frame(\n    matrix(, nrow = , ncol = 5))\n  \n  # DF Duplicates\n  d_dup = as.data.frame(\n    matrix(, nrow = 1, ncol = length(req_file)))\n  \n  # DF Time\n  time_rec = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n  \n  # DF Significance P-Values\n  sigval = as.data.frame(\n    matrix(, nrow = 8, ncol = length(req_file)))\n  sigval_type = as.data.frame(\n    matrix(, nrow = 8, ncol = length(req_file)))\n  sigval.sl = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n  \n  # DF Indices\n  indices = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 4))\n  \n  # DF MannKendall\n  mk = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n  mk1 = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n  \n  # DF SC\n  sc = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n}\n\nfile_i = 1\ntype_w = 0\n\n# Analysis\nwhile(file_i<length(req_file) + 1){ \n  data = read.xlsx(paste(dir,\"Lower_48\",req_file[file_i],sep=\"/\"))\n  colnames(data) <- data[6,]\n  \n  # Delete unnecessary rows until first data\n  data = data[(grep(\"^Date\", \n                    data[,1])+1):length(data[,1]),]\n  \n  data[,1] <-as.POSIXct(as.numeric(data[,1])*24*3600\n                        + as.POSIXct(\"1899-12-30 00:00\"))\n  \n  # Convert strings into numbers\n  data[,2] = as.numeric(data[,2])\n  data[,3] = as.numeric(data[,3])\n  data[,4] = as.numeric(data[,4])\n  \n  while(type_w<4){\n    # Read storm type\n    if(type_w==0){\n      # Commingled\n      data_u <- data\n    }\n    if(type_w==1){\n      # Non-thunderstorm\n      data_u <- data[-c(which(data[,5]!=\"1\")),]\n      \n    }\n    if(type_w==2){\n      # Thunderstorm\n      data_u <- data[-c(which(data[,7]!=\"1\")),]\n    }\n    if(type_w==3){\n      # Tropical Storm\n      data_u <- data[-c(which(data[,10]!=\"1\")),]\n    }\n    \n    # no_YEARS: how many data points are there?\n    no_YEARS = length(unique(year(data_u[,1])))\n    \n    if(no_YEARS < 3){\n      sigval[2*(type_w)+1,file_i] = NaN\n      sigval[2*(type_w)+2,file_i] = NaN\n      \n      sigval_type[2*(type_w)+1,file_i] = NaN\n      sigval_type[2*(type_w)+2,file_i] = NaN\n      \n      # Mann Kendall P-Values\n      mk[type_w+1,file_i] = NaN\n      \n      d_sign[type_w+1,file_i] = NaN\n    }\n    else{\n      i = 1\n      j = 1\n      k = 1\n      year = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n      data_u[,14] = year(data_u[,1])\n      while(k<(no_YEARS)){\n        # Identify which data row is the last one\n        # from all of the year n\n        while(data_u[i + j - 1,14] == data_u[i + j,14]){\n          j = j + 1\n        }\n        j = i + j - 1\n        \n        if(m == 1){\n          # Mean\n          year[k,1] = sum(data_u[i:j,3])/length(i:j)\n          type = \"Mean\"\n        }else if(m == 2){\n          # Median\n          year[k,1] = median(data_u[i:j,3]) \n          type = \"Median\"\n        }else if(m == 3){\n          # Coefficient of Variation\n          year[k,1] = sd(data_u[i:j,3])/\n            (sum(data_u[i:j,3])/length(i:j)) \n          type = \"Coefficient of Variation\"\n        }else if(m == 4){\n          # Maximum\n          year[k,1] = max(data_u[i:j,3])\n          year[k,2] = data_u[i,1]\n          year[k,3] = year(data_u[i,1])\n          year[k,4] = data[match(max(data_u[i:j,3]),\n                                 data_u[,3]),4]\n          type = \"Maximum\"\n        }else {\n          # Frequency\n          year[k,1] = length(i:j)\n          type = \"Frequency\"\n        }\n        \n        # i variable is added by 1\n        # to move on to the next year\n        i = j + 1\n        j = 1\n        k = k + 1\n        if(k == (no_YEARS)){\n          last = length(data_u[,3])\n          if(m == 1){\n            year[k,1] = sum(data_u[i:last,3])/length(i:last)\n          }else if(m == 2){\n            year[k,1] = median(data_u[i:last,3])\n          }else if(m == 3){\n            year[k,1] = sd(data_u[i:last,3])/\n              (sum(data_u[i:last,3])/length(i:last)) \n          }else if(m == 4){\n            year[k,1] = max(data_u[i:last,3])\n            year[k,2] = data_u[last,1]\n            year[k,3] = year(data_u[last,1])\n            year[k,4] = data[match(max(data_u[i:last,3]),\n                                   data_u[,3]),4]\n          }else{\n            year[k,1] = length(i:last)\n          }\n          k = k + 1\n        }\n      }\n      used_data = log(year[,1])\n      used_date = year[,3]\n    \n      t = used_date - min(used_date)\n      t[which(is.na(used_data))] = NA\n      used_data <- used_data[!is.na(used_data)]\n      t <- t[!is.na(t)]\n      \n      time_rec[type_w+1,file_i] = length(t)\n      \n      # Linear Regression Model\n      sta.lm = lm(used_data~t)\n      \n      if(sta.lm$coefficients[2]>0){\n        d_sign[type_w+1,file_i] = \"P\"\n        }else{\n        d_sign[type_w+1,file_i] = \"N\"\n      }\n      \n      if(all((table(used_data)==1)==T)==F){      \n        d_dup[file_i] = 2\n      }\n      \n      # P-Values for Intercept and Trend Coefficients\n      sigval[2*(type_w)+1,file_i] = format(round(\n        summary(sta.lm)$coefficients[1,4], 4), nsmall = 5)\n      sigval[2*(type_w)+2,file_i] = format(round(\n        summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n      sigval.sl[type_w,file_i] = format(round(\n        sta.lm$coefficients[2], 5), nsmall = 5)\n      \n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[1,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      sigval_type[2*(type_w)+1,file_i] = format(round(alp,5), nsmall = 5)\n      sigval_type[2*(type_w)+2,file_i] = format(round(\n        pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n      sc_acf = acf(used_data, plot=F)\n      sc[type_w+1,file_i] = sc_acf$acf[2]\n      \n      # Mann Kendall P-Values\n      mk1[type_w+1,file_i] = format(round(\n        as.numeric(mk.test(ts(used_data, start = used_date[1],\n                              end = used_date[length(used_date)]))[5]),5),\n        nsmall = 5)\n\n      # Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk[type_w+1,file_i] = 2*(1-pnorm(abs(mk_z)))\n    }\n    type_w = type_w + 1\n  }\n  type_w = 0\n  file_i = file_i + 1\n}\n\n# Transpose and more QCQA\nsigval = t(sigval)\nsigval_type = t(sigval_type)\nsigval.sl = t(sigval.sl)\nmk = t(mk)\nsc = t(sc)\ntime_rec = t(time_rec)\nd_sign = t(d_sign)\n\nstations_dup = stations[!duplicated(stations[,2]),]\n\n# Manually choose stations\nwanted = as.data.frame(\n  matrix(, nrow = , ncol = ))\n\na = 1\nb = 1\nwhile(b!=3){\n  \n  if(b==1){\n    no = c(30,30,30,7) # Default\n    \n    # Look at a group of stations\n    pv <- readline(\"Input type of data desired:\n                  ======================================\n                  1: Commingled\n                  2: Non-Thunderstorm\n                  3: Thunderstorm\n                  4: Tropical\n                  \")\n    pv <- as.numeric(pv)\n    \n    o <- readline(\"Input minimum number of years:\n                  \")\n    no[pv] = as.numeric(o)\n    \n  }\n  \n  b <- readline(\"Desired data:\n                  ======================================\n                  Input station number or \n                  type 0 to stop recording\n                  type 1 to get all significant stations\n                  type 2 to get ALL stations.\n                  \")\n  b <- as.numeric(b)\n  \n  if(b==0){\n    b = 3\n    super_stations = \n      stations_dup[match(wanted[,1],stations_dup[,1]),]\n  }else if(b==1){\n    b = 3\n    stations_dup = stations[!duplicated(stations[,2]),]\n    stations_dup = cbind(stations_dup,d_sign[,pv],\n                         time_rec[,pv],\n                         mk[,pv],\n                         sigval[,c((pv-1)+pv,2*pv)],\n                         sigval_type[,c((pv-1)+pv,2*pv)],\n                         sigval.sl[,pv])\n    sigsta = \n      which((stations_dup[,14]>=no[pv])&(stations_dup[,15]<0.050001))\n    wanted = as.data.frame(stations_dup[sigsta,])\n    super_stations = wanted\n    \n  }else if(b==2){\n    b = 3\n    stations_dup = cbind(stations_dup,d_sign[,pv],\n                         time_rec[,pv],\n                         mk[,pv])\n    # stations_dup = cbind(stations_dup,d_sign[,pv],\n    #                      time_rec[,pv],\n    #                      mk[,pv],\n    #                      sigval[,c((pv-1)+pv,2*pv)],\n    #                      sigval_type[,c((pv-1)+pv,2*pv)],\n    #                      sigval.sl[,pv])\n    sigsta = \n      which(stations_dup[,14]>=no[pv])\n    sigsta = which((stations_dup[,14]>=no[pv])&\n                     (sc[,pv]<=1.96/sqrt(time_rec[,pv]))&\n                     (sc[,pv]>=-1.96/sqrt(time_rec[,pv]))&\n                     (mk[,pv]<0.050001))\n    wanted = as.data.frame(stations_dup[sigsta,])\n    super_stations = wanted\n    }else if((b!=0)|(b!=1)|(b!=2)|(b!=3)){\n      if(nchar(b)==6){\n      wanted[a,] = as.matrix(b)\n      a = a + 1  \n    }else{\n      message(\"Stations number must be six digits\")\n    }\n  }\n}\n\n\n\n# \n# write.csv(wanted, file = \"trop_f.csv\")\n\nassign(\"lmom\",as.data.frame(\n  matrix(, nrow = dim(wanted)[1], ncol = 12)))\n\nfile_i = 1\nwhile(file_i<dim(wanted)[1] + 1){ \n  \n  dis_ang = read.xlsx(paste(dir,\"/risk2_\",texty[pv],\n                            \"_proj.xlsx\",sep=\"\"))\n  data = read.xlsx(\n    paste(dir,\"Lower_48\",\n          paste(\"station_matrix_\",wanted[file_i,1],\n                \"_update.xlsx\",sep=\"\"),\n          sep=\"/\"))\n  colnames(data) <- data[6,]\n  \n  # Delete unnecessary rows until first data\n  data = data[(grep(\"^Date\", \n                    data[,1])+1):length(data[,1]),]\n  \n  data[,1] <-as.POSIXct(as.numeric(data[,1])*24*3600\n                        + as.POSIXct(\"1899-12-30 00:00\"))\n  \n  # Convert strings into numbers\n  data[,2] = as.numeric(data[,2])\n  data[,3] = as.numeric(data[,3])\n  \n  # Read storm type\n  if(pv==1){\n    # Commingled\n    data_u <- data\n    text.storm = \"Commingled\"\n  }\n  if(pv==2){\n    # Non-thunderstorm\n    data_u <- data[-c(which(data[,5]!=\"1\")),]\n    text.storm = \"Non-Thunderstorm\"\n  }\n  if(pv==3){\n    # Thunderstorm\n    data_u <- data[-c(which(data[,7]!=\"1\")),]\n    text.storm = \"Thunderstorm\"\n  }\n  if(pv==4){\n    # Tropical Storm\n    data_u <- data[-c(which(data[,10]!=\"1\")),]\n    text.storm = \"Tropical Storm\"\n  }\n  \n  # no_YEARS: how many data points are there?\n  no_YEARS = length(unique(year(data_u[,1])))\n  if(no_YEARS < 4){\n    # sigval[2*pv-1,file_i] = NaN\n    # sigval[2*pv,file_i] = NaN\n    # \n    # sigval_type[2*pv-1,file_i] = NaN\n    # sigval_type[2*pv,file_i] = NaN\n    # \n    # # Mann Kendall P-Values\n    # mk[pv,file_i] = NaN\n    # \n    # d_sign[pv,file_i] = NaN\n  }\n  else{\n    i = 1\n    j = 1\n    k = 1\n    year = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n    data_u[,14] = year(data_u[,1])\n    while(k<(no_YEARS)){\n      # Identify which data row is the last one\n      # from all of the year n\n      while(data_u[i + j - 1,14] == data_u[i + j,14]){\n        j = j + 1\n      }\n      j = i + j - 1\n      \n      if(m == 1){\n        # Mean\n        year[k,1] = sum(data_u[i:j,3])/length(i:j)\n        type = \"Mean\"\n      }else if(m == 2){\n        # Median\n        year[k,1] = median(data_u[i:j,3]) \n        type = \"Median\"\n      }else if(m == 3){\n        # Coefficient of Variation\n        year[k,1] = sd(data_u[i:j,3])/\n          (sum(data_u[i:j,3])/length(i:j)) \n        type = \"Coefficient of Variation\"\n      }else if(m == 4){\n        # Maximum\n        year[k,1] = max(data_u[i:j,3])\n        year[k,2] = data_u[i,1]\n        year[k,3] = year(data_u[i,1])\n        year[k,4] = data[match(max(data_u[i:j,3]),\n                               data_u[,3]),4]\n        type = \"Maximum\"\n      }else{\n        # Frequency\n        year[k,1] = length(i:j)\n        type = \"Frequency\"\n      }\n      \n      # i variable is added by 1\n      # to move on to the next year\n      i = j + 1\n      j = 1\n      k = k + 1\n      if(k == (no_YEARS)){\n        last = length(data_u[,3])\n        if(m == 1){\n          year[k,1] = sum(data_u[i:last,3])/length(i:last)\n        }else if(m == 2){\n          year[k,1] = median(data_u[i:last,3])\n        }else if(m == 3){\n          year[k,1] = sd(data_u[i:last,3])/\n            (sum(data_u[i:last,3])/length(i:last)) \n        }else if(m == 4){\n          year[k,1] = max(data_u[i:last,3])\n          year[k,2] = data_u[last,1]\n          year[k,3] = year(data_u[last,1])\n          year[k,4] = data[match(max(data_u[i:last,3]),\n                                 data_u[,3]),4]\n        }else{\n          year[k,1] = length(i:last)\n        }\n        k = k + 1\n      }\n    }\n    used_data = log(year[,1])\n    used_date = year[,2]\n    \n    t = used_date - min(used_date)\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n  }\n  # if(file_i == 1){\n  #   if(pv!=4){\n  #     ns = 15:45\n  #   }else{\n  #     ns = 4:24\n  #   }\n  #   acf_ns = (-1+1.645*sqrt(ns-2))/(ns-1)\n  #   plot(ns, acf_ns, ylim = c(0,1), xlim = c(20,40), \n  #        type = \"n\", xlab = \"Period of record, n\",\n  #        ylab = \"ACF\")\n  #   lines(ns,acf_ns)\n  # }\n  if(no_YEARS>=4){\n    used_data = log(used_data)\n    lmom[file_i,1] = \n      super_stations[file_i,1]\n    lmom[file_i,2] = \n      length(used_data)\n    lmom[file_i,3:7] = \n      Lmoments(used_data, rmax = 5)\n    cov[file_i,1:2] =\n      as.data.frame(cbind(super_stations[file_i,1],\n                          mean(used_data)/pop.sd(used_data)))\n    cov[file_i,3] = mean(used_data)\n    cov[file_i,4] = mean(as.numeric(year[,4]))\n    cov[file_i,5] = median(as.numeric(year[,4]))\n    # acf_ns = acf(year[,1], plot = F)\n    # points(length(year[,1]),acf_ns$acf[2], pch = 19)\n  }\n  file_i = file_i + 1\n}\n\n# write.csv(stations_dup,\n#           file = paste(\"risk2\",mi,\"_\",sig,pv,\".csv\",sep=\"\"))\nlmom[,8] = lmom[,4]/lmom[,3]\nlmom[,9] = lmom[,5]/lmom[,4]\nlmom[,10] = lmom[,6]/lmom[,4]\nlmom[,11] = lmom[,7]/lmom[,4]\ncolnames(lmom) <- c(\"Station\",\"n\",\"mean\",\"l2\",\n                    \"l3\",\"l4\",\"l5\",\"t\",\"t3\",\"t4\",\"t5\")\n\n\nif(pv==5){\n\n## L-moment diagram \n# LCV/LSkew\nif(pv==1){\n  par(mfrow=c(2,2))\n  }\nif(pv<3){\n  par(mar = c(7,4,2,1))\n}else{\n  par(mar = c(9,4,0,1))\n}\nplot(lmom[,9],lmom[,8],\n     xlim = c(-0.5,0.5), ylim = c(0,0.5),\n     pch = 19, cex = 0.5,\n     xlab = \"L-Kurtosis\",\n     ylab = \"L-CV\", \n     cex.lab = 1.25,\n     cex.axis = 1.25,\n     type = \"n\")\nsam.t3 = seq(-1,1,length.out = 100000)\nlmrd.ln2 = 1.16008*sam.t3 - 0.05325*(sam.t3^2) -\n  0.10501*(sam.t3^4) - 0.00103*(sam.t3^6)\nlmrd.ga2 = 1.74139*sam.t3 - 2.59736*(sam.t3^3) +\n  2.09911*(sam.t3^4) - 0.35948*(sam.t3^6)\nlmrd.we2 = 0.17864 + 1.02381*sam.t3 - \n  0.17878*(sam.t3^2) - 0.00894*(sam.t3^4) - \n  0.01443*(sam.t3^6)\nlmrd.gp2 = 0.33299 + 0.44559*sam.t3 - \n  0.16641*(sam.t3^2) + 0.09111*(sam.t3^5) - \n  0.03625*(sam.t3^7)\nlines(sam.t3, lmrd.ln2, col = colors[1])\nlines(sam.t3, lmrd.ga2, col = colors[2])\nlines(sam.t3, lmrd.we2, col = colors[3])\nlines(sam.t3, lmrd.gp2, col = colors[4])\npoints(lmom[,9],lmom[,8],\n       pch = 19, cex = 0.5)\ntext(-.54,.48,text.storm, cex = 2, pos = 4)\n\npar(mfrow=c(1,1))\npar(mar = c(0,4.1,0,1.1))\npar(oma=c(0,4,17,4))\nlegend(\"bottom\", \n       c(\"Observations\", \"LN2 Theory\", \n         \"GA2 Theory\", \"WE2 Theory\", \n         \"GP2 Theory\"),\n       col = c(\"black\",colors[1:4]), \n       pch = c(19, NA, NA, NA, NA),\n       lty = c(NA, 1, 1, 1, 1),\n       pt.cex = c(0.25, NA, NA, NA, NA),\n       horiz = T,\n       bty = \"n\")\n\n# LKurtosis/LSkew\nif(pv==1){\n  par(mfrow=c(2,2))\n}\nif(pv<3){\n  par(mar = c(7,4,2,1))\n}else{\n  par(mar = c(9,4,0,1))\n}\nplot(lmom[,9],lmom[,10],\n     xlim = c(-0.5,0.5), ylim = c(0,0.5),\n     pch = 19, cex = 0.5,\n     xlab = \"L-Kurtosis\",\n     ylab = \"L-Skewness\", \n     cex.lab = 1.25,\n     cex.axis = 1.25,\n     type = \"n\")\nsam.t3 = seq(-1,1,length.out = 100000)\nlmrd.gev = 0.10701 + 0.1109*sam.t3 + \n  0.84838*(sam.t3^2) - 0.06669*(sam.t3^3) + \n  0.00567*(sam.t3^4) - 0.04208*(sam.t3^5) + \n  0.03763*(sam.t3^6)\nlmrd.ln3 = 0.12282 + 0.77518*(sam.t3^2) + \n  0.12279*(sam.t3^4) - 0.13638*(sam.t3^6) + \n  0.11368*(sam.t3^8)\nlmrd.pt3 = 0.1224 + 0.30115*(sam.t3^2) + \n  0.95812*(sam.t3^4) - 0.57488*(sam.t3^6) + \n  0.19383*(sam.t3^8)\nlmrd.glo = 0.16667 + 0.83333*(sam.t3^2)\nlmrd.gpa = 0.20196*sam.t3 + 0.95924*(sam.t3^2) - \n  0.20096*(sam.t3^3) + 0.4061*(sam.t3^4)\nlines(sam.t3, lmrd.gev, col = colors[1])\nlines(sam.t3, lmrd.ln3, col = colors[2])\nlines(sam.t3, lmrd.pt3, col = colors[3])\nlines(sam.t3, lmrd.glo, col = colors[4])\nlines(sam.t3, lmrd.gpa, col = colors[5])\npoints(lmom[,9],lmom[,10],\n       pch = 19, cex = 0.5)\ntext(0,.48,text.storm, cex = 2)\n\npar(mfrow=c(1,1))\npar(mar = c(0,4.1,0,1.1))\npar(oma=c(0,4,0,4))\nlegend(\"bottom\", \n       c(\"Observations\", \"GEV Theory\", \n         \"LN3 Theory\", \"PT3 Theory\", \n         \"GLO Theory\", \"GPA Theory\"),\n       col = c(\"black\",colors[1:5]), \n       pch = c(19, NA, NA, NA, NA, NA),\n       lty = c(NA, 1, 1, 1, 1, 1),\n       pt.cex = c(0.25, NA, NA, NA, NA, NA),\n       horiz = F,\n       bty = \"n\",\n       ncol = 3)\n\n\n}\n\n\n\n\ndis_ang_sup = \n  dis_ang[match(super_stations[,1],dis_ang[,2]),\n          c(tail(1:dim(dis_ang)[2],2))]\n\n# 3 if Mean, 4 if Median\nk_ang = 3\nkdata = as.data.frame(cbind(super_stations$Field1,\n                            super_stations$LAT,\n                            super_stations$LON,\n                            as.numeric(as.character(\n                              super_stations$ELEV_M_)),\n                            cov[match(super_stations$Field1,cov[,1]),2],\n                            cov[match(super_stations$Field1,cov[,1]),k_ang],\n                            lmom$t[match(super_stations$Field1,lmom[,1])],\n                            lmom$t3[match(super_stations$Field1,lmom[,1])],\n                            lmom$t4[match(super_stations$Field1,lmom[,1])],\n                            dis_ang_sup))\ncolnames(kdata) = c(\"id\",\"lat\",\"lon\",\"elev\", \n                    \"cov\",\"ang\",\"L-CV\",\"L-Skewness\",\n                    \"L-Kurtosis\",\"d\",\"staang\")\nkdata.sca = kdata\n\n# Feature Scaling\ni = 1\nwhile(i<dim(kdata)[2]){\n  kdata.sca[,i+1] = (kdata[,i+1]-min(kdata[,i+1]))/\n    (max(kdata[,i+1])-min(kdata[,i+1]))\n  i = i + 1\n}\n\nvari = colnames(kdata)[2:dim(kdata)[2]]\nwind_vari = vari[c(4,6,7,8)]\npiv_vari = vari[1]\nnpiv_vari = vari[2:10]\n\n# Pivoted Clustering\ncl = floor(sqrt(dim(wanted)[1]/2))\ncl_m = ceiling(sqrt(dim(wanted)[1]/2))+1\nwhile(cl<cl_m){\n  d = 1\n  assign(paste(\"maxsos\",cl,sep=\"\"), as.data.frame(\n    matrix(, nrow = 8, ncol = 9)))\n  while(d<length(npiv_vari)){\n    combvar = t(as.data.frame(combn(npiv_vari,d)))\n    assign(paste(\"sos\",d,\"c\",cl,sep=\"\"), as.data.frame(\n      matrix(, nrow = dim(combvar)[1], ncol = dim(combvar)[2]+1)))\n    maxsos = get(paste(\"maxsos\",cl,sep=\"\"))\n    dd = 1\n      while(dd<dim(combvar)[1]+1){\n        sos = get(paste(\"sos\",d,\"c\",cl,sep=\"\"))\n        \n        kmdd = pam(kdata.sca[c(piv_vari,combvar[dd,])],cl)\n        kmdd.sos = summary(silhouette(kmdd))$si.summary[4]\n        \n        sos[dd,] = c(kmdd.sos,\n                     combvar[dd,1:dim(combvar)[2]])\n        sos[,1] = as.numeric(sos[,1])\n        assign(paste(\"sos\",d,\"c\",cl,sep=\"\"),sos)\n        dd = dd + 1\n      }\n    maxsos[d,] = sos[which.max(sos[,1]),]\n    assign(paste(\"maxsos\",cl,sep=\"\"),maxsos)\n    d = d + 1\n  }\n  cl = cl + 1\n}\n\n\n\n# Lat, Lon, L-CV\ncl.tabl = as.data.frame(\n  matrix(, nrow = , ncol = 12))\nclu.f = floor(sqrt(dim(wanted)[1]/2))\nfor (clu in clu.f:(clu.f+1)){\n  clusters = pam(kdata.sca[,c(2,3,7)], clu)\n  {\n    par(mar=c(2,2,2,2))\n    png(file=paste0(dir,\"/plots/\",texty[pv],\n                           \"/latlonlcv_fff\",clu,\".png\"),\n        w = 800, h = 600)\n    mappoint= cbind(dis_ang[as.numeric(rownames(kdata)),c(9,10)],\n                  clusters$clustering)\n    newmap <- getMap(resolution = \"low\")\n    # windows(800, 600, pointsize = 12)\n    plot(newmap, xlim = c(-80, -65), ylim = c(25, 45), asp = 1)\n    legend(-63,30,legend = c(1:clu,\"Discordant\"),\n           bg = \"white\",col=c(colors[1:clu],\"black\"), \n           bty = \"n\", pch = c(rep(20,clu),1), \n           pt.cex = c(rep(3,clu),6), \n           pt.lwd = c(rep(1,clu),3),\n           ncol = 1, cex = 1.5)\n\n    ll = 1\n    while(ll<length(colnames(clusters$medoids))+1){\n      text(-63,42.5-ll*1.5,\n           capitalize(colnames(clusters$medoids)[ll]),\n           cex = 2.5)\n      ll = ll + 1\n    }\n    \n    text(-63,35,\n         paste0(\"Si: \",summary(silhouette(clusters))$si.summary[4]),\n         cex = 2.5)\n    \n    cl = 1\n    if(clu!=clu.f){\n      dimc = dim(cl.tabl)[1]\n    }else{\n      dimc = 0\n    }\n    while(cl<length(unique(clusters$clustering))+1){\n      cl.index = which(as.numeric(clusters$clustering)==cl)\n      dm.table = cbind(kdata[cl.index,1],\n                       wanted[cl.index,14],\n                       cov[cl.index,3],\n                       lmom[cl.index,8:11])\n      colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n      dm.test = regtst(dm.table[,1:6],5000)\n      is.dm.ex = \n        length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n      n.good.fit = length(which(abs(dm.test$Z)<1.64))\n      \n      # Check if there are discordant stations\n      if(is.dm.ex>0){\n        dm.sta = \n          dm.table[which(as.numeric(dm.test$D)>dm.test$Dcrit[1]),1]\n        dm.point = cbind(dis_ang[match(dm.sta,dis_ang[,2]),c(9,10)],cl)\n        points(dm.point$LON,dm.point$LAT,\n               col=colors[cl], cex=6, pch = 1,\n               lwd = 3)\n      }\n      \n      if(clu==clu.f){\n        cl.tabl[cl,1] = clu\n        cl.tabl[cl,2] = cl\n        cl.tabl[cl,3] = length(dm.test$D)\n        cl.tabl[cl,4] = \n          length(which(dm.test$D>dm.test$Dcrit[1]))\n        cl.tabl[cl,5] = dm.test$H[1]\n        n.g = 1\n        if(n.good.fit!=0){\n          for(n.g in 1:n.good.fit){\n            cl.tabl[cl,11+(2*n.g)-1] = sort(abs(dm.test$Z))[n.g]\n            cl.tabl[cl,11+(2*n.g)] = attributes(sort(abs(dm.test$Z))[n.g])\n          }\n        }else{\n          cl.tabl[cl,12] = sort(abs(abs(dm.test$Z)-1.64))[1] + 1.64\n          cl.tabl[cl,13] = attributes(sort(abs(abs(dm.test$Z)-1.64))[1])\n        }\n      }else{\n        cl.tabl[dimc + cl,1] = clu\n        cl.tabl[dimc + cl,2] = cl\n        cl.tabl[dimc + cl,3] = length(dm.test$D)\n        cl.tabl[dimc + cl,4] = \n          length(which(dm.test$D>dm.test$Dcrit[1]))\n        cl.tabl[dimc + cl,5] = dm.test$H[1]\n        n.g = 1\n        if(n.good.fit!=0){\n          for(n.g in 1:n.good.fit){\n            cl.tabl[dimc + cl,11+(2*n.g)-1] = sort(abs(dm.test$Z))[n.g]\n            cl.tabl[dimc + cl,11+(2*n.g)] = attributes(sort(abs(dm.test$Z))[n.g])\n          }\n        }else{\n          cl.tabl[dimc + cl,12] = sort(abs(abs(dm.test$Z)-1.64))[1] + 1.64\n          cl.tabl[dimc + cl,13] = attributes(sort(abs(abs(dm.test$Z)-1.64))[1])\n        }\n      }\n  \n      f = 1\n      \n      ## Obtain desired data\n      file = paste0(\"station_matrix_\",\n                    kdata.sca[which(clusters$clustering==cl),1],\n                    \"_update.xlsx\")\n      if(is.dm.ex>0){\n        file_r = paste0(\"station_matrix_\",\n                        dm.sta,\n                        \"_update.xlsx\")\n        file = file[which(!(file %in% file_r))]\n      }\n      yearr = as.data.frame(\n        matrix(, nrow = , ncol = 2))\n      \n      ## Identify beginning and ending years\n      while(f<length(file)+1){\n        data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n        \n        \n        # Delete unnecessary rows until first data\n        data_gg = data_g[(grep(\"^Date\", \n                               data_g[,1])+1):length(data_g[,1]),]\n        \n        # assign data_gg column names as data_g's row 6 \n        # because it has column names aligned correctly\n        colnames(data_gg) <- data_g[6,]\n        \n        # Date/Time Format originally MM/DD/YYYY HH:MM\n        # Distorted to Excel Serial Date (start from 1900-01-01)\n        # numbers while loading. Hence, these are  \n        # converted to YYYY-MM-DD HH:MM format\n        data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                                 + as.POSIXct(\"1899-12-30 00:00\"))\n        \n        # Convert strings into numbers\n        data_gg[,2] = as.numeric(data_gg[,2])\n        data_gg[,3] = as.numeric(data_gg[,3])\n        \n        # Read storm type\n        # Read storm type\n        \n        if(pv==2){\n          # Non-thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n        }\n        if(pv==3){\n          # Thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n        }\n        if(pv==4){\n          # Tropical Storm\n          data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n        }\n        \n        # Obtain minimum and maximum years of each station\n        yearr[f,1] = min(unique(year(data_gg[,1])))\n        yearr[f,2] = max(unique(year(data_gg[,1])))\n        \n        f = f + 1\n      }\n      \n      super_data = as.data.frame(\n        matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n               ncol = 2*length(file)))\n      super_data[,seq(2,length(file)*2,2)] = \n        min(yearr[,1]):max(yearr[,2])\n      \n      f = 1\n      \n      # Place max accordingly\n      while(f<length(file)+1){\n        data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n        \n        # Delete unnecessary rows until first data\n        data_gg = data_g[(grep(\"^Date\", \n                               data_g[,1])+1):length(data_g[,1]),]\n        \n        # assign data_gg column names as data_g's row 6 \n        # because it has column names aligned correctly\n        colnames(data_gg) <- data_g[6,]\n        \n        # Date/Time Format originally MM/DD/YYYY HH:MM\n        # Distorted to Excel Serial Date (start from 1900-01-01)\n        # numbers while loading. Hence, these are  \n        # converted to YYYY-MM-DD HH:MM format\n        data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                                 + as.POSIXct(\"1899-12-30 00:00\"))\n        \n        # Convert strings into numbers\n        data_gg[,2] = as.numeric(data_gg[,2])\n        data_gg[,3] = as.numeric(data_gg[,3])\n        \n        # Read storm type\n        if(pv==2){\n          # Non-thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n        }\n        if(pv==3){\n          # Thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n        }\n        if(pv==4){\n          # Tropical Storm\n          data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n        }\n        \n        # Annual\n        {\n          i = 1\n          j = 1\n          k = 1\n          # no_YEARS: how many data points are there?\n          no_YEARS = length(unique(year(data_gg[,1])))\n          year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n          data_gg[,14] = year(data_gg[,1])\n          while(k<(no_YEARS)){\n            # Identify which data row is the last one\n            # from all of the year n\n            while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n              j = j + 1\n            }\n            j = i + j - 1\n            \n            if(m == 1){\n              # Mean\n              year[k] = sum(data_gg[i:j,3])/length(i:j)\n              type = \"Mean\"\n            }else if(m == 2){\n              # Median\n              year[k] = median(data_gg[i:j,3]) \n              type = \"Median\"\n            }else if(m == 3){\n              # Coefficient of Variation\n              year[k] = sd(data_gg[i:j,3])/\n                (sum(data_gg[i:j,3])/length(i:j)) \n              type = \"Coefficient of Variation\"\n            }else if(m == 4){\n              # Maximum\n              year_a[k,1] = max(data_gg[i:j,3])\n              year_a[k,2] = as.character(data_gg[i,1])\n              year_a[k,3] = year(data_gg[i,1])\n              type = \"Maximum\"\n            }else if(m == 5){\n              # Frequency\n              year[k] = length(i:j)\n              type = \"Frequency\"\n            }else{\n              # L-Moments\n              L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n                type = \"L-CV\"\n              }else if(n == 2){\n                year[k] = lsk\n                type = \"L-Skewness\"\n              }else{\n                year[k] = lku\n                type = \"L-Kurtosis\"\n              }\n            }\n            \n            # i variable is added by 1\n            # to move on to the next year\n            i = j + 1\n            j = 1\n            k = k + 1\n            if(k == (no_YEARS)){\n              last = length(data_gg[,3])\n              if(m == 1){\n                year[k] = sum(data_gg[i:last,3])/length(i:last)\n              }else if(m == 2){\n                year[k] = median(data_gg[i:last,3])\n              }else if(m == 3){\n                year[k] = sd(data_gg[i:last,3])/\n                  (sum(data_gg[i:last,3])/length(i:last)) \n              }else if(m == 4){\n                year_a[k,1] = max(data_gg[i:last,3])\n                year_a[k,2] = as.character(data_gg[last,1])\n                year_a[k,3] = year(data_gg[last,1])\n              }else if(m == 5){\n                year[k] = length(i:last)\n              }else{\n                L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n                lcv = L[2]/L[1] # L-cv\n                lsk = L[3]/L[2] # L-Skew\n                lku = L[4]/L[2] # L-Kurtosis\n                \n                l1 = fit$mle[1]/(1+fit$mle[2])\n                l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n                \n                t2[k,1] = l2/l1\n                t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n                \n                if(n == 1){\n                  year[k] = lcv\n                }else if(n == 2){\n                  year[k] = lsk\n                }else{\n                  year[k] = lku\n                }\n              }\n              k = k + 1\n            }\n          }\n          used_data = year_a[,1]\n          used_date = year_a[,2]\n        }\n        \n        \n        # Obtain desired data\n        super_data[,2*(f-1)+1] = \n          ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n                 year_a[,1],NA)\n        \n        f = f + 1\n      }\n      \n      # Obtain max from every row, ignoring NA\n      cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n      \n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      used_data = log(used_data)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n      \n      # Remove data points that are NA (not recorded)\n      # by indexing together\n      t = yearr_t - min(yearr_t)\n      t[which(is.na(used_data))] = NA\n      used_data <- used_data[!is.na(used_data)]\n      t <- t[!is.na(t)]\n      \n      # Significance\n      # Linear Regression Model\n      sta.lm = lm(used_data~t)\n      \n      # P-Values for Intercept and Trend Coefficients\n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[2,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      \n      # Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk = 2*(1-pnorm(abs(mk_z)))\n      \n      if(clu==clu.f){\n        cl.tabl[cl,6] = format(round(\n          summary(sta.lm)$coefficients[2,1], 6), nsmall = 5)\n        cl.tabl[cl,7] = format(round(\n          summary(sta.lm)$coefficients[2,4], 6), nsmall = 5)\n        cl.tabl[cl,8] = format(round(alp,5), nsmall = 5)\n        cl.tabl[cl,9] = format(round(\n          pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n        cl.tabl[cl,10] = mk\n      }else{\n        cl.tabl[dimc + cl,6] = format(round(\n          summary(sta.lm)$coefficients[2,1], 6), nsmall = 5)\n        cl.tabl[dimc + cl,7] = format(round(\n          summary(sta.lm)$coefficients[2,4], 6), nsmall = 5)\n        cl.tabl[dimc + cl,8] = format(round(alp,5), nsmall = 6)\n        cl.tabl[dimc + cl,9] = format(round(\n          pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n        cl.tabl[dimc + cl,10] = mk\n      }\n      \n      cl = cl + 1\n    }\n    \n    mm = 1\n    while(mm < max(clusters$clustering)+1){\n      mmm = which(clusters$clustering %in% mm)\n      points(mappoint$LON[mmm],mappoint$LAT[mmm],\n             col=colors[mm], cex=4, pch = 20)\n      mm = mm + 1\n    }\n    \n    # savePlot(\"clipboard\", type=\"wmf\")\n    dev.off()\n  }\n  \n  # Plotting\n  cl = 1\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    \n    file = paste0(\"station_matrix_\",\n                  kdata.sca[cl.index,1],\n                  \"_update.xlsx\")\n    \n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    n.good.fit = length(which(abs(dm.test$Z)<1.64))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    f = 1\n    \n    ## Identify beginning and ending years\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      # Read storm type\n      \n      if(pv==1){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==2){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==3){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Obtain minimum and maximum years of each station\n      yearr[f,1] = min(unique(year(data_gg[,1])))\n      yearr[f,2] = max(unique(year(data_gg[,1])))\n      \n      f = f + 1\n    }\n    \n    super_data = as.data.frame(\n      matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n             ncol = 2*length(file)))\n    super_data[,seq(2,length(file)*2,2)] = \n      min(yearr[,1]):max(yearr[,2])\n    \n    f = 1\n    \n    # Place max accordingly\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,2]\n      }\n      \n      \n      # Obtain desired data\n      super_data[,2*(f-1)+1] = \n        ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n               year_a[,1],NA)\n      \n      f = f + 1\n    }\n    \n    # Obtain max from every row, ignoring NA\n    cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n    \n    \n    if(dim(super_data)[2]<3){\n      super_data = super_data[which(!is.na(super_data[,1])),]\n      yearr_t = super_data[,2]\n      used_data = super_data[,1]\n    }else{\n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n    }\n    \n    # Remove data points that are NA (not recorded)\n    # by indexing together\n    t = yearr_t\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n    # Significance\n    # Linear Regression Model\n    sta.lm = lm(log(used_data)~t)\n    \n    # P-Values for Intercept and Trend Coefficients\n    t1 = sta.lm$coefficients[2]/\n      summary(sta.lm)$coefficients[2,2]\n    \n    alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n    \n    t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n      1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n    \n    # Log transformation and linear regression\n    ln_used_data = log(used_data)\n    lm.year = lm(ln_used_data~t)\n    m.year = lm(used_data~t)\n    sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t\n    rho = cor(ln_used_data,t)\n    \n    # Conditional Moments of y\n    mu_y.w = lm.year$coefficients[1] + \n      lm.year$coefficients[2]*t\n    vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)\n    sk_y.w = skewness(ln_used_data) - \n      skewness(t)*(lm.year$coefficients[2]^3)\n    \n    # Conditional Moments of x\n    mu_x.w = exp(mu_y.w + (vr_y.w)/2)\n    vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)\n    cv_x.w = sqrt(exp(vr_y.w)-1)\n    sk_x.w = 3*cv_x.w + cv_x.w^3\n    \n    # Stationary\n    mean.sta = mean(ln_used_data)\n    sd.sta = sqrt(sum((ln_used_data - \n                         mean(ln_used_data))^2)/\n                    length(ln_used_data))\n    YR = c(10,20,30)\n    LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))\n    t_i = 1\n    \n    {\n      # PPCC \n      b_coef = cor(ln_used_data, t)*sd(ln_used_data)/sd(t)\n      mu_y.pp = mean.sta + b_coef*(t-mean(t))\n      sd_y.pp = sqrt(var(ln_used_data)*(1-cor(ln_used_data, t)^2))\n      z_i.s = sort((ln_used_data - mean.sta)/sd.sta)\n      z_i.n = sort((ln_used_data - mu_y.pp)/sd_y.pp)\n      rank_i.s = rank(z_i.s)\n      rank_i.n = rank(z_i.n)\n      p_pos.s = (rank_i.s - 0.375)/(length(ln_used_data) + 0.25)\n      p_pos.n = (rank_i.n - 0.375)/(length(ln_used_data) + 0.25)\n      \n      z_ii.s = qnorm(p_pos.s)\n      z_ii.n = qnorm(p_pos.n)\n      if(cl==1){\n        if(clu==2){\n          png(filename=paste0(dir,\"/plots/\",texty[pv],\n                              \"/pp_lll_\",clu,\".png\"),\n              w = 800, h = 400)\n          par(mar=c(8,4.5,2,2))\n          par(mfrow=c(1,2))\n          txt = 1\n          txt2 = 1\n          }else if(clu==3){\n            png(filename=paste0(dir,\"/plots/\",texty[pv],\n                                \"/pp_lll_\",clu,\".png\"),\n                w = 900, h = 300)\n            par(mar=c(8,4.5,2,2))\n            par(mfrow=c(1,3))\n            txt = 1.5\n            txt2 = txt\n            }else if(clu==4){\n              png(filename=paste0(dir,\"/plots/\",texty[pv],\n                                  \"/pp_lll_\",clu,\".png\"),\n                  w = 800, h = 800)\n              par(mar=c(8,4.5,2,2))\n              par(mfrow=c(2,2))\n              txt = 1.5\n              txt2 = txt\n              }\n      }\n      \n      plot(z_ii.s,z_i.s, xlab = \"Normal Quantiles\",\n           ylab = \"Ordered Standardized Observations\",\n           type = \"n\", tck = 0.02,\n           xlim = c(-3,3), ylim = c(-3,3),\n           cex.lab = txt, xaxt= \"n\", yaxt =\"n\")\n      axis(1, cex.axis = txt)\n      axis(2, cex.axis = txt)\n      \n      points(z_ii.n,z_i.n, pch = 21, col = \"gray\",\n             bg = \"gray\", cex = 2)\n      points(z_ii.s,z_i.s, pch = 20)\n      abline(0,1, lwd = 1)\n      \n      corst = round(cor(z_ii.s,z_i.s),4)\n      corns = round(cor(z_ii.n,z_i.n),4)\n      \n      leg_s = bquote(PPCC[s] == .(corst))\n      leg_n = bquote(PPCC[ns] == .(corns))\n      text(-3,2.75,leg_s, pos = 4,\n           bty = \"n\",\n           cex = txt2)\n      text(-3,2.25,leg_n, pos = 4,\n           bty = \"n\",\n           cex = txt2)\n      text(3.25,-2.25, \n           paste0(clu,\"-Means\"), pos = 2,\n           cex = txt2)\n      text(3.25,-1.75, \n           text.storm, pos = 2,\n           cex = txt2)\n      text(3.25,-2.25, \n           paste0(clu,\"-Means\"), pos = 2,\n           cex = txt2)\n      text(3.25,-2.75, \n           paste(\"Cluster\",cl), pos = 2,\n           cex = txt2)\n\n      if(cl==length(unique(clusters$clustering))){\n        add_legend(\"bottom\", \n                   legend = c(\"Stationary\", \"Nonstationary\"),\n                   ncol = 2, pch = c(20, 21),\n                   col = c(\"black\", \"gray\"), pt.cex = c(1,2),\n                   pt.bg = c(NA, \"gray\"), bty =\"n\",\n                   cex = 2)\n        dev.off()\n      }\n    }\n    \n    if(dis==\"pe3\"){\n      # LP3\n      n_lp = length(ln_used_data)\n      sk_y.w.lp = (1+6/n_lp)*\n        ((n_lp*sum((ln_used_data-mean(ln_used_data))^3)))/\n        ((n_lp-1)*(n_lp-2)*sd(ln_used_data)^3)\n      a.lp3 = max(2/sk_y.w.lp,0.4)\n      b.lp3 = 1 + 0.0144*((max(0,sk_y.w.lp-2.25))^2)\n      f.lp3 = sk_y.w.lp - 0.063*(max(0,sk_y.w.lp-1)^1.85)\n      h.lp3 = (b.lp3 - 2/(sk_y.w.lp*a.lp3))^(1/3)\n      k.p_max2 = 1-((f.lp3/6)^2) + qnorm(1-(1/rp))*((f.lp3/6))\n      kp_i = 1\n      k.p = as.data.frame(\n        matrix(, nrow = length(k.p_max2), \n               ncol = 1))\n      for(kp_i in 1:length(k.p_max2)){\n        k.p[kp_i,] = max(h.lp3,k.p_max2[kp_i])\n      }\n      k.p = a.lp3*(k.p^3) - b.lp3\n      \n      ppp_sta = exp(mean.sta + k.p*sd.sta)\n      ppp_hom = exp(mean.hom + k.p*sd.hom)\n      ppp_het = exp(mean.het + k.p*sd.het)\n    }\n    if(dis==\"gno\"){\n      # LN2\n      ppp_sta.ln2 = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n      ppp_hom.ln2 = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n      ppp_het.ln2 = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n\n      # LN3\n      # u = ln(x - tau)\n      # Stationarity\n      vr_x = sum((used_data-mean(used_data))^2)/\n        (length(used_data)-1)\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      \n      B = .5*(-g_x + sqrt(g_x^2 + 4))\n      the = (1 - B^(2/3))/(B^(1/3))\n      ome = the^2 + 1\n      \n      mu_u = .5*log(vr_x/(ome*(ome-1)))\n      vr_u = log(ome)\n      ta_x = mean(used_data) - sqrt(vr_x)/the\n      ppp_sta.ln3 = ta_x + exp(mu_u + qnorm(1-(1/rp))*sqrt(vr_u))\n      \n      # Nonstationarity\n      # Homogeneous\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      B_non = .5*(-sk_x.w.hom + sqrt(sk_x.w.hom^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.hom/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.hom - sqrt(vr_x.w.hom)/the_non\n      ppp_hom.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      # Heterogeneous\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      B_non = .5*(-sk_x.w.het + sqrt(sk_x.w.het^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.het/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.het - sqrt(vr_x.w.het)/the_non\n      ppp_het.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      ln.com = c(ppp_sta.ln2, ppp_hom.ln2, ppp_het.ln2,\n                 ppp_sta.ln3, ppp_hom.ln3, ppp_het.ln3)\n      ln.max = signif(ceiling(max(ln.com)), digits = 1)\n      ln.min = signif(round_any(min(ln.com), 10, f = ceiling), digits = 1)\n      plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(ln.min,ln.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.ln2, col = colors[1], lwd = 2)\n      lines(rp_gev, ppp_hom.ln2, col = colors[1], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln2, col = colors[1], lwd = 2, lty = 3)\n      lines(rp_gev, ppp_sta.ln3, col = colors[2], lwd = 2)\n      lines(rp_gev, ppp_hom.ln3, col = colors[2], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln3, col = colors[2], lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n      \n    \n        }\n    if(dis==\"gev\"){\n      dis = \"GEV\"\n      # Stationarity\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      if((g_x<=1.15)&(g_x>=-0.7)){\n        k_sta = 0.0087*g_x^3 + 0.0582^2 - 0.32*g_x + 0.2778\n      }else{\n        k_sta = -.31158*(1-exp(-.4556*(g_x - 0.97134)))\n      }\n      \n      a_sta = sign(k_sta)*k_sta*sqrt(vr_x)/\n        sqrt(gamma(1+2*k_sta)-(gamma(1+2*k_sta))^2)\n      e_sta = mean(used_data) - a_sta*(gamma(1+k_sta)-1)/k_sta\n      \n      pp = seq(0.01,1, length.out = 10001)\n      rp_gev = (1-pp)^-1\n      ppp_sta.gev = e_sta + (a_sta/k_sta)*(1-(-log(pp))^k_sta)\n      \n      # Nonstationarity\n      # Homogeneous\n      if((sk_x.w<=1.15)&(sk_x.w>=-0.7)){\n        k_non.hom = 0.0087*sk_x.w^3 + 0.0582^2 - 0.32*sk_x.w + 0.2778\n      }else{\n        k_non.hom = -.31158*(1-exp(-.4556*(sk_x.w - 0.97134)))\n      }\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      a_non.hom = sign(k_non.hom)*k_non.hom*sqrt(vr_x.w.hom)/\n        sqrt(gamma(1+2*k_non.hom)-(gamma(1+2*k_non.hom))^2)\n      e_non.hom = mean(used_data) - a_non.hom*(gamma(1+k_non.hom)-1)/k_non.hom\n      ppp_hom.gev = e_non.hom + (a_non.hom/k_non.hom)*(1-(-log(pp))^k_non.hom)\n      \n      # Heterogeneous\n      k_non.het = k_non.hom\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      a_non.het = sign(k_non.het)*k_non.het*sqrt(vr_x.w.het)/\n        sqrt(gamma(1+2*k_non.het)-(gamma(1+2*k_non.het))^2)\n      e_non.het = mean(used_data) - a_x*(gamma(1+k_non.het)-1)/k_non.het\n      ppp_het.gev = e_non.het + (a_non.het/k_non.het)*(1-(-log(pp))^k_non.het)\n      \n      last1700 = tail(which(rp_gev<1701),1)\n      gev.com = c(ppp_sta.gev, ppp_hom.gev, ppp_het.gev)[1:last1700]\n      gev.max = signif(ceiling(max(gev.com)), digits = 1)\n      gev.min = signif(round_any(min(gev.com), 10, f = ceiling), digits = 1)\n      plot(rp_gev,ppp_sta.gev, log=\"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(gev.min,gev.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.gev, lwd = 2)\n      lines(rp_gev, ppp_hom.gev, lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.gev, lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n    }\n    cl = cl + 1\n  }\n}\nfor (clu in clu.f:(clu.f+1)){\n  clusters = pam(kdata.sca[,c(2,3,7)], clu)\n  # Plotting\n  cl = 1\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    \n    file = paste0(\"station_matrix_\",\n                  kdata.sca[cl.index,1],\n                  \"_update.xlsx\")\n    \n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    n.good.fit = length(which(abs(dm.test$Z)<1.64))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    f = 1\n    \n    ## Identify beginning and ending years\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      # Read storm type\n      \n      if(pv==1){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==2){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==3){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Obtain minimum and maximum years of each station\n      yearr[f,1] = min(unique(year(data_gg[,1])))\n      yearr[f,2] = max(unique(year(data_gg[,1])))\n      \n      f = f + 1\n    }\n    \n    super_data = as.data.frame(\n      matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n             ncol = 2*length(file)))\n    super_data[,seq(2,length(file)*2,2)] = \n      min(yearr[,1]):max(yearr[,2])\n    \n    f = 1\n    \n    # Place max accordingly\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,2]\n      }\n      \n      \n      # Obtain desired data\n      super_data[,2*(f-1)+1] = \n        ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n               year_a[,1],NA)\n      \n      f = f + 1\n    }\n    \n    # Obtain max from every row, ignoring NA\n    cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n    \n    \n    if(dim(super_data)[2]<3){\n      super_data = super_data[which(!is.na(super_data[,1])),]\n      yearr_t = super_data[,2]\n      used_data = super_data[,1]\n    }else{\n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n    }\n    \n    # Remove data points that are NA (not recorded)\n    # by indexing together\n    t = yearr_t\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n    # Significance\n    # Linear Regression Model\n    sta.lm = lm(log(used_data)~t)\n    \n    # P-Values for Intercept and Trend Coefficients\n    t1 = sta.lm$coefficients[2]/\n      summary(sta.lm)$coefficients[2,2]\n    \n    alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n    \n    t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n      1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n    \n    # Log transformation and linear regression\n    ln_used_data = log(used_data)\n    lm.year = lm(ln_used_data~t)\n    m.year = lm(used_data~t)\n    sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t\n    rho = cor(ln_used_data,t)\n    \n    # Conditional Moments of y\n    mu_y.w = lm.year$coefficients[1] + \n      lm.year$coefficients[2]*t\n    vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)\n    sk_y.w = skewness(ln_used_data) - \n      skewness(t)*(lm.year$coefficients[2]^3)\n    \n    # Conditional Moments of x\n    mu_x.w = exp(mu_y.w + (vr_y.w)/2)\n    vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)\n    cv_x.w = sqrt(exp(vr_y.w)-1)\n    sk_x.w = 3*cv_x.w + cv_x.w^3\n    \n    # Stationary\n    mean.sta = mean(ln_used_data)\n    sd.sta = sqrt(sum((ln_used_data - \n                         mean(ln_used_data))^2)/\n                    length(ln_used_data))\n    YR = c(10,20,30)\n    LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))\n    t_i = 1\n    \n    while(t_i<4){\n      t_non = c(tail(t,1),2020,2030)\n      # Nonstationary - Mean\n      mean.hom = unname(lm.year$coefficients[1] + \n                          lm.year$coefficients[2]*t_non[t_i])\n      sd.hom = unname(sqrt(var(ln_used_data)-\n                             lm.year$coefficients[2]^2*\n                             var(t)))\n      \n      # Nonstationary - Mean + Cv\n      res = (((lm.year$residuals)^(2))^(1/3))\n      lm.res = lm(res~t)\n      mean.het = mean.hom\n      sd.het = sqrt((lm.res$coefficients[1]+\n                       lm.res$coefficients[2]*tail(t,1))^3 +\n                      3*(summary(lm.res)$sigma^2)*\n                      (lm.res$coefficients[1]+\n                         lm.res$coefficients[2]*t_non[t_i]))\n      \n      \n      \n      \n      # Generate return period data points\n      rp = seq(1.01,1700, length.out = 10001)\n      \n      ppp_sta.ln2 = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n      ppp_hom.ln2 = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n      ppp_het.ln2 = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n      \n      if(t_i==1){\n        png(filename=paste0(dir,\"/plots/\",texty[pv],\n                            \"/cLLL\",clu,\"_\",cl,\".png\"),\n            w = 800, h = 600)\n        par(mfrow=c(1,2))\n        plot(t,ln_used_data, xlab = \"Year\",\n             ylab = \"ln (Peak Wind Gust)\",\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.1)\n        abline(lm(ln_used_data~t))\n        axis(side=1, \n             at = c(1980,1990,2000,2010),\n             cex.axis = 1.5)\n        axis(side=2, \n             cex.axis = 1.5)\n        \n        plot(t, res, xlab = \"Year\",\n             ylab = expression(epsilon^{2/3}),\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.1)\n        abline(lm(res~t))\n        axis(side=1, \n             at = c(1980,1990,2000,2010),\n             cex.axis = 1.5)\n        axis(side=2, \n             cex.axis = 1.5)\n        dev.off()\n        \n        plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n             xlim = c(1,3500), ylim = c(ln2.min,ln2.max),\n             xlab = \"Return Period\",\n             ylab = \"Peak wind gust (mph)\",\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.5)\n        \n        \n        max.mean = unname(lm.year$coefficients[1] + \n                            lm.year$coefficients[2]*2030)\n        max.sd = sqrt((lm.res$coefficients[1]+\n                         lm.res$coefficients[2]*2010)^3 +\n                        3*(summary(lm.res)$sigma^2)*\n                        (lm.res$coefficients[1]+\n                           lm.res$coefficients[2]*2030))\n        max.hom = exp(max.mean + qnorm(1-(1/rp))*sd.hom)\n        max.het = exp(max.mean + qnorm(1-(1/rp))*max.sd)\n        ln2.max = round_any(max(max.hom,max.het,ppp_sta.ln2),\n                            10, \n                            f = ceiling)\n        \n        ln2.min = round_any(min(c(ppp_sta.ln2,ppp_hom.ln2,ppp_het.ln2)),\n                            10, f = floor)\n        \n        par(mar = c(4,4,2,2))\n        png(filename=paste0(dir,\"/plots/\",texty[pv],\n                            \"/lll\",clu,\"_\",cl,\".png\"),\n            w = 800, h = 600)\n        plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n             xlim = c(1,3500), ylim = c(ln2.min,ln2.max),\n             xlab = \"Return Period\",\n             ylab = \"Peak wind gust (mph)\",\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.5)\n        axis(side=1, labels = T, \n             at=c(1, 10, 50, 100,300,700,1700),\n             cex.axis = 1.5)\n        axis(side=2, labels = T, \n             at=seq(40,240,10),\n             cex.axis = 1)\n        abline(v=c(1, 10, 50, 100, 300,700, 1700), \n               h=seq(40,240,10), \n               col=\"gray\", lty=3)\n        legend(\"bottomright\", \n               c(\"Stationary\", \n                 \"Trend in Mean\",\n                 \"Trend in Mean + CV\"), \n               lty = c(1,2,3),\n               lwd = 2,\n               cex = 1.5)\n        lines(rp, ppp_sta.ln2, col = colors[cl], lwd = 2)\n      }\n      \n      \n      lines(rp, ppp_hom.ln2, col = colors[cl], lwd = 1+t_i, lty = 2)\n      lines(rp, ppp_het.ln2, col = colors[cl], lwd = 1+t_i, lty = 3)\n      text(x= 1700, y= max(ppp_hom.ln2), cex = 1,\n           pos = 4, labels = paste(t_non[t_i], \"HOM\"))\n      text(x= 1700, y= max(ppp_het.ln2), cex = 1,\n           pos = 4, labels = paste(t_non[t_i], \"HET\"))\n      \n      LLL[1,t_i] = ppp_sta.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i] = ppp_sta.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i] = ppp_sta.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i] = ppp_sta.ln2[which.min(abs(rp-1700))]\n      LLL[1,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-1700))]\n      LLL[1,t_i + 6] = ppp_het.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i + 6] = ppp_het.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i + 6] = ppp_het.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i + 6] = ppp_het.ln2[which.min(abs(rp-1700))]\n      if(t_i==3){\n        assign(paste(\"LLL\",clu,cl,YR[t_i], sep = \"_\"),\n               LLL)\n      }\n      t_i = t_i + 1\n    }\n    \n    \n    if(dis==\"pe3\"){\n      # LP3\n      n_lp = length(ln_used_data)\n      sk_y.w.lp = (1+6/n_lp)*\n        ((n_lp*sum((ln_used_data-mean(ln_used_data))^3)))/\n        ((n_lp-1)*(n_lp-2)*sd(ln_used_data)^3)\n      a.lp3 = max(2/sk_y.w.lp,0.4)\n      b.lp3 = 1 + 0.0144*((max(0,sk_y.w.lp-2.25))^2)\n      f.lp3 = sk_y.w.lp - 0.063*(max(0,sk_y.w.lp-1)^1.85)\n      h.lp3 = (b.lp3 - 2/(sk_y.w.lp*a.lp3))^(1/3)\n      k.p_max2 = 1-((f.lp3/6)^2) + qnorm(1-(1/rp))*((f.lp3/6))\n      kp_i = 1\n      k.p = as.data.frame(\n        matrix(, nrow = length(k.p_max2), \n               ncol = 1))\n      for(kp_i in 1:length(k.p_max2)){\n        k.p[kp_i,] = max(h.lp3,k.p_max2[kp_i])\n      }\n      k.p = a.lp3*(k.p^3) - b.lp3\n      \n      ppp_sta = exp(mean.sta + k.p*sd.sta)\n      ppp_hom = exp(mean.hom + k.p*sd.hom)\n      ppp_het = exp(mean.het + k.p*sd.het)\n    }\n    if(dis==\"gno\"){\n      # LN2\n      ppp_sta.ln2 = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n      ppp_hom.ln2 = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n      ppp_het.ln2 = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n      \n      # LN3\n      # u = ln(x - tau)\n      # Stationarity\n      vr_x = sum((used_data-mean(used_data))^2)/\n        (length(used_data)-1)\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      \n      B = .5*(-g_x + sqrt(g_x^2 + 4))\n      the = (1 - B^(2/3))/(B^(1/3))\n      ome = the^2 + 1\n      \n      mu_u = .5*log(vr_x/(ome*(ome-1)))\n      vr_u = log(ome)\n      ta_x = mean(used_data) - sqrt(vr_x)/the\n      ppp_sta.ln3 = ta_x + exp(mu_u + qnorm(1-(1/rp))*sqrt(vr_u))\n      \n      # Nonstationarity\n      # Homogeneous\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      B_non = .5*(-sk_x.w.hom + sqrt(sk_x.w.hom^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.hom/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.hom - sqrt(vr_x.w.hom)/the_non\n      ppp_hom.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      # Heterogeneous\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      B_non = .5*(-sk_x.w.het + sqrt(sk_x.w.het^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.het/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.het - sqrt(vr_x.w.het)/the_non\n      ppp_het.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      ln.com = c(ppp_sta.ln2, ppp_hom.ln2, ppp_het.ln2,\n                 ppp_sta.ln3, ppp_hom.ln3, ppp_het.ln3)\n      ln.max = signif(ceiling(max(ln.com)), digits = 1)\n      ln.min = signif(round_any(min(ln.com), 10, f = ceiling), digits = 1)\n      plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(ln.min,ln.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.ln2, col = colors[1], lwd = 2)\n      lines(rp_gev, ppp_hom.ln2, col = colors[1], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln2, col = colors[1], lwd = 2, lty = 3)\n      lines(rp_gev, ppp_sta.ln3, col = colors[2], lwd = 2)\n      lines(rp_gev, ppp_hom.ln3, col = colors[2], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln3, col = colors[2], lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n      \n      \n    }\n    if(dis==\"gev\"){\n      dis = \"GEV\"\n      # Stationarity\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      if((g_x<=1.15)&(g_x>=-0.7)){\n        k_sta = 0.0087*g_x^3 + 0.0582^2 - 0.32*g_x + 0.2778\n      }else{\n        k_sta = -.31158*(1-exp(-.4556*(g_x - 0.97134)))\n      }\n      \n      a_sta = sign(k_sta)*k_sta*sqrt(vr_x)/\n        sqrt(gamma(1+2*k_sta)-(gamma(1+2*k_sta))^2)\n      e_sta = mean(used_data) - a_sta*(gamma(1+k_sta)-1)/k_sta\n      \n      pp = seq(0.01,1, length.out = 10001)\n      rp_gev = (1-pp)^-1\n      ppp_sta.gev = e_sta + (a_sta/k_sta)*(1-(-log(pp))^k_sta)\n      \n      # Nonstationarity\n      # Homogeneous\n      if((sk_x.w<=1.15)&(sk_x.w>=-0.7)){\n        k_non.hom = 0.0087*sk_x.w^3 + 0.0582^2 - 0.32*sk_x.w + 0.2778\n      }else{\n        k_non.hom = -.31158*(1-exp(-.4556*(sk_x.w - 0.97134)))\n      }\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      a_non.hom = sign(k_non.hom)*k_non.hom*sqrt(vr_x.w.hom)/\n        sqrt(gamma(1+2*k_non.hom)-(gamma(1+2*k_non.hom))^2)\n      e_non.hom = mean(used_data) - a_non.hom*(gamma(1+k_non.hom)-1)/k_non.hom\n      ppp_hom.gev = e_non.hom + (a_non.hom/k_non.hom)*(1-(-log(pp))^k_non.hom)\n      \n      # Heterogeneous\n      k_non.het = k_non.hom\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      a_non.het = sign(k_non.het)*k_non.het*sqrt(vr_x.w.het)/\n        sqrt(gamma(1+2*k_non.het)-(gamma(1+2*k_non.het))^2)\n      e_non.het = mean(used_data) - a_x*(gamma(1+k_non.het)-1)/k_non.het\n      ppp_het.gev = e_non.het + (a_non.het/k_non.het)*(1-(-log(pp))^k_non.het)\n      \n      last1700 = tail(which(rp_gev<1701),1)\n      gev.com = c(ppp_sta.gev, ppp_hom.gev, ppp_het.gev)[1:last1700]\n      gev.max = signif(ceiling(max(gev.com)), digits = 1)\n      gev.min = signif(round_any(min(gev.com), 10, f = ceiling), digits = 1)\n      plot(rp_gev,ppp_sta.gev, log=\"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(gev.min,gev.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.gev, lwd = 2)\n      lines(rp_gev, ppp_hom.gev, lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.gev, lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n    }\n    cl = cl + 1\n  }\n}\ncl.tabl[which(cl.tabl[,5]<1),11] = \"AHom\" \ncl.tabl[which(cl.tabl[,5]>2),11] = \"DHet\" \ncl.tabl[which((cl.tabl[,5]<2)&(cl.tabl[,5]>1)),11] = \"PHet\"\ncolnames(cl.tabl) = c(\"Cluster\", \"Cluster ID\", \"N\", \n                      \"N_d\", \"H\", \"SlopeC\", \n                      \"SlopeP\", \"TypeI\", \"TypeII\", \n                      \"MK\", \"Type\")\nlll.cl.tabl = cl.tabl\n\n# Slope Coefficient Plot\n# 3 Pval, 4 Type I, 5 Type II, 6 MK\nsig_tar = 4\nclu.f = floor(sqrt(dim(wanted)[1]/2))\nfor(clu in clu.f:(clu.f+1)){\n  \n  ref_st = read.csv(paste0(dir,\"/plots/table.csv\"), header = F)\n  \n  if(pv==1){\n    colnames(ref_st) = sapply(ref_st[2,], as.character)\n    ref_st = ref_st[3:9,1:11]\n\n  }else if(pv==2){\n    colnames(ref_st) = sapply(ref_st[12,], as.character)\n    ref_st = ref_st[13:19,1:11]\n\n  }else if(pv==3){\n    colnames(ref_st) = sapply(ref_st[22,], as.character)\n    ref_st = ref_st[23:27,1:11]\n  }else if(pv == 4){\n    colnames(ref_st) = sapply(ref_st[30,], as.character)\n    ref_st = ref_st[31:35,1:11]\n  }\n  \n  clusters = pam(kdata.sca[,c(2,3,7)], clu)\n  das = c(1:clu)\n  cl = 1\n  \n  # Go to every cluster...\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      dm.sta = \n        dm.table[which(as.numeric(dm.test$D)>dm.test$Dcrit[1]),1]\n      dm.point = cbind(dis_ang[match(dm.sta,dis_ang[,2]),c(9,10)],cl)\n    }\n    f = 1\n    \n    ## Obtain desired data and remove discordant stations\n    file = paste0(\"station_matrix_\",\n                  kdata.sca[which(clusters$clustering==cl),1],\n                  \"_update.xlsx\")\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    sig_stat = as.data.frame(\n      matrix(, nrow = length(file), \n             ncol = 6))\n    colnames(sig_stat) = c(\"id\", \"Pval\", \"TypeI\",\n                           \"TypeII\", \"MK\", \"SlopeC\")\n    \n    f = 1\n    \n    # Obtain statistics\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,3]\n      }\n      \n      t = used_date - min(used_date)\n      \n      # Significance\n      # Linear Regression Model\n      sta.lm = lm(log(used_data)~t)\n      \n      # P-Values for Intercept and Trend Coefficients\n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[2,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      ## Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk = 2*(1-pnorm(abs(mk_z)))\n      \n      \n      colnames(sig_stat) = c(\"id\", \"SlopeC\", \"Pval\", \n                             \"TypeI\", \"TypeII\", \"MK\")\n      sig_stat[f,1] = as.numeric(substr(file[f],16,21))\n      sig_stat[f,2] = format(round(\n        summary(sta.lm)$coefficients[2,1], 5), nsmall = 5)\n      sig_stat[f,3] = format(round(\n        summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n      sig_stat[f,4] = format(round(alp,5), nsmall = 5)\n      sig_stat[f,5] = format(round(\n        pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n      sig_stat[f,6] = mk\n      \n      f = f + 1\n    }\n    \n    \n    if(cl==1){\n      par(mar = c(4,4,2,2))\n      png(filename=paste0(dir,\"/plots/\",texty[pv],\n                          \"/slopelllC\",clu,\".png\"),\n          w = 800, h = 600)\n      plot(das,\n           seq(-.03,.03, length.out=clu),\n           type = \"n\",\n           xlab = \"Clusters\",\n           ylab = \"Slope Coefficient\",\n           xaxt = \"n\",\n           xlim = c(0.5,clu+0.5),\n           ylim = c(-0.03,0.03),\n           cex.lab = 1.5)\n    }\n    \n    if(as.numeric(sapply((\n      subset(ref_st,ref_st[,1]==clu&ref_st[,2]==cl)[sig_tar+4]),\n      as.character))>0.05){\n      rect(cl-.5,-1,cl+.5,1,col = rgb(0.5,0.5,0.5,1/4),\n           border = NA)\n    }\n    # Significant points\n    cl_si = which(sig_stat[,sig_tar]<0.05)\n    points(seq(cl,cl,length.out=length(cl_si)),\n           sig_stat[cl_si,2],\n           pch = 3,cex = 2)\n    # Insignificant points\n    cl_insi = which(sig_stat[,sig_tar]>0.050001)\n    points(seq(cl,cl,length.out=length(cl_insi)),\n           sig_stat[cl_insi,2],\n           pch = 19,cex = 0.5)\n    # Superstation point\n    points(cl,\n           subset(ref_st,ref_st[,1]==clu.f&ref_st[,2]==cl)[6],\n           pch= 18, col = \"red\")\n    \n    abline(h = 0, col = \"gray\", lty = \"dotted\")\n    axis(1, at=cl, labels = cl, tick = T)\n    cl = cl + 1\n  }\n  legend(\"bottomleft\",\n         legend=c(\"Insignificant\",\n                  \"Significant\",\n                  \"Cluster Superstation\"),\n         col=c(\"black\",\"black\",\"red\"),\n         pch=c(19,3,18),\n         pt.cex=c(0.5,2,1),\n         cex = 1.5)\n  dads = as.numeric(sapply(ref_st[which(ref_st[,1]==clu),6],\n                           as.character))\n  points(das,dads[das], pch= 18, col = \"red\")\n  dev.off()\n}\n\n\n# LLL = rbind(LLL_2_1_30,LLL_2_2_30,\n#             LLL_3_1_30,LLL_3_2_30,LLL_3_3_30)\n# LLL = rbind(LLL_3_1_30,LLL_3_2_30,LLL_3_3_30, \n#             LLL_4_1_30,LLL_4_2_30,LLL_4_3_30,LLL_4_4_30)\n# write.table(LLL, \"clipboard\", sep=\"\\t\", row.names=FALSE, col.names=FALSE)\n\n# Optimal\ncl.tabl = as.data.frame(\n  matrix(, nrow = , ncol = 15))\nclu.f = floor(sqrt(dim(wanted)[1]/2))\nfor (clu in clu.f:(clu.f+1)){\n  par(mar=c(2,2,2,2))\n  png(filename=paste0(dir,\"/plots/\",texty[pv],\n                       \"/optimal_f\",clu,\".png\"),\n       w = 800, h = 600)\n  max_clu = get(paste(\"maxsos\",clu,sep=\"\"))\n  colclu = match(c(\"lat\",max_clu[2,c(2:3)]),colnames(kdata.sca))\n  clusters = pam(kdata.sca[,colclu], clu)\n  \n  {\n    mappoint= cbind(dis_ang[as.numeric(rownames(kdata)),c(9,10)],\n                    clusters$clustering)\n    newmap <- getMap(resolution = \"low\")\n    # windows(800, 600, pointsize = 12)\n    plot(newmap, xlim = c(-80, -65), ylim = c(25, 45), asp = 1)\n    legend(-63,30,legend = c(1:clu,\"Discordant\"),\n           bg = \"white\",col=c(colors[1:clu],\"black\"), \n           bty = \"n\", pch = c(rep(20,clu),1), \n           pt.cex = c(rep(3,clu),6), \n           pt.lwd = c(rep(1,clu),3),\n           ncol = 1, cex = 1.5)\n    ll = 1\n    while(ll<length(colnames(clusters$medoids))+1){\n      if(ll==1){\n        text(-63,42.5-ll*1.5,\"Lat\",\n             cex = 2.5)\n      }\n      text(-63,42.5-ll*1.5,\n           capitalize(colnames(clusters$medoids)[ll]),\n           cex = 2.5)\n      ll = ll + 1\n    }\n    text(-63,35,\n         paste0(\"Si: \",summary(silhouette(clusters))$si.summary[4]),\n         cex = 2.5)\n    \n    cl = 1\n    if(clu!=clu.f){\n      dimc = dim(cl.tabl)[1]\n    }else{\n      dimc = 0\n    }\n    while(cl<length(unique(clusters$clustering))+1){\n      cl.index = which(as.numeric(clusters$clustering)==cl)\n      dm.table = cbind(kdata[cl.index,1],\n                       wanted[cl.index,14],\n                       cov[cl.index,3],\n                       lmom[cl.index,8:11])\n      colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n      dm.test = regtst(dm.table[,1:6],5000)\n      is.dm.ex = \n        length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n      n.good.fit = length(which(abs(dm.test$Z)<1.64))\n      \n      if(is.dm.ex>0){\n        dm.sta = \n          dm.table[which(as.numeric(dm.test$D)>dm.test$Dcrit[1]),1]\n        dm.point = cbind(dis_ang[match(dm.sta,dis_ang[,2]),c(9,10)],cl)\n        points(dm.point$LON,dm.point$LAT,\n               col=colors[cl], cex=6, pch = 1,\n               lwd = 3)\n      }\n      if(clu==clu.f){\n        cl.tabl[cl,1] = clu\n        cl.tabl[cl,2] = cl\n        cl.tabl[cl,3] = length(dm.test$D)\n        cl.tabl[cl,4] = \n          length(which(dm.test$D>dm.test$Dcrit[1]))\n        cl.tabl[cl,5] = dm.test$H[1]\n        if(n.good.fit!=0){\n          for(n.g in 1:n.good.fit){\n            cl.tabl[cl,14+(2*n.g)-1] = sort(abs(dm.test$Z))[n.g]\n            cl.tabl[cl,14+(2*n.g)] = attributes(sort(abs(dm.test$Z))[n.g])\n          }\n        }else{\n          cl.tabl[cl,15] = sort(abs(abs(dm.test$Z)-1.64))[1] + 1.64\n          cl.tabl[cl,16] = attributes(sort(abs(abs(dm.test$Z)-1.64))[1])\n          }\n        }else{\n        cl.tabl[dimc + cl,1] = clu\n        cl.tabl[dimc + cl,2] = cl\n        cl.tabl[dimc + cl,3] = length(dm.test$D)\n        cl.tabl[dimc + cl,4] = \n          length(which(dm.test$D>dm.test$Dcrit[1]))\n        cl.tabl[dimc + cl,5] = dm.test$H[1]\n        if(n.good.fit!=0){\n          for(n.g in 1:n.good.fit){\n            cl.tabl[dimc + cl,14+(2*n.g)-1] = sort(abs(dm.test$Z))[n.g]\n            cl.tabl[dimc + cl,14+(2*n.g)] = attributes(sort(abs(dm.test$Z))[n.g])\n          }\n        }else{\n          cl.tabl[dimc + cl,15] = sort(abs(abs(dm.test$Z)-1.64))[1] + 1.64\n          cl.tabl[dimc + cl,16] = attributes(sort(abs(abs(dm.test$Z)-1.64))[1])\n        }\n        }\n      f = 1\n      \n      ## Obtain desired data\n      file = paste0(\"station_matrix_\",\n                    kdata.sca[which(clusters$clustering==cl),1],\n                    \"_update.xlsx\")\n      if(is.dm.ex>0){\n        file_r = paste0(\"station_matrix_\",\n                        dm.sta,\n                        \"_update.xlsx\")\n        file = file[which(!(file %in% file_r))]\n      }\n      yearr = as.data.frame(\n        matrix(, nrow = , ncol = 2))\n      \n      ## Identify beginning and ending years\n      while(f<length(file)+1){\n        data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n        \n        \n        # Delete unnecessary rows until first data\n        data_gg = data_g[(grep(\"^Date\", \n                               data_g[,1])+1):length(data_g[,1]),]\n        \n        # assign data_gg column names as data_g's row 6 \n        # because it has column names aligned correctly\n        colnames(data_gg) <- data_g[6,]\n        \n        # Date/Time Format originally MM/DD/YYYY HH:MM\n        # Distorted to Excel Serial Date (start from 1900-01-01)\n        # numbers while loading. Hence, these are  \n        # converted to YYYY-MM-DD HH:MM format\n        data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                                 + as.POSIXct(\"1899-12-30 00:00\"))\n        \n        # Convert strings into numbers\n        data_gg[,2] = as.numeric(data_gg[,2])\n        data_gg[,3] = as.numeric(data_gg[,3])\n        \n        # Read storm type\n        # Read storm type\n        \n        if(pv==2){\n          # Non-thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n        }\n        if(pv==3){\n          # Thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n        }\n        if(pv==4){\n          # Tropical Storm\n          data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n        }\n        \n        # Obtain minimum and maximum years of each station\n        yearr[f,1] = min(unique(year(data_gg[,1])))\n        yearr[f,2] = max(unique(year(data_gg[,1])))\n        \n        f = f + 1\n      }\n      \n      super_data = as.data.frame(\n        matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n               ncol = 2*length(file)))\n      super_data[,seq(2,length(file)*2,2)] = \n        min(yearr[,1]):max(yearr[,2])\n      \n      f = 1\n      \n      # Place max accordingly\n      while(f<length(file)+1){\n        data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n        \n        # Delete unnecessary rows until first data\n        data_gg = data_g[(grep(\"^Date\", \n                               data_g[,1])+1):length(data_g[,1]),]\n        \n        # assign data_gg column names as data_g's row 6 \n        # because it has column names aligned correctly\n        colnames(data_gg) <- data_g[6,]\n        \n        # Date/Time Format originally MM/DD/YYYY HH:MM\n        # Distorted to Excel Serial Date (start from 1900-01-01)\n        # numbers while loading. Hence, these are  \n        # converted to YYYY-MM-DD HH:MM format\n        data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                                 + as.POSIXct(\"1899-12-30 00:00\"))\n        \n        # Convert strings into numbers\n        data_gg[,2] = as.numeric(data_gg[,2])\n        data_gg[,3] = as.numeric(data_gg[,3])\n        \n        # Read storm type\n        if(pv==2){\n          # Non-thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n        }\n        if(pv==3){\n          # Thunderstorm\n          data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n        }\n        if(pv==4){\n          # Tropical Storm\n          data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n        }\n        \n        # Annual\n        {\n          i = 1\n          j = 1\n          k = 1\n          # no_YEARS: how many data points are there?\n          no_YEARS = length(unique(year(data_gg[,1])))\n          year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n          data_gg[,14] = year(data_gg[,1])\n          while(k<(no_YEARS)){\n            # Identify which data row is the last one\n            # from all of the year n\n            while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n              j = j + 1\n            }\n            j = i + j - 1\n            \n            if(m == 1){\n              # Mean\n              year[k] = sum(data_gg[i:j,3])/length(i:j)\n              type = \"Mean\"\n            }else if(m == 2){\n              # Median\n              year[k] = median(data_gg[i:j,3]) \n              type = \"Median\"\n            }else if(m == 3){\n              # Coefficient of Variation\n              year[k] = sd(data_gg[i:j,3])/\n                (sum(data_gg[i:j,3])/length(i:j)) \n              type = \"Coefficient of Variation\"\n            }else if(m == 4){\n              # Maximum\n              year_a[k,1] = max(data_gg[i:j,3])\n              year_a[k,2] = as.character(data_gg[i,1])\n              year_a[k,3] = year(data_gg[i,1])\n              type = \"Maximum\"\n            }else if(m == 5){\n              # Frequency\n              year[k] = length(i:j)\n              type = \"Frequency\"\n            }else{\n              # L-Moments\n              L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n                type = \"L-CV\"\n              }else if(n == 2){\n                year[k] = lsk\n                type = \"L-Skewness\"\n              }else{\n                year[k] = lku\n                type = \"L-Kurtosis\"\n              }\n            }\n            \n            # i variable is added by 1\n            # to move on to the next year\n            i = j + 1\n            j = 1\n            k = k + 1\n            if(k == (no_YEARS)){\n              last = length(data_gg[,3])\n              if(m == 1){\n                year[k] = sum(data_gg[i:last,3])/length(i:last)\n              }else if(m == 2){\n                year[k] = median(data_gg[i:last,3])\n              }else if(m == 3){\n                year[k] = sd(data_gg[i:last,3])/\n                  (sum(data_gg[i:last,3])/length(i:last)) \n              }else if(m == 4){\n                year_a[k,1] = max(data_gg[i:last,3])\n                year_a[k,2] = as.character(data_gg[last,1])\n                year_a[k,3] = year(data_gg[last,1])\n              }else if(m == 5){\n                year[k] = length(i:last)\n              }else{\n                L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n                lcv = L[2]/L[1] # L-cv\n                lsk = L[3]/L[2] # L-Skew\n                lku = L[4]/L[2] # L-Kurtosis\n                \n                l1 = fit$mle[1]/(1+fit$mle[2])\n                l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n                \n                t2[k,1] = l2/l1\n                t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n                \n                if(n == 1){\n                  year[k] = lcv\n                }else if(n == 2){\n                  year[k] = lsk\n                }else{\n                  year[k] = lku\n                }\n              }\n              k = k + 1\n            }\n          }\n          used_data = year_a[,1]\n          used_date = year_a[,2]\n        }\n        \n        # Obtain desired data\n        super_data[,2*(f-1)+1] = \n          ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n                 year_a[,1],NA)\n        \n        f = f + 1\n      }\n      \n      # Obtain max from every row, ignoring NA\n      cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n      vector.is.empty <- function(x) return(length(x) ==0 )\n      \n      dim_sup = dim(super_data[,seq(1,length(file)*2-1,2)])[2]\n      # Superstation values\n      if(vector.is.empty(dim_sup)){\n        used_data = super_data[,seq(1,length(file)*2-1,2)]\n        nas = which(is.na(used_data))\n        used_data = used_data[!is.na(used_data)]\n        yearr_t = min(yearr[,1]):max(yearr[,2])\n        yearr_t[nas] = NA\n        yearr_t = yearr_t[!is.na(yearr_t)]\n        \n        # Remove data points that are NA (not recorded)\n        # by indexing together\n        t = yearr_t - min(yearr_t)\n        t[which(is.na(used_data))] = NA\n        used_data <- used_data[!is.na(used_data)]\n        t <- t[!is.na(t)]\n      }else if(dim_sup>1){\n        used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n        yearr_t = min(yearr[,1]):max(yearr[,2])\n        \n        # Remove data points that are NA (not recorded)\n        # by indexing together\n        t = yearr_t - min(yearr_t)\n        t[which(is.na(used_data))] = NA\n        used_data <- used_data[!is.na(used_data)]\n        t <- t[!is.na(t)]\n      }\n      \n      used_data = log(used_data)\n      \n      # Significance\n      # Linear Regression Model\n      sta.lm = lm(used_data~t)\n      \n      # P-Values for Intercept and Trend Coefficients\n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[2,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      \n      # Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk = 2*(1-pnorm(abs(mk_z)))\n      \n      if(clu==clu.f){\n        cl.tabl[cl,6] = format(round(\n          summary(sta.lm)$coefficients[2,1], 5), nsmall = 5)\n        cl.tabl[cl,7] = format(round(\n          summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n        cl.tabl[cl,8] = format(round(alp,5), nsmall = 5)\n        cl.tabl[cl,9] = format(round(\n          pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n        cl.tabl[cl,10] = mk\n        cl.tabl[cl,12:14] = colnames(clusters$medoids)\n      }else{\n        cl.tabl[dimc + cl,6] = format(round(\n          summary(sta.lm)$coefficients[2,1], 5), nsmall = 5)\n        cl.tabl[dimc + cl,7] = format(round(\n          summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n        cl.tabl[dimc + cl,8] = format(round(alp,5), nsmall = 5)\n        cl.tabl[dimc + cl,9] = format(round(\n          pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n        cl.tabl[dimc + cl,10] = mk\n        cl.tabl[dimc + cl,12:14] = colnames(clusters$medoids)\n      }\n      \n      cl = cl + 1\n    }\n    \n    mm = 1\n    while(mm < max(clusters$clustering)+1){\n      mmm = which(clusters$clustering %in% mm)\n      points(mappoint$LON[mmm],mappoint$LAT[mmm],\n             col=colors[mm], cex=4, pch = 20)\n      mm = mm + 1\n    }\n    \n    # savePlot(\"clipboard\", type=\"wmf\")\n    dev.off()\n  }\n  \n  # Plotting\n  cl = 1\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    \n    file = paste0(\"station_matrix_\",\n                  kdata.sca[cl.index,1],\n                  \"_update.xlsx\")\n    \n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    n.good.fit = length(which(abs(dm.test$Z)<1.64))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    f = 1\n    ## Identify beginning and ending years\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      # Read storm type\n      \n      if(pv==1){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==2){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==3){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Obtain minimum and maximum years of each station\n      yearr[f,1] = min(unique(year(data_gg[,1])))\n      yearr[f,2] = max(unique(year(data_gg[,1])))\n      \n      f = f + 1\n    }\n    \n    super_data = as.data.frame(\n      matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n             ncol = 2*length(file)))\n    super_data[,seq(2,length(file)*2,2)] = \n      min(yearr[,1]):max(yearr[,2])\n    \n    f = 1\n    \n    # Place max accordingly\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,2]\n      }\n      \n      \n      # Obtain desired data\n      super_data[,2*(f-1)+1] = \n        ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n               year_a[,1],NA)\n      \n      f = f + 1\n    }\n    \n    # Obtain max from every row, ignoring NA\n    cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n    \n    if(dim(super_data)[2]<3){\n      super_data = super_data[which(!is.na(super_data[,1])),]\n      yearr_t = super_data[,2]\n      used_data = super_data[,1]\n    }else{\n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n      }\n    \n    # Remove data points that are NA (not recorded)\n    # by indexing together\n    t = yearr_t\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n    # Significance\n    # Linear Regression Model\n    sta.lm = lm(log(used_data)~t)\n    \n    # P-Values for Intercept and Trend Coefficients\n    t1 = sta.lm$coefficients[2]/\n      summary(sta.lm)$coefficients[2,2]\n    \n    alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n    \n    t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n      1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n    \n    # Log transformation and linear regression\n    ln_used_data = log(used_data)\n    lm.year = lm(ln_used_data~t)\n    m.year = lm(used_data~t)\n    sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t\n    rho = cor(ln_used_data,t)\n    \n    # Conditional Moments of y\n    mu_y.w = lm.year$coefficients[1] + \n      lm.year$coefficients[2]*t\n    vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)\n    sk_y.w = skewness(ln_used_data) - \n      skewness(t)*(lm.year$coefficients[2]^3)\n    \n    # Conditional Moments of x\n    mu_x.w = exp(mu_y.w + (vr_y.w)/2)\n    vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)\n    cv_x.w = sqrt(exp(vr_y.w)-1)\n    sk_x.w = 3*cv_x.w + cv_x.w^3\n    \n    # Stationary\n    mean.sta = mean(ln_used_data)\n    sd.sta = sqrt(sum((ln_used_data - \n                         mean(ln_used_data))^2)/\n                    length(ln_used_data))\n    \n    YR = c(10,20,30)\n    LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))\n    \n    {\n      # PPCC \n      b_coef = cor(ln_used_data, t)*sd(ln_used_data)/sd(t)\n      mu_y.pp = mean.sta + b_coef*(t-mean(t))\n      sd_y.pp = sqrt(var(ln_used_data)*(1-cor(ln_used_data, t)^2))\n      z_i.s = sort((ln_used_data - mean.sta)/sd.sta)\n      z_i.n = sort((ln_used_data - mu_y.pp)/sd_y.pp)\n      rank_i.s = rank(z_i.s)\n      rank_i.n = rank(z_i.n)\n      p_pos.s = (rank_i.s - 0.375)/(length(ln_used_data) + 0.25)\n      p_pos.n = (rank_i.n - 0.375)/(length(ln_used_data) + 0.25)\n      \n      z_ii.s = qnorm(p_pos.s)\n      z_ii.n = qnorm(p_pos.n)\n      if(cl==1){\n        if(clu==2){\n          png(filename=paste0(dir,\"/plots/\",texty[pv],\n                              \"/pp_opt\",clu,\".png\"),\n              w = 800, h = 400)\n          par(mar=c(8,4.5,2,2))\n          par(mfrow=c(1,2))\n          txt = 1\n          txt2 = 1\n        }else if(clu==3){\n          png(filename=paste0(dir,\"/plots/\",texty[pv],\n                              \"/pp_opt\",clu,\".png\"),\n              w = 900, h = 300)\n          par(mar=c(8,4.5,2,2))\n          par(mfrow=c(1,3))\n          txt = 1.5\n          txt2 = txt\n        }else if(clu==4){\n          png(filename=paste0(dir,\"/plots/\",texty[pv],\n                              \"/pp_opt\",clu,\".png\"),\n              w = 800, h = 800)\n          par(mar=c(8,4.5,2,2))\n          par(mfrow=c(2,2))\n          txt = 1.5\n          txt2 = txt\n        }\n      }\n    plot(z_ii.s,z_i.s, xlab = \"Normal Quantiles\",\n         ylab = \"Ordered Standardized Observations\",\n         type = \"n\", tck = 0.02,\n         xlim = c(-3,3), ylim = c(-3,3),\n         cex.lab = txt, xaxt= \"n\", yaxt =\"n\")\n    axis(1, cex.axis = txt)\n    axis(2, cex.axis = txt)\n    \n    points(z_ii.n,z_i.n, pch = 21, col = \"gray\",\n           bg = \"gray\", cex = 2)\n    points(z_ii.s,z_i.s, pch = 20)\n    abline(0,1, lwd = 1)\n    \n    corst = round(cor(z_ii.s,z_i.s),4)\n    corns = round(cor(z_ii.n,z_i.n),4)\n    \n    leg_s = bquote(PPCC[s] == .(corst))\n    leg_n = bquote(PPCC[ns] == .(corns))\n    text(-3,2.75,leg_s, pos = 4,\n         bty = \"n\",\n         cex = txt2)\n    text(-3,2.25,leg_n, pos = 4,\n         bty = \"n\",\n         cex = txt2)\n    text(3.25,-2.25, \n         paste0(clu,\"-Means\"), pos = 2,\n         cex = txt2)\n    text(3.25,-1.75, \n         text.storm, pos = 2,\n         cex = txt2)\n    text(3.25,-2.25, \n         paste0(clu,\"-Means\"), pos = 2,\n         cex = txt2)\n    text(3.25,-2.75, \n         paste(\"Cluster\",cl), pos = 2,\n         cex = txt2)\n    if(cl==length(unique(clusters$clustering))){\n      add_legend(\"bottom\", \n                 legend = c(\"Stationary\", \"Nonstationary\"),\n                 ncol = 2, pch = c(20, 21),\n                 col = c(\"black\", \"gray\"), pt.cex = c(1,2),\n                 pt.bg = c(NA, \"gray\"), bty =\"n\",\n                 cex = 2)\n      dev.off()\n    }\n    }\n    \n    if(dis==\"pe3\"){\n      # LP3\n      n_lp = length(ln_used_data)\n      sk_y.w.lp = (1+6/n_lp)*\n        ((n_lp*sum((ln_used_data-mean(ln_used_data))^3)))/\n        ((n_lp-1)*(n_lp-2)*sd(ln_used_data)^3)\n      a.lp3 = max(2/sk_y.w.lp,0.4)\n      b.lp3 = 1 + 0.0144*((max(0,sk_y.w.lp-2.25))^2)\n      f.lp3 = sk_y.w.lp - 0.063*(max(0,sk_y.w.lp-1)^1.85)\n      h.lp3 = (b.lp3 - 2/(sk_y.w.lp*a.lp3))^(1/3)\n      k.p_max2 = 1-((f.lp3/6)^2) + qnorm(1-(1/rp))*((f.lp3/6))\n      kp_i = 1\n      k.p = as.data.frame(\n        matrix(, nrow = length(k.p_max2), \n               ncol = 1))\n      for(kp_i in 1:length(k.p_max2)){\n        k.p[kp_i,] = max(h.lp3,k.p_max2[kp_i])\n      }\n      k.p = a.lp3*(k.p^3) - b.lp3\n      \n      ppp_sta = exp(mean.sta + k.p*sd.sta)\n      ppp_hom = exp(mean.hom + k.p*sd.hom)\n      ppp_het = exp(mean.het + k.p*sd.het)\n    }\n    if(dis==\"gno\"){\n      # LN2\n      ppp_sta.ln2 = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n      ppp_hom.ln2 = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n      ppp_het.ln2 = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n      \n      # LN3\n      # u = ln(x - tau)\n      # Stationarity\n      vr_x = sum((used_data-mean(used_data))^2)/\n        (length(used_data)-1)\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      \n      B = .5*(-g_x + sqrt(g_x^2 + 4))\n      the = (1 - B^(2/3))/(B^(1/3))\n      ome = the^2 + 1\n      \n      mu_u = .5*log(vr_x/(ome*(ome-1)))\n      vr_u = log(ome)\n      ta_x = mean(used_data) - sqrt(vr_x)/the\n      ppp_sta.ln3 = ta_x + exp(mu_u + qnorm(1-(1/rp))*sqrt(vr_u))\n      \n      # Nonstationarity\n      # Homogeneous\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      B_non = .5*(-sk_x.w.hom + sqrt(sk_x.w.hom^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.hom/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.hom - sqrt(vr_x.w.hom)/the_non\n      ppp_hom.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      # Heterogeneous\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      B_non = .5*(-sk_x.w.het + sqrt(sk_x.w.het^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.het/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.het - sqrt(vr_x.w.het)/the_non\n      ppp_het.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      ln.com = c(ppp_sta.ln2, ppp_hom.ln2, ppp_het.ln2,\n                 ppp_sta.ln3, ppp_hom.ln3, ppp_het.ln3)\n      ln.max = signif(ceiling(max(ln.com)), digits = 1)\n      ln.min = signif(round_any(min(ln.com), 10, f = ceiling), digits = 1)\n      plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(ln.min,ln.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.ln2, col = colors[1], lwd = 2)\n      lines(rp_gev, ppp_hom.ln2, col = colors[1], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln2, col = colors[1], lwd = 2, lty = 3)\n      lines(rp_gev, ppp_sta.ln3, col = colors[2], lwd = 2)\n      lines(rp_gev, ppp_hom.ln3, col = colors[2], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln3, col = colors[2], lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n      \n      \n    }\n    if(dis==\"gev\"){\n      dis = \"GEV\"\n      # Stationarity\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      if((g_x<=1.15)&(g_x>=-0.7)){\n        k_sta = 0.0087*g_x^3 + 0.0582^2 - 0.32*g_x + 0.2778\n      }else{\n        k_sta = -.31158*(1-exp(-.4556*(g_x - 0.97134)))\n      }\n      \n      a_sta = sign(k_sta)*k_sta*sqrt(vr_x)/\n        sqrt(gamma(1+2*k_sta)-(gamma(1+2*k_sta))^2)\n      e_sta = mean(used_data) - a_sta*(gamma(1+k_sta)-1)/k_sta\n      \n      pp = seq(0.01,1, length.out = 10001)\n      rp_gev = (1-pp)^-1\n      ppp_sta.gev = e_sta + (a_sta/k_sta)*(1-(-log(pp))^k_sta)\n      \n      # Nonstationarity\n      # Homogeneous\n      if((sk_x.w<=1.15)&(sk_x.w>=-0.7)){\n        k_non.hom = 0.0087*sk_x.w^3 + 0.0582^2 - 0.32*sk_x.w + 0.2778\n      }else{\n        k_non.hom = -.31158*(1-exp(-.4556*(sk_x.w - 0.97134)))\n      }\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      a_non.hom = sign(k_non.hom)*k_non.hom*sqrt(vr_x.w.hom)/\n        sqrt(gamma(1+2*k_non.hom)-(gamma(1+2*k_non.hom))^2)\n      e_non.hom = mean(used_data) - a_non.hom*(gamma(1+k_non.hom)-1)/k_non.hom\n      ppp_hom.gev = e_non.hom + (a_non.hom/k_non.hom)*(1-(-log(pp))^k_non.hom)\n      \n      # Heterogeneous\n      k_non.het = k_non.hom\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      a_non.het = sign(k_non.het)*k_non.het*sqrt(vr_x.w.het)/\n        sqrt(gamma(1+2*k_non.het)-(gamma(1+2*k_non.het))^2)\n      e_non.het = mean(used_data) - a_x*(gamma(1+k_non.het)-1)/k_non.het\n      ppp_het.gev = e_non.het + (a_non.het/k_non.het)*(1-(-log(pp))^k_non.het)\n      \n      last1700 = tail(which(rp_gev<1701),1)\n      gev.com = c(ppp_sta.gev, ppp_hom.gev, ppp_het.gev)[1:last1700]\n      gev.max = signif(ceiling(max(gev.com)), digits = 1)\n      gev.min = signif(round_any(min(gev.com), 10, f = ceiling), digits = 1)\n      plot(rp_gev,ppp_sta.gev, log=\"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(gev.min,gev.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.gev, lwd = 2)\n      lines(rp_gev, ppp_hom.gev, lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.gev, lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n    }\n    cl = cl + 1\n  }\n}\nfor (clu in clu.f:(clu.f+1)){\n  max_clu = get(paste(\"maxsos\",clu,sep=\"\"))\n  colclu = match(c(\"lat\",max_clu[2,c(2:3)]),colnames(kdata.sca))\n  clusters = pam(kdata.sca[,colclu], clu)\n\n  \n  # Plotting\n  cl = 1\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    \n    file = paste0(\"station_matrix_\",\n                  kdata.sca[cl.index,1],\n                  \"_update.xlsx\")\n    \n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    n.good.fit = length(which(abs(dm.test$Z)<1.64))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    f = 1\n    ## Identify beginning and ending years\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      # Read storm type\n      \n      if(pv==1){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==2){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==3){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Obtain minimum and maximum years of each station\n      yearr[f,1] = min(unique(year(data_gg[,1])))\n      yearr[f,2] = max(unique(year(data_gg[,1])))\n      \n      f = f + 1\n    }\n    \n    super_data = as.data.frame(\n      matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])), \n             ncol = 2*length(file)))\n    super_data[,seq(2,length(file)*2,2)] = \n      min(yearr[,1]):max(yearr[,2])\n    \n    f = 1\n    \n    # Place max accordingly\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,2]\n      }\n      \n      \n      # Obtain desired data\n      super_data[,2*(f-1)+1] = \n        ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),\n               year_a[,1],NA)\n      \n      f = f + 1\n    }\n    \n    # Obtain max from every row, ignoring NA\n    cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)\n    \n    if(dim(super_data)[2]<3){\n      super_data = super_data[which(!is.na(super_data[,1])),]\n      yearr_t = super_data[,2]\n      used_data = super_data[,1]\n    }else{\n      # Superstation values\n      used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)\n      yearr_t = min(yearr[,1]):max(yearr[,2])\n    }\n    \n    # Remove data points that are NA (not recorded)\n    # by indexing together\n    t = yearr_t\n    t[which(is.na(used_data))] = NA\n    used_data <- used_data[!is.na(used_data)]\n    t <- t[!is.na(t)]\n    \n    # Significance\n    # Linear Regression Model\n    sta.lm = lm(log(used_data)~t)\n    \n    # P-Values for Intercept and Trend Coefficients\n    t1 = sta.lm$coefficients[2]/\n      summary(sta.lm)$coefficients[2,2]\n    \n    alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n    \n    t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n      1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n    \n    # Log transformation and linear regression\n    ln_used_data = log(used_data)\n    lm.year = lm(ln_used_data~t)\n    m.year = lm(used_data~t)\n    sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t\n    rho = cor(ln_used_data,t)\n    \n    # Conditional Moments of y\n    mu_y.w = lm.year$coefficients[1] + \n      lm.year$coefficients[2]*t\n    vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)\n    sk_y.w = skewness(ln_used_data) - \n      skewness(t)*(lm.year$coefficients[2]^3)\n    \n    # Conditional Moments of x\n    mu_x.w = exp(mu_y.w + (vr_y.w)/2)\n    vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)\n    cv_x.w = sqrt(exp(vr_y.w)-1)\n    sk_x.w = 3*cv_x.w + cv_x.w^3\n    \n    # Stationary\n    mean.sta = mean(ln_used_data)\n    sd.sta = sqrt(sum((ln_used_data - \n                         mean(ln_used_data))^2)/\n                    length(ln_used_data))\n    \n    YR = c(10,20,30)\n    LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))\n    \n    t_i = 1\n    while(t_i<4){\n      t_non = c(tail(t,1),2020,2030)\n      # Nonstationary - Mean\n      mean.hom = unname(lm.year$coefficients[1] + \n                          lm.year$coefficients[2]*t_non[t_i])\n      sd.hom = unname(sqrt(var(ln_used_data)-\n                             lm.year$coefficients[2]^2*\n                             var(t)))\n      \n      # Nonstationary - Mean + Cv\n      res = (((lm.year$residuals)^(2))^(1/3))\n      lm.res = lm(res~t)\n      mean.het = mean.hom\n      sd.het = sqrt((lm.res$coefficients[1]+\n                       lm.res$coefficients[2]*tail(t,1))^3 +\n                      3*(summary(lm.res)$sigma^2)*\n                      (lm.res$coefficients[1]+\n                         lm.res$coefficients[2]*t_non[t_i]))\n      \n      # Generate return period data points\n      rp = seq(1.01,1700, length.out = 10001)\n      \n      ppp_sta.ln2 = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n      ppp_hom.ln2 = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n      ppp_het.ln2 = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n      \n      \n      if(t_i==1){\n        png(filename=paste0(dir,\"/plots/\",texty[pv],\n                            \"/cOPT\",clu,\"_\",cl,\".png\"),\n            w = 800, h = 600)\n        par(mfrow=c(1,2))\n        plot(t,ln_used_data, xlab = \"Year\",\n             ylab = \"ln (Peak Wind Gust)\",,\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.1)\n        axis(side=1, \n             at = c(1980,1990,2000,2010),\n             cex.axis = 1.5)\n        axis(side=2, \n             cex.axis = 1.5)\n        abline(lm(ln_used_data~t))\n        \n        plot(t, res, xlab = \"Year\",\n             ylab = expression(epsilon^{2/3}),\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.1)\n        abline(lm(res~t))\n        axis(side=1, \n             at = c(1980,1990,2000,2010),\n             cex.axis = 1.5)\n        axis(side=2, \n             cex.axis = 1.5)\n        dev.off()\n        \n        max.mean = unname(lm.year$coefficients[1] + \n                            lm.year$coefficients[2]*2030)\n        max.sd = sqrt((lm.res$coefficients[1]+\n                         lm.res$coefficients[2]*2030)^3 +\n                        3*(summary(lm.res)$sigma^2)*\n                        (lm.res$coefficients[1]+\n                           lm.res$coefficients[2]*2030))\n        max.hom = exp(max.mean + qnorm(1-(1/rp))*sd.hom)\n        max.het = exp(max.mean + qnorm(1-(1/rp))*max.sd)\n        ln2.max = round_any(max(max.hom,max.het,ppp_sta.ln2),\n                            10, \n                            f = ceiling)\n        if((pv==4)&(cl==2)){\n          ln2.max = 180\n        }\n        ln2.min = round_any(min(c(ppp_sta.ln2,ppp_hom.ln2,ppp_het.ln2)),\n                            10, f = floor)\n        \n        \n        par(mar = c(4,4,2,2))\n        png(filename=paste0(dir,\"/plots/\",texty[pv],\n                            \"/opt\",clu,\"_\",cl,\".png\"),\n            w = 800, h = 600)\n        plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n             xlim = c(1,3500), ylim = c(ln2.min,ln2.max),\n             xlab = \"Return Period\",\n             ylab = \"Peak wind gust (mph)\",\n             xaxt = \"n\",\n             yaxt = \"n\",\n             cex.lab = 1.5)\n        axis(side=1, labels = T, \n             at=c(1, 10, 50, 100,300,700,1700),\n             cex.axis = 1.5)\n        axis(side=2, labels = T, \n             at=seq(40,240,10),\n             cex.axis = 1)\n        abline(v=c(1, 10, 50, 100, 300,700, 1700), \n               h=seq(40,240,10), \n               col=\"gray\", lty=3)\n        legend(\"bottomright\", \n               c(\"Stationary\", \n                 \"Trend in Mean\",\n                 \"Trend in Mean + Cv\"), \n               lty = c(1,2,3),\n               lwd = 2,\n               cex = 1.5)\n        lines(rp, ppp_sta.ln2, col = colors[cl], lwd = 2)\n      }\n      \n      \n      lines(rp, ppp_hom.ln2, col = colors[cl], lwd = 1+t_i, lty = 2)\n      lines(rp, ppp_het.ln2, col = colors[cl], lwd = 1+t_i, lty = 3)\n      text(x= 1700, y= max(ppp_hom.ln2), cex = 0.75,\n           pos = 4, labels = paste(t_non[t_i], \"HOM\"))\n      text(x= 1700, y= max(ppp_het.ln2), cex = 0.75,\n           pos = 4, labels = paste(t_non[t_i], \"HET\"))\n      \n      LLL[1,t_i] = ppp_sta.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i] = ppp_sta.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i] = ppp_sta.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i] = ppp_sta.ln2[which.min(abs(rp-1700))]\n      LLL[1,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i + 3] = ppp_hom.ln2[which.min(abs(rp-1700))]\n      LLL[1,t_i + 6] = ppp_het.ln2[which.min(abs(rp-100))]\n      LLL[2,t_i + 6] = ppp_het.ln2[which.min(abs(rp-300))]\n      LLL[3,t_i + 6] = ppp_het.ln2[which.min(abs(rp-700))]\n      LLL[4,t_i + 6] = ppp_het.ln2[which.min(abs(rp-1700))]\n      \n      if(t_i==3){\n        assign(paste(\"LLL\",clu,cl,YR[t_i], sep = \"_\"),\n               LLL)\n      }\n      t_i = t_i + 1\n    }\n    \n    dev.off()\n    \n    if(dis==\"pe3\"){\n      # LP3\n      n_lp = length(ln_used_data)\n      sk_y.w.lp = (1+6/n_lp)*\n        ((n_lp*sum((ln_used_data-mean(ln_used_data))^3)))/\n        ((n_lp-1)*(n_lp-2)*sd(ln_used_data)^3)\n      a.lp3 = max(2/sk_y.w.lp,0.4)\n      b.lp3 = 1 + 0.0144*((max(0,sk_y.w.lp-2.25))^2)\n      f.lp3 = sk_y.w.lp - 0.063*(max(0,sk_y.w.lp-1)^1.85)\n      h.lp3 = (b.lp3 - 2/(sk_y.w.lp*a.lp3))^(1/3)\n      k.p_max2 = 1-((f.lp3/6)^2) + qnorm(1-(1/rp))*((f.lp3/6))\n      kp_i = 1\n      k.p = as.data.frame(\n        matrix(, nrow = length(k.p_max2), \n               ncol = 1))\n      for(kp_i in 1:length(k.p_max2)){\n        k.p[kp_i,] = max(h.lp3,k.p_max2[kp_i])\n      }\n      k.p = a.lp3*(k.p^3) - b.lp3\n      \n      ppp_sta = exp(mean.sta + k.p*sd.sta)\n      ppp_hom = exp(mean.hom + k.p*sd.hom)\n      ppp_het = exp(mean.het + k.p*sd.het)\n    }\n    if(dis==\"gno\"){\n      # LN2\n      ppp_sta.ln2 = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)\n      ppp_hom.ln2 = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)\n      ppp_het.ln2 = exp(mean.het + qnorm(1-(1/rp))*sd.het)\n      \n      # LN3\n      # u = ln(x - tau)\n      # Stationarity\n      vr_x = sum((used_data-mean(used_data))^2)/\n        (length(used_data)-1)\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      \n      B = .5*(-g_x + sqrt(g_x^2 + 4))\n      the = (1 - B^(2/3))/(B^(1/3))\n      ome = the^2 + 1\n      \n      mu_u = .5*log(vr_x/(ome*(ome-1)))\n      vr_u = log(ome)\n      ta_x = mean(used_data) - sqrt(vr_x)/the\n      ppp_sta.ln3 = ta_x + exp(mu_u + qnorm(1-(1/rp))*sqrt(vr_u))\n      \n      # Nonstationarity\n      # Homogeneous\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      B_non = .5*(-sk_x.w.hom + sqrt(sk_x.w.hom^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.hom/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.hom - sqrt(vr_x.w.hom)/the_non\n      ppp_hom.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      # Heterogeneous\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      B_non = .5*(-sk_x.w.het + sqrt(sk_x.w.het^2 + 4))\n      the_non = (1-B_non^(2/3))/(B_non^(1/3))\n      ome_non = the_non^2 + 1\n      mu_u.x = .5*log(vr_x.w.het/(ome_non*(ome_non-1)))\n      vr_u.x = log(ome_non)\n      ta_x.w = mu_x.w.het - sqrt(vr_x.w.het)/the_non\n      ppp_het.ln3 = ta_x.w + exp(mu_u.x + qnorm(1-(1/rp))*sqrt(vr_u.x))\n      \n      ln.com = c(ppp_sta.ln2, ppp_hom.ln2, ppp_het.ln2,\n                 ppp_sta.ln3, ppp_hom.ln3, ppp_het.ln3)\n      ln.max = signif(ceiling(max(ln.com)), digits = 1)\n      ln.min = signif(round_any(min(ln.com), 10, f = ceiling), digits = 1)\n      plot(rp,ppp_sta.ln2, log = \"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(ln.min,ln.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.ln2, col = colors[1], lwd = 2)\n      lines(rp_gev, ppp_hom.ln2, col = colors[1], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln2, col = colors[1], lwd = 2, lty = 3)\n      lines(rp_gev, ppp_sta.ln3, col = colors[2], lwd = 2)\n      lines(rp_gev, ppp_hom.ln3, col = colors[2], lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.ln3, col = colors[2], lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n      \n      \n    }\n    if(dis==\"gev\"){\n      dis = \"GEV\"\n      # Stationarity\n      g_x = length(used_data)*sum((used_data-mean(used_data))^3)/\n        ((length(used_data)-1)*(length(used_data)-2)*sqrt(vr_x)^3)\n      if((g_x<=1.15)&(g_x>=-0.7)){\n        k_sta = 0.0087*g_x^3 + 0.0582^2 - 0.32*g_x + 0.2778\n      }else{\n        k_sta = -.31158*(1-exp(-.4556*(g_x - 0.97134)))\n      }\n      \n      a_sta = sign(k_sta)*k_sta*sqrt(vr_x)/\n        sqrt(gamma(1+2*k_sta)-(gamma(1+2*k_sta))^2)\n      e_sta = mean(used_data) - a_sta*(gamma(1+k_sta)-1)/k_sta\n      \n      pp = seq(0.01,1, length.out = 10001)\n      rp_gev = (1-pp)^-1\n      ppp_sta.gev = e_sta + (a_sta/k_sta)*(1-(-log(pp))^k_sta)\n      \n      # Nonstationarity\n      # Homogeneous\n      if((sk_x.w<=1.15)&(sk_x.w>=-0.7)){\n        k_non.hom = 0.0087*sk_x.w^3 + 0.0582^2 - 0.32*sk_x.w + 0.2778\n      }else{\n        k_non.hom = -.31158*(1-exp(-.4556*(sk_x.w - 0.97134)))\n      }\n      mu_x.w.hom = exp(mean.hom + (sd.hom^2)/2)\n      vr_x.w.hom = exp(2*mean.hom + sd.hom^2)*(exp(sd.hom^2)-1)\n      cv_x.w.hom = sqrt(exp(sd.hom^2)-1)\n      sk_x.w.hom = 3*cv_x.w.hom + cv_x.w.hom^3\n      \n      a_non.hom = sign(k_non.hom)*k_non.hom*sqrt(vr_x.w.hom)/\n        sqrt(gamma(1+2*k_non.hom)-(gamma(1+2*k_non.hom))^2)\n      e_non.hom = mean(used_data) - a_non.hom*(gamma(1+k_non.hom)-1)/k_non.hom\n      ppp_hom.gev = e_non.hom + (a_non.hom/k_non.hom)*(1-(-log(pp))^k_non.hom)\n      \n      # Heterogeneous\n      k_non.het = k_non.hom\n      mu_x.w.het = exp(mean.het + (sd.het^2)/2)\n      vr_x.w.het = exp(2*mean.het + sd.het^2)*(exp(sd.het^2)-1)\n      cv_x.w.het = sqrt(exp(sd.het^2)-1)\n      sk_x.w.het = 3*cv_x.w.het + cv_x.w.het^3\n      \n      a_non.het = sign(k_non.het)*k_non.het*sqrt(vr_x.w.het)/\n        sqrt(gamma(1+2*k_non.het)-(gamma(1+2*k_non.het))^2)\n      e_non.het = mean(used_data) - a_x*(gamma(1+k_non.het)-1)/k_non.het\n      ppp_het.gev = e_non.het + (a_non.het/k_non.het)*(1-(-log(pp))^k_non.het)\n      \n      last1700 = tail(which(rp_gev<1701),1)\n      gev.com = c(ppp_sta.gev, ppp_hom.gev, ppp_het.gev)[1:last1700]\n      gev.max = signif(ceiling(max(gev.com)), digits = 1)\n      gev.min = signif(round_any(min(gev.com), 10, f = ceiling), digits = 1)\n      plot(rp_gev,ppp_sta.gev, log=\"x\", typ=\"n\", \n           xlim = c(1,1700), ylim = c(gev.min,gev.max),\n           xlab = \"Return Period\",\n           ylab = \"Recurrence Level (mph)\",\n           xaxt = \"n\")\n      axis(side=1, labels = T, at=c(100,300,700,1700))\n      abline(v=c(100, 300,700, 1700), \n             h=seq(40,160,5), \n             col=\"gray\", lty=3)\n      lines(rp_gev, ppp_sta.gev, lwd = 2)\n      lines(rp_gev, ppp_hom.gev, lwd = 2, lty = 2)\n      lines(rp_gev, ppp_het.gev, lwd = 2, lty = 3)\n      legend(\"bottomright\", \n             c(\"Stationary\", \n               \"Trend in Mean\",\n               \"Trend in Mean + Cv\"), \n             lty = c(1,2,3),\n             lwd = 2)\n    }\n    cl = cl + 1\n  }\n}\ncl.tabl[which(cl.tabl[,5]<1),11] = \"AHom\"\ncl.tabl[which(cl.tabl[,5]>2),11] = \"DHet\"\ncl.tabl[which((cl.tabl[,5]<2)&(cl.tabl[,5]>1)),11] = \"PHet\"\ncolnames(cl.tabl) = c(\"Cluster\",\"Cluster ID\", \"N\", \"N_d\", \n                      \"H\",\"SlopeC\",\"SlopeP\",\n                      \"TypeI\",\"TypeII\",\"MK\",\"Type\",\n                      \"Var1\",\"Var2\",\"Var3\")\nemptycol = as.data.frame(\n  matrix(\".\", nrow = dim(cl.tabl)[1], ncol = 1))\ncl.tabl = cbind(lll.cl.tabl,emptycol,cl.tabl)\nwrite.csv(cl.tabl,file = paste0(dir,\"/plots/\",\n                                texty[pv],\n                                \"/table11f.csv\"))\n\n# Slope Coefficient Plot\n# 3 Pval, 4 Type I, 5 Type II, 6 MK\nsig_tar = 4\nclu.f = floor(sqrt(dim(wanted)[1]/2))\nfor(clu in clu.f:(clu.f+1)){\n  \n  ref_st = read.csv(paste0(dir,\"/plots/table.csv\"), header = F)\n  \n  if(pv==1){\n    colnames(ref_st) = sapply(ref_st[2,], as.character)\n    ref_st = ref_st[3:9,]\n    ref_st = ref_st[,(which(ref_st ==\".\", arr.ind= T)[2,2]+1):dim(ref_st)[2]]\n  }else if(pv==2){\n    colnames(ref_st) = sapply(ref_st[12,], as.character)\n    ref_st = ref_st[13:19,]\n    ref_st = ref_st[,(which(ref_st ==\".\", arr.ind= T)[2,2]+1):dim(ref_st)[2]]\n  }else if(pv==3){\n    colnames(ref_st) = sapply(ref_st[22,], as.character)\n    ref_st = ref_st[23:27,]\n    ref_st = ref_st[,(which(ref_st ==\".\", arr.ind= T)[2,2]+1):dim(ref_st)[2]]\n    \n  }else if(pv == 4){\n    colnames(ref_st) = sapply(ref_st[30,], as.character)\n    ref_st = ref_st[31:35,]\n    ref_st = ref_st[,(which(ref_st ==\".\", arr.ind= T)[2,2]+1):dim(ref_st)[2]]\n    \n  }\n  \n  clusters = pam(kdata.sca[,colclu], clu)\n  das = c(1:clu)\n  cl = 1\n  \n  # Go to every cluster...\n  while(cl<length(unique(clusters$clustering))+1){\n    \n    cl.index = which(as.numeric(clusters$clustering)==cl)\n    dm.table = cbind(kdata[cl.index,1],\n                     wanted[cl.index,14],\n                     cov[cl.index,3],\n                     lmom[cl.index,8:11])\n    colnames(dm.table)[1:3] = c(\"id\", \"n\", \"mean\")\n    dm.test = regtst(dm.table[,1:6],5000)\n    is.dm.ex = \n      length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))\n    \n    # Check if there are discordant stations\n    if(is.dm.ex>0){\n      dm.sta = \n        dm.table[which(as.numeric(dm.test$D)>dm.test$Dcrit[1]),1]\n      dm.point = cbind(dis_ang[match(dm.sta,dis_ang[,2]),c(9,10)],cl)\n    }\n    f = 1\n    \n    ## Obtain desired data and remove discordant stations\n    file = paste0(\"station_matrix_\",\n                  kdata.sca[which(clusters$clustering==cl),1],\n                  \"_update.xlsx\")\n    if(is.dm.ex>0){\n      file_r = paste0(\"station_matrix_\",\n                      dm.sta,\n                      \"_update.xlsx\")\n      file = file[which(!(file %in% file_r))]\n    }\n    yearr = as.data.frame(\n      matrix(, nrow = , ncol = 2))\n    \n    sig_stat = as.data.frame(\n      matrix(, nrow = length(file), \n             ncol = 6))\n    colnames(sig_stat) = c(\"id\", \"Pval\", \"TypeI\",\n                           \"TypeII\", \"MK\", \"SlopeC\")\n    \n    f = 1\n    \n    # Obtain statistics\n    while(f<length(file)+1){\n      data_g = read.xlsx(paste(dir,\"/Lower_48/\",file[f],sep=\"\"))\n      \n      # Delete unnecessary rows until first data\n      data_gg = data_g[(grep(\"^Date\", \n                             data_g[,1])+1):length(data_g[,1]),]\n      \n      # assign data_gg column names as data_g's row 6 \n      # because it has column names aligned correctly\n      colnames(data_gg) <- data_g[6,]\n      \n      # Date/Time Format originally MM/DD/YYYY HH:MM\n      # Distorted to Excel Serial Date (start from 1900-01-01)\n      # numbers while loading. Hence, these are  \n      # converted to YYYY-MM-DD HH:MM format\n      data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600\n                               + as.POSIXct(\"1899-12-30 00:00\"))\n      \n      # Convert strings into numbers\n      data_gg[,2] = as.numeric(data_gg[,2])\n      data_gg[,3] = as.numeric(data_gg[,3])\n      \n      # Read storm type\n      if(pv==2){\n        # Non-thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,5]!=\"1\")),]\n      }\n      if(pv==3){\n        # Thunderstorm\n        data_gg <- data_gg[-c(which(data_gg[,7]!=\"1\")),]\n      }\n      if(pv==4){\n        # Tropical Storm\n        data_gg <- data_gg[-c(which(data_gg[,10]!=\"1\")),]\n      }\n      \n      # Annual\n      {\n        i = 1\n        j = 1\n        k = 1\n        # no_YEARS: how many data points are there?\n        no_YEARS = length(unique(year(data_gg[,1])))\n        year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n        data_gg[,14] = year(data_gg[,1])\n        while(k<(no_YEARS)){\n          # Identify which data row is the last one\n          # from all of the year n\n          while(data_gg[i + j - 1,14] == data_gg[i + j,14]){\n            j = j + 1\n          }\n          j = i + j - 1\n          \n          if(m == 1){\n            # Mean\n            year[k] = sum(data_gg[i:j,3])/length(i:j)\n            type = \"Mean\"\n          }else if(m == 2){\n            # Median\n            year[k] = median(data_gg[i:j,3]) \n            type = \"Median\"\n          }else if(m == 3){\n            # Coefficient of Variation\n            year[k] = sd(data_gg[i:j,3])/\n              (sum(data_gg[i:j,3])/length(i:j)) \n            type = \"Coefficient of Variation\"\n          }else if(m == 4){\n            # Maximum\n            year_a[k,1] = max(data_gg[i:j,3])\n            year_a[k,2] = as.character(data_gg[i,1])\n            year_a[k,3] = year(data_gg[i,1])\n            type = \"Maximum\"\n          }else if(m == 5){\n            # Frequency\n            year[k] = length(i:j)\n            type = \"Frequency\"\n          }else{\n            # L-Moments\n            L = Lmoments(data_gg[i:j,3], na.rm=TRUE)\n            lcv = L[2]/L[1] # L-cv\n            lsk = L[3]/L[2] # L-Skew\n            lku = L[4]/L[2] # L-Kurtosis\n            \n            l1 = fit$mle[1]/(1+fit$mle[2])\n            l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n            \n            t2[k,1] = l2/l1\n            t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n            \n            if(n == 1){\n              year[k] = lcv\n              type = \"L-CV\"\n            }else if(n == 2){\n              year[k] = lsk\n              type = \"L-Skewness\"\n            }else{\n              year[k] = lku\n              type = \"L-Kurtosis\"\n            }\n          }\n          \n          # i variable is added by 1\n          # to move on to the next year\n          i = j + 1\n          j = 1\n          k = k + 1\n          if(k == (no_YEARS)){\n            last = length(data_gg[,3])\n            if(m == 1){\n              year[k] = sum(data_gg[i:last,3])/length(i:last)\n            }else if(m == 2){\n              year[k] = median(data_gg[i:last,3])\n            }else if(m == 3){\n              year[k] = sd(data_gg[i:last,3])/\n                (sum(data_gg[i:last,3])/length(i:last)) \n            }else if(m == 4){\n              year_a[k,1] = max(data_gg[i:last,3])\n              year_a[k,2] = as.character(data_gg[last,1])\n              year_a[k,3] = year(data_gg[last,1])\n            }else if(m == 5){\n              year[k] = length(i:last)\n            }else{\n              L = Lmoments(data_gg[i:last,3], na.rm=TRUE)\n              lcv = L[2]/L[1] # L-cv\n              lsk = L[3]/L[2] # L-Skew\n              lku = L[4]/L[2] # L-Kurtosis\n              \n              l1 = fit$mle[1]/(1+fit$mle[2])\n              l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))\n              \n              t2[k,1] = l2/l1\n              t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])\n              \n              if(n == 1){\n                year[k] = lcv\n              }else if(n == 2){\n                year[k] = lsk\n              }else{\n                year[k] = lku\n              }\n            }\n            k = k + 1\n          }\n        }\n        used_data = year_a[,1]\n        used_date = year_a[,3]\n      }\n      \n      t = used_date - min(used_date)\n      \n      # Significance\n      # Linear Regression Model\n      sta.lm = lm(log(used_data)~t)\n      \n      # P-Values for Intercept and Trend Coefficients\n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[2,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      ## Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk = 2*(1-pnorm(abs(mk_z)))\n      \n      \n      colnames(sig_stat) = c(\"id\", \"SlopeC\", \"Pval\", \n                             \"TypeI\", \"TypeII\", \"MK\")\n      sig_stat[f,1] = as.numeric(substr(file[f],16,21))\n      sig_stat[f,2] = format(round(\n        summary(sta.lm)$coefficients[2,1], 5), nsmall = 5)\n      sig_stat[f,3] = format(round(\n        summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n      sig_stat[f,4] = format(round(alp,5), nsmall = 5)\n      sig_stat[f,5] = format(round(\n        pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n      sig_stat[f,6] = mk\n      \n      f = f + 1\n    }\n    \n    \n    if(cl==1){\n      par(mar = c(4,4,2,2))\n      png(filename=paste0(dir,\"/plots/\",texty[pv],\n                          \"/slopeoptC\",clu,\".png\"),\n          w = 800, h = 600)\n      plot(das,\n           seq(-.03,.03, length.out=clu),\n           type = \"n\",\n           xlab = \"Clusters\",\n           ylab = \"Slope Coefficient\",\n           xaxt = \"n\",\n           xlim = c(0.5,clu+0.5),\n           ylim = c(-0.03,0.03),\n           cex.lab = 1.5)\n    }\n    \n    if(as.numeric(sapply((\n      subset(ref_st,ref_st[,1]==clu&ref_st[,2]==cl)[sig_tar+4]),\n      as.character))>0.05){\n      rect(cl-.5,-1,cl+.5,1,col = rgb(0.5,0.5,0.5,1/4),\n           border = NA)\n    }\n    # Significant points\n    cl_si = which(sig_stat[,sig_tar]<0.05)\n    points(seq(cl,cl,length.out=length(cl_si)),\n           sig_stat[cl_si,2],\n           pch = 3,cex = 2)\n    # Insignificant points\n    cl_insi = which(sig_stat[,sig_tar]>0.050001)\n    points(seq(cl,cl,length.out=length(cl_insi)),\n           sig_stat[cl_insi,2],\n           pch = 19,cex = 0.5)\n    # Superstation point\n    points(cl,\n           subset(ref_st,ref_st[,1]==clu.f&ref_st[,2]==cl)[6],\n           pch= 18, col = \"red\")\n    \n    abline(h = 0, col = \"gray\", lty = \"dotted\")\n    axis(1, at=cl, labels = cl, tick = T)\n    cl = cl + 1\n  }\n  legend(\"bottomleft\",\n         legend=c(\"Insignificant\",\n                  \"Significant\",\n                  \"Cluster Superstation\"),\n         col=c(\"black\",\"black\",\"red\"),\n         pch=c(19,3,18),\n         pt.cex=c(0.5,2,1))\n  dads = as.numeric(sapply(ref_st[which(ref_st[,1]==clu.f),6],\n                           as.character))\n  points(das,dads[das], pch= 18, col = \"red\")\n  dev.off\n}\n\n\n\n\nplot(X_ST,Y_ST, xlab = \"Normal Quantiles\",\n     ylab = \"Ordered Standardized Observations\",\n     type = \"n\", tck = 0.02,\n     xlim = c(-3,3), ylim = c(-3,3))\npoints(X_NS,Y_NS, pch = 21, col = \"gray\",\n       bg = \"gray\", cex = 2)\npoints(X_ST,Y_ST, pch = 20)\nabline(0,1, lwd = 2)\nleg_s = bquote(PPCC[s] == .(corst))\nleg_n = bquote(PPCC[ns] == .(corns))\ntext(-3,2.75,leg_s, pos = 4,\n     bty = \"n\",\n     cex = 1.25)\ntext(-3,2.25,leg_n, pos = 4,\n     bty = \"n\",\n     cex = 1.25)\ntext(3.25,-2.5, text_plot, pos = 2,\n     cex = 1.75)\n\ntype_w = type_w + 1\n\n\n## Real Space\n\nwhile(type_w < 4){\n  \n  ######################################\n  ######################################\n  ######################################\n  if(type_w==0){\n    par(mfrow=c(3,1)) \n    y_min = min(year[,3])\n  }\n\n  \n  lm.year = lm(used_data~t)\n  \n  # Conditional Moment\n  mean_y_w = unname(mean(used_data) + \n                      lm.year$coefficients[2]*(t-mean(t)))\n  \n  X_cor = lm.year$coefficients[2]*sd(t)/sd(ln_used_data)\n  sd_y_w = sqrt((1-X_cor^2)*var(ln_used_data))\n  skew_y_w = skewness(ln_used_data) - \n    lm.year$coefficients[2]^3*skewness(t)\n  \n  Y_ST = sort((ln_used_data - mean(ln_used_data))/sd(ln_used_data))\n  Y_NS = sort((ln_used_data - mean_y_w)/sd_y_w)\n  \n  t_w_ST = t[order((ln_used_data - mean(ln_used_data))/sd(ln_used_data))]\n  t_w_NS = t[order((ln_used_data - mean_y_w)/sd_y_w)]\n  \n  Y_ST = (Y_ST + mean(ln_used_data))/sd(ln_used_data) \n  Y_NS = (Y_NS + mean(ln_used_data) + \n            lm.year$coefficients[2]*(t_w_NS-mean(t)))/sd_y_w\n  p_i = (1:length(ln_used_data)-0.375)/(length(ln_used_data)+0.25)\n  X = qnorm(p_i)\n  X_ST = exp(mean(ln_used_data) + X*sd(ln_used_data))\n  X_NS = exp(mean(ln_used_data) + lm.year$coefficients[2]*(t_w_NS-mean(t)) \n             + X*sd(ln_used_data))\n  corst = round(cor(X_ST,Y_ST),4)\n  corns = round(cor(X_NS,Y_NS),4)\n  \n  if(type_w == 0){\n    par(mfrow=c(2,2), new=TRUE)\n  }\n  \n  plot(X_ST,Y_ST, xlab = \"Normal Quantiles\",\n       ylab = \"Ordered Observations\",\n       type = \"n\", tck = 0.02,\n       xlim = c(0,100), ylim = c(0,100))\n  points(X_NS,Y_NS, pch = 21, col = \"gray\",\n         bg = \"gray\", cex = 2)\n  points(X_ST,Y_ST, pch = 20)\n  abline(0,1, lwd = 2)\n  leg_s = bquote(PPCC[s] == .(corst))\n  leg_n = bquote(PPCC[ns] == .(corns))\n  text(0,95,leg_s, pos = 4,\n       bty = \"n\",\n       cex = 1.25)\n  text(0,80,leg_n, pos = 4,\n       bty = \"n\",\n       cex = 1.25)\n  text(105,5, text_plot, pos = 2,\n       cex = 1.75)\n  \n  type_w = type_w + 1\n}\n\nlegend(\"topleft\",\n       legend = c(\n         substitute(paste(\n           PPCC[st],\" = \",cors),\n           list(cors = corst)),\n         substitute(paste(\n           PPCC[ns],\" = \",corn),\n           list(corn = corns))),\n       bty = \"n\",\n       cex = 1.25)\n\nlegend(\"topleft\",\n       legend = c(\n         substitute(paste(\n           PPCC[st],\" = \",cors),\n           list(cors = corst)),\n         substitute(paste(\n           PPCC[ns],\" = \",corn),\n           list(corn = corns))),\n       bty = \"n\",\n       cex = 1.25)\n\nlegend('bottomright',\n       legend = c(\"Theoretical\",\n                  \"Nonstationary\",\n                  \"Stationary\"),\n       lwd = c(2,NA,NA),lty = c(1,NA,NA),\n       pch = c(NA,21,20),pt.cex = c(NA,2,1),\n       col = c(\"black\",\"gray\",\"black\"),\n       pt.bg = c(NA,\"gray\",\"black\"))\n\n\n\n\n\n\n\nsavePlot(\"clipboard\", type=\"wmf\")\nwhile(d<6){\n  if(d==1){\n    c = as.numeric(readline(\"Number of clusters? \"))\n    dd <- readline(\"Type of data:\n    ======================================\n    lat: latitude\n    lon: longitude\n    t: L-CV\n    d: distance to shore\n    ang: angle\n                  \")\n  }else if(d>1){\n    dd <- readline(paste(\"Currently selected: \",var,\n    \n    \"Type of data:\n    ======================================\n    lat: latitude\n    lon: longitude\n    t: L-CV\n    d: distance to shore\n    ang: angle\n\n    0 to stop\n                  \",sep = \"\"))\n  }\n  \n  if(dd==0){\n    d = 6\n  }else if(match(dd,colnames(kdata.sca)[2:10])>0){\n    var[d,1] = dd\n    d = d + 1\n  }else{\n    message(\"Must select from given options\")\n  }\n}\n\n\n\n\n\n\n\n\n\nasdf = matrix(, nrow = 7, ncol = 2)\nfor(i in 1:7){\n  asd = kmeans(kdata.sca[,c(2,3,6)],i)\n  asdf[i,1] = i\n  asdf[i,2] = as.matrix(asd$tot.withinss)\n}\nplot(asdf,type=\"l\")\n\n\nfor(i in 1:7){\n  asd = kmeans(mydata,i)\n  print(asd$tot.withinss)\n}\n\nselectedData <- kdata[var[,1]]\nclusters <- kmeans(selectedData, c)\nkmeanPlot <- par(mar = c(5.1, 4.1, 0, 1))\nplot(selectedData,\n     col = clusters$cluster,\n     pch = 20, cex = 3)\nkdata_clust = cbind(kdata,clusters$cluster)\n\n\n\n\n\nregtst(cbind(lmom[,c(1:3,8:11)]),5000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nassign(paste(\"lmom\",pv,sep=\"\"),lmom)\n\n\nwhile(pv<5){\n  stations_dup = stations[!duplicated(stations[,2]),]\n  stations_dup = cbind(stations_dup,d_sign[,pv],\n                       time_rec[,pv],\n                       mk[,pv])\n  stations_dup[which(is.na(stations_dup[,14])),14] = 0\n  stations_dup[which(is.na(stations_dup[,15])),15] = 1\n  colnames(stations_dup)[13:15] = c(\"sign\",\"time\",\"mk\")\n  sigsta = \n    which((stations_dup[,14]>=no[pv])&(stations_dup[,15]<0.050001))\n  new_stations = stations_dup[sigsta,]\n  wanted = as.data.frame(new_stations)\n  \n  assign(\"lmom\",as.data.frame(\n    matrix(, nrow = dim(wanted)[1], ncol = 10)))\n  \n  file_i = 1\n  while(file_i<length(sigsta) + 1){ \n    data = read.xlsx(\n      paste(dir,\"Lower_48\",\n            paste(\"station_matrix_\",new_stations[file_i,2],\n                  \"_update.xlsx\",sep=\"\"),\n            sep=\"/\"))\n    colnames(data) <- data[6,]\n    \n    # Delete unnecessary rows until first data\n    data = data[(grep(\"^Date\", \n                      data[,1])+1):length(data[,1]),]\n    \n    data[,1] <-as.POSIXct(as.numeric(data[,1])*24*3600\n                          + as.POSIXct(\"1899-12-30 00:00\"))\n    \n    # Convert strings into numbers\n    data[,2] = as.numeric(data[,2])\n    data[,3] = as.numeric(data[,3])\n    \n    # Read storm type\n    if(pv==1){\n      # Commingled\n      data_u <- data\n    }\n    if(pv==2){\n      # Non-thunderstorm\n      data_u <- data[-c(which(data[,5]!=\"1\")),]\n    }\n    if(pv==3){\n      # Thunderstorm\n      data_u <- data[-c(which(data[,7]!=\"1\")),]\n    }\n    if(pv==4){\n      # Tropical Storm\n      data_u <- data[-c(which(data[,10]!=\"1\")),]\n    }\n    \n    # no_YEARS: how many data points are there?\n    no_YEARS = length(unique(year(data_u[,1])))\n    if(no_YEARS < 2){\n      sigval[2*pv-1,file_i] = NaN\n      sigval[2*pv,file_i] = NaN\n      \n      sigval_type[2*pv-1,file_i] = NaN\n      sigval_type[2*pv,file_i] = NaN\n      \n      # Mann Kendall P-Values\n      mk[pv,file_i] = NaN\n      \n      d_sign[pv,file_i] = NaN\n    }\n    else{\n      i = 1\n      j = 1\n      k = 1\n      year = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n      data_u[,14] = year(data_u[,1])\n      while(k<(no_YEARS)){\n        # Identify which data row is the last one\n        # from all of the year n\n        while(data_u[i + j - 1,14] == data_u[i + j,14]){\n          j = j + 1\n        }\n        j = i + j - 1\n        \n        if(m == 1){\n          # Mean\n          year[k,1] = sum(data_u[i:j,3])/length(i:j)\n          type = \"Mean\"\n        }else if(m == 2){\n          # Median\n          year[k,1] = median(data_u[i:j,3]) \n          type = \"Median\"\n        }else if(m == 3){\n          # Coefficient of Variation\n          year[k,1] = sd(data_u[i:j,3])/\n            (sum(data_u[i:j,3])/length(i:j)) \n          type = \"Coefficient of Variation\"\n        }else if(m == 4){\n          # Maximum\n          year[k,1] = max(data_u[i:j,3])\n          year[k,2] = data_u[i,1]\n          year[k,3] = year(data_u[i,1])\n          type = \"Maximum\"\n        }else{\n          # Frequency\n          year[k,1] = length(i:j)\n          type = \"Frequency\"\n        }\n        \n        # i variable is added by 1\n        # to move on to the next year\n        i = j + 1\n        j = 1\n        k = k + 1\n        if(k == (no_YEARS)){\n          last = length(data_u[,3])\n          if(m == 1){\n            year[k,1] = sum(data_u[i:last,3])/length(i:last)\n          }else if(m == 2){\n            year[k,1] = median(data_u[i:last,3])\n          }else if(m == 3){\n            year[k,1] = sd(data_u[i:last,3])/\n              (sum(data_u[i:last,3])/length(i:last)) \n          }else if(m == 4){\n            year[k,1] = max(data_u[i:last,3])\n            year[k,2] = data_u[last,1]\n            year[k,3] = year(data_u[last,1])\n          }else{\n            year[k,1] = length(i:last)\n          }\n          k = k + 1\n        }\n      }\n      used_data = year[,1]\n      used_date = year[,3]\n      \n      t = used_date - min(used_date)\n      t[which(is.na(used_data))] = NA\n      used_data <- used_data[!is.na(used_data)]\n      t <- t[!is.na(t)]\n      \n    }\n    \n    lmom[file_i,1] = \n      new_stations[file_i,1]\n    lmom[file_i,2] = \n      length(used_data)\n    lmom[file_i,3:7] = \n      Lmoments(used_data,rmax=5)\n    file_i = file_i + 1\n  }\n  # write.csv(stations_dup,\n  #           file = paste(\"risk2\",mi,\"_\",sig,pv,\".csv\",sep=\"\"))\n  lmom[,8] = lmom[,4]/lmom[,3]\n  lmom[,9] = lmom[,5]/lmom[,4]\n  lmom[,10] = lmom[,6]/lmom[,4]\n  lmom[,11] = lmom[,7]/lmom[,4]\n  colnames(lmom) <- c(\"Station\",\"n\",\"mean\",\"l2\",\n                      \"l3\",\"l4\",\"l5\",\"t\",\"t3\",\"t4\",\"t5\")\n  assign(paste(\"lmom\",pv,sep=\"\"),lmom)\n  pv = pv + 1\n}\n\nregtst(cbind(lmom[,c(1:3,8:11)]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nu_i = as.matrix(lmom1[,c(6,7,8)])\nns = length(lmom1[,1])\nu_u = rep.row(t(colSums(u_i)/ns),20)\n\ni = 1\nwhile(i<ns+1){\n  t(u_i[i,]-u_u[i,]) %*% (u_i[i,]-u_u[i,])\n  matrix(u_i[1,]-u_u)%*%t(matrix(u_i[1,]-u_u))\n}\n\nS = (t(u_i-u_u) %*% (u_i-u_u))/(ns-1)\n(u_i-u_u)*solve(S)*t(u_i-u_u)\ni = 1\nassign(\"S\",as.data.frame(\n  matrix(, nrow = length(sigsta), ncol = 8)))\n\nu_u = u_u[rep(1:nrow(u_u),each=ns),] \nS = ((ns-1)^-1)*colSums(t(u_i-u_u)*(u_i-u_u))\nD = (ns/(3*(ns-1)))*(u_i-u_u)*(S^-1)*t(u_i-u_u)\n\n# Identify stations that are significant / MK\npv = 1\nwhile(pv<5){\n  indices[1:length(which(mk[,pv] < 0.050001)),pv] = \n    as.numeric(\n      substr(file[req_stations[which(mk[,pv] < 0.050001)]],\n             16,21))\n  \n  sig = \"mk\"\n  \n  stations_dup = stations[!duplicated(stations[,2]),]\n  stations_dup = cbind(stations_dup,d_sign[,pv],\n        time_rec[,pv])\n  colnames(stations_dup)[13:14] = c(\"sign\",\"time\")\n  req_stations_sig = \n    which(stations_dup[,2] %in% indices[,pv])\n  write.csv(stations_dup[req_stations_sig,],\n            file = paste(\"risk2\",mi,\"_\",sig,pv,\".csv\",sep=\"\"))\n  pv = pv + 1\n}\n\n# Copy stations that are significant!\npv = 1\nwhile(pv<5){\n  pvt = 1\n  while(pvt<(which(is.na(indices)[,pv])[1])){\n    file.copy(\n      from = \n        paste(dir,\"/Lower_48/station_matrix_\",\n              indices[pvt,pv],\"_update.xlsx\",sep=\"\"),\n      to = paste(dir,\"/Lower_48_Filter\",mi,\"/station_matrix_\",\n                 indices[pvt,pv],\"_update.xlsx\",sep=\"\"),\n      overwrite=T)\n    pvt = pvt + 1\n  }\n  pv = pv + 1\n}\n\n# Reiterate again with the filtered stations\nfile = list.files(\n  paste(dir,paste(\"Lower_48_Filter\",mi,sep=\"\"),sep=\"/\"),\n  pattern=paste(\"^station_matrix_\",sep=\"\"))\nstations = read.xlsx(paste(\"stations_risk2\",mi,\".xlsx\",sep=\"\"))\nstations_dup = stations[!duplicated(stations[,2]),]\nreq_stations = \n  which(as.numeric(substr(file,16,21)) %in% stations[,2])\nreq_stations_s = \n  which(stations_dup[,2] %in% as.numeric(substr(file,16,21)))\nreq_file = file[req_stations]\n\n{\n  # DF Sign\n  d_sign = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n  \n  # DF Duplicates\n  d_dup = as.data.frame(\n    matrix(, nrow = 1, ncol = length(req_file)))\n  \n  # DF Time\n  time_rec = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n  \n  # DF Significance P-Values\n  sigval = as.data.frame(\n    matrix(, nrow = 8, ncol = length(req_file)))\n  sigval_type = as.data.frame(\n    matrix(, nrow = 8, ncol = length(req_file)))\n  \n  # DF Indices\n  indices = as.data.frame(\n    matrix(, nrow = length(req_file), ncol = 4))\n  \n  # DF MannKendall\n  mk = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n  mk1 = as.data.frame(\n    matrix(, nrow = 4, ncol = length(req_file)))\n}\n\nfile_i = 1\ntype_w = 0\nn = c(30,30,30,5)\n\nwhile(file_i<length(req_file) + 1){ \n  data = read.xlsx(paste(dir,\"Lower_48\",req_file[file_i],sep=\"/\"))\n  colnames(data) <- data[6,]\n  \n  # Delete unnecessary rows until first data\n  data = data[(grep(\"^Date\", \n                    data[,1])+1):length(data[,1]),]\n  \n  data[,1] <-as.POSIXct(as.numeric(data[,1])*24*3600\n                        + as.POSIXct(\"1899-12-30 00:00\"))\n  \n  # Convert strings into numbers\n  data[,2] = as.numeric(data[,2])\n  data[,3] = as.numeric(data[,3])\n  \n  while(type_w<4){\n    # Read storm type\n    if(type_w==0){\n      # Commingled\n      data_u <- data\n    }\n    if(type_w==1){\n      # Non-thunderstorm\n      data_u <- data[-c(which(data[,5]!=\"1\")),]\n    }\n    if(type_w==2){\n      # Thunderstorm\n      data_u <- data[-c(which(data[,7]!=\"1\")),]\n    }\n    if(type_w==3){\n      # Tropical Storm\n      data_u <- data[-c(which(data[,10]!=\"1\")),]\n    }\n    \n    # no_YEARS: how many data points are there?\n    no_YEARS = length(unique(year(data_u[,1])))\n    if(no_YEARS < 2){\n      sigval[2*(type_w)+1,file_i] = NaN\n      sigval[2*(type_w)+2,file_i] = NaN\n      \n      sigval_type[2*(type_w)+1,file_i] = NaN\n      sigval_type[2*(type_w)+2,file_i] = NaN\n      \n      # Mann Kendall P-Values\n      mk[type_w+1,file_i] = NaN\n      \n      d_sign[type_w+1,file_i] = NaN\n    }\n    else{\n      i = 1\n      j = 1\n      k = 1\n      year = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))\n      data_u[,14] = year(data_u[,1])\n      while(k<(no_YEARS)){\n        # Identify which data row is the last one\n        # from all of the year n\n        while(data_u[i + j - 1,14] == data_u[i + j,14]){\n          j = j + 1\n        }\n        j = i + j - 1\n        \n        if(m == 1){\n          # Mean\n          year[k,1] = sum(data_u[i:j,3])/length(i:j)\n          type = \"Mean\"\n        }else if(m == 2){\n          # Median\n          year[k,1] = median(data_u[i:j,3]) \n          type = \"Median\"\n        }else if(m == 3){\n          # Coefficient of Variation\n          year[k,1] = sd(data_u[i:j,3])/\n            (sum(data_u[i:j,3])/length(i:j)) \n          type = \"Coefficient of Variation\"\n        }else if(m == 4){\n          # Maximum\n          year[k,1] = max(data_u[i:j,3])\n          year[k,2] = data_u[i,1]\n          year[k,3] = year(data_u[i,1])\n          type = \"Maximum\"\n        }else{\n          # Frequency\n          year[k,1] = length(i:j)\n          type = \"Frequency\"\n        }\n        \n        # i variable is added by 1\n        # to move on to the next year\n        i = j + 1\n        j = 1\n        k = k + 1\n        if(k == (no_YEARS)){\n          last = length(data_u[,3])\n          if(m == 1){\n            year[k,1] = sum(data_u[i:last,3])/length(i:last)\n          }else if(m == 2){\n            year[k,1] = median(data_u[i:last,3])\n          }else if(m == 3){\n            year[k,1] = sd(data_u[i:last,3])/\n              (sum(data_u[i:last,3])/length(i:last)) \n          }else if(m == 4){\n            year[k,1] = max(data_u[i:last,3])\n            year[k,2] = data_u[last,1]\n            year[k,3] = year(data_u[last,1])\n          }else{\n            year[k,1] = length(i:last)\n          }\n          k = k + 1\n        }\n      }\n      used_data = year[,1]\n      used_date = year[,3]\n      \n      t = used_date - min(used_date)\n      t[which(is.na(used_data))] = NA\n      used_data <- used_data[!is.na(used_data)]\n      t <- t[!is.na(t)]\n      \n      time_rec[type_w+1,file_i] = length(t)\n      \n      # Linear Regression Model\n      sta.lm = lm(log(used_data)~t)\n      \n      if(sta.lm$coefficients[2]>0)\n      {d_sign[type_w+1,file_i] = \"P\"}\n      else{\n        d_sign[type_w+1,file_i] = \"N\"\n      }\n      \n      if(all((table(used_data)==1)==T)==F){      \n        d_dup[file_i] = 2\n      }\n      \n      # P-Values for Intercept and Trend Coefficients\n      sigval[2*(type_w)+1,file_i] = format(round(\n        summary(sta.lm)$coefficients[1,4], 4), nsmall = 5)\n      sigval[2*(type_w)+2,file_i] = format(round(\n        summary(sta.lm)$coefficients[2,4], 5), nsmall = 5)\n      \n      t1 = sta.lm$coefficients[2]/\n        summary(sta.lm)$coefficients[1,2]\n      \n      alp = pt(t1,df = length(used_data)-2, lower.tail = F)\n      \n      t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) - \n        1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))\n      \n      sigval_type[2*(type_w)+1,file_i] = format(round(alp,5), nsmall = 5)\n      sigval_type[2*(type_w)+2,file_i] = format(round(\n        pt(t2,df = length(used_data)-2, lower.tail = T),5), nsmall = 5)\n      \n      # Mann Kendall P-Values\n      mk1[type_w+1,file_i] = format(round(\n        as.numeric(mk.test(ts(used_data, start = used_date[1],\n                              end = used_date[length(used_date)]))[5]),5),\n        nsmall = 5)\n      \n      # Manual MK\n      # S\n      mk_s = 0\n      mk_j = 1\n      while(mk_j<length(used_data)){\n        mk_i = mk_j + 1\n        while(mk_i<length(used_data)+1){\n          mk_s = mk_s + sign(used_data[mk_i] - used_data[mk_j])\n          mk_i = mk_i + 1\n        }\n        mk_j = mk_j + 1\n      }\n      \n      # VARS\n      mk_n = length(used_data)\n      vars = (mk_n*(mk_n-1)*(2*mk_n+5))/18\n      \n      # ZMK\n      if(vars>0){\n        mk_z = (mk_s-1)/(sqrt(vars))\n      }else if(vars<0){\n        mk_z = (mk_s+1)/(sqrt(vars))\n      }else if(vars==0){\n        mk_z = 0\n      }\n      \n      # Pval\n      mk[type_w+1,file_i] = 2*(1-pnorm(abs(mk_z)))\n    }\n    type_w = type_w + 1\n  }\n  colnames(time_rec)[file_i] = stations[match(\n    as.numeric(substr(file[file_i],16,21)),stations[,2]),2]\n  colnames(mk)[file_i] = stations[match(\n    as.numeric(substr(file[file_i],16,21)),stations[,2]),2]\n  colnames(d_sign)[file_i] = stations[match(\n    as.numeric(substr(file[file_i],16,21)),stations[,2]),2]\n  # colnames(sigval)[file_i] = iata[match(substr(file[file_i],16,21),iata[,2]),1]\n  # colnames(sigval_type)[file_i] = iata[match(substr(file[file_i],16,21),iata[,2]),1]\n  # \n  type_w = 0\n  file_i = file_i + 1\n}\n\n# Transpose\nmk = t(mk)\ntime_rec = t(time_rec)\nd_sign = t(d_sign)\n\npv = 1\nwhile(pv<5){\n  indices[1:length(which((time_rec[,pv] > n[pv])&(mk[,pv]<0.0500001))),\n          pv] = as.numeric(\n            substr(file[which((time_rec[,pv] > n[pv])&(mk[,pv]<0.0500001))],\n                   16,21))\n\n  req_stations_sig = \n    which(stations_dup[,2] %in% indices[,pv])\n  req_stations_sig_s =\n    which(rownames(mk) %in% indices[,pv])\n  write_temp = stations_dup[req_stations_sig,]\n  write_temp = cbind(write_temp,\n                     d_sign[req_stations_sig_s,pv],\n                     time_rec[req_stations_sig_s,pv])\n  colnames(write_temp)[13:14] = c(\"sign\",\"time\")\n  write.csv(write_temp,\n    file = paste(texty[pv],mi,\".csv\",sep=\"\"))\n  pv = pv + 1\n}\n\nwrite_temp = stations_dup[req_stations_sig,]\nwrite_temp = cbind(write_temp,d_sign[req_stations_sig,pv],\n)\n\npv = 1\nwhile(pv<4){\n  paste(dir,\"Lower_48\",\n        paste(\"station_matrix_\",indices[i,pv],\".xlsx\",sep=\"\"),\n        sep=\"/\")\n}\n\npv = 1\nwhile(pv<4){\n  write.csv(as.numeric(\n    substr(file[which((time_rec[,pv] > 7)&(mk[,pv]<0.0500001))],\n           16,21)),\n    file = paste(\"\",texty[pv],\".csv\",sep=\"\"))\n  write.csv(d_sign[which((time_rec[,pv] > 7)&(mk[,pv]<0.0500001)),],\n            file = paste(\"s_\",texty[pv],\".csv\",sep=\"\"))\n  \n  pv = pv + 1\n}\n\n\npv = 1\nwhile(pv<5){\n  \n  pv_m = match(which(time_rec[,pv]>30),\n               which(mk[,pv]<0.0500001))\n  pv_m = pv_m[!is.na(pv_m)]\n  which(mk[,pv]<0.0500001)[pv_m]\n  pv_m\n}\n\nwhich(time_rec[,1]>30)[2]\nwhich(mk[,1]<0.0500001)[7]\n(time_rec>30)&(mk<0.0500001)\n\n\n\npv = 1\nsigval_pv = 1\n\nwhile(pv<9){\n  if(sigval_pv == 1){\n    indices = as.numeric(substr(\n      file[req_stations[which(sigval[pv,] < 0.050001)]],\n      16,21))\n    sig = \"sig\"\n  }else if(sigval_pv == 2){\n    indices = as.numeric(substr(\n      file[req_stations[which(sigval_type[pv,] < 0.050001)]],\n      16,21))\n    sig = \"sigtyp\"\n  }else if(sigval_pv == 3){\n\n    }\n  \n  stations_dup = stations[!duplicated(stations[,2]),]\n  \n  req_stations_sig = \n    which(stations_dup[,2] %in% indices)\n  # write.csv(stations_dup[req_stations_sig,],\n  #           file = paste(\"risk2\",sig,pv,\".csv\",sep=\"\"))\n  pv = pv + 1\n  if((pv == 9)&(sigval_pv == 1)){\n    sigval_pv = 2\n    pv = 1\n  }else if ((pv == 9)&(sigval_pv == 2)){\n    pv = 9\n  }\n}\n\n\n\n\n\n\npv = 1\nwhile(pv<5){\n  indices = as.numeric(substr(\n    file[req_stations[which(mk[pv,] < 0.050001)]],\n    16,21))\n\n  sig = \"mk\"\n  \n  stations_dup = stations[!duplicated(stations[,2]),]\n  \n  req_stations_sig = \n    which(stations_dup[,2] %in% indices)\n  # write.csv(stations_dup[req_stations_sig,],\n  #           file = paste(\"risk2\",sig,pv,\".csv\",sep=\"\"))\n  pv = pv + 1\n}\n\n\n\n\n\n\ncolnames(d_sign) <- as.numeric(substr(file[req_stations],16,21))\nwrite.csv(d_sign,\n          file = \"risk2_50mi_sign.csv\")\n\n\nstations[which(unique(stations[,2]) %in% indices),2] %in% indices\n\nwhich(sigval_type[2,] < 0.051)\n\n",
    "created" : 1508489420634.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3663052433",
    "id" : "6ABED60",
    "lastKnownWriteTime" : 1503209513,
    "last_content_update" : 1503209513,
    "path" : "C:/Users/Jai/Box Sync/Wind/eastern_seaboard_n (Jai.Chung@tufts.edu 2).R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 13,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}