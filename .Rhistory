t2[k,1] = l2/l1
t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])
if(n == 1){
year[k] = lcv
}else if(n == 2){
year[k] = lsk
}else{
year[k] = lku
}
}
k = k + 1
}
}
used_data = year_a[,1]
used_date = year_a[,2]
}
# Obtain desired data
super_data[,2*(f-1)+1] =
ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),
year_a[,1],NA)
f = f + 1
}
# Obtain max from every row, ignoring NA
cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)
if(dim(super_data)[2]<3){
super_data = super_data[which(!is.na(super_data[,1])),]
yearr_t = super_data[,2]
used_data = super_data[,1]
}else{
# Superstation values
used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)
yearr_t = min(yearr[,1]):max(yearr[,2])
}
# Remove data points that are NA (not recorded)
# by indexing together
t = yearr_t
t[which(is.na(used_data))] = NA
used_data <- used_data[!is.na(used_data)]
t <- t[!is.na(t)]
# Significance
# Linear Regression Model
sta.lm = lm(log(used_data)~t)
# P-Values for Intercept and Trend Coefficients
t1 = sta.lm$coefficients[2]/
summary(sta.lm)$coefficients[2,2]
alp = pt(t1,df = length(used_data)-2, lower.tail = F)
t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) -
1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))
# Log transformation and linear regression
ln_used_data = log(used_data)
lm.year = lm(ln_used_data~t)
m.year = lm(used_data~t)
sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t
rho = cor(ln_used_data,t)
# Conditional Moments of y
mu_y.w = lm.year$coefficients[1] +
lm.year$coefficients[2]*t
vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)
sk_y.w = skewness(ln_used_data) -
skewness(t)*(lm.year$coefficients[2]^3)
# Conditional Moments of x
mu_x.w = exp(mu_y.w + (vr_y.w)/2)
vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)
cv_x.w = sqrt(exp(vr_y.w)-1)
sk_x.w = 3*cv_x.w + cv_x.w^3
# Stationary
mean.sta = mean(ln_used_data)
sd.sta = sqrt(sum((ln_used_data -
mean(ln_used_data))^2)/
length(ln_used_data))
YR = c(10,20,30)
LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))
t_i = 1
{
# PPCC
b_coef = cor(ln_used_data, t)*sd(ln_used_data)/sd(t)
mu_y.pp = mean.sta + b_coef*(t-mean(t))
sd_y.pp = sqrt(var(ln_used_data)*(1-cor(ln_used_data, t)^2))
z_i.s = sort((ln_used_data - mean.sta)/sd.sta)
z_i.n = sort((ln_used_data - mu_y.pp)/sd_y.pp)
rank_i.s = rank(z_i.s)
rank_i.n = rank(z_i.n)
p_pos.s = (rank_i.s - 0.375)/(length(ln_used_data) + 0.25)
p_pos.n = (rank_i.n - 0.375)/(length(ln_used_data) + 0.25)
z_ii.s = qnorm(p_pos.s)
z_ii.n = qnorm(p_pos.n)
corst = round(cor(z_ii.s,z_i.s),4)
corns = round(cor(z_ii.n,z_i.n),4)
}
cl = cl + 1
}
}
# Call packages that will be used
{
library(xlsx)
library(plyr)
library(moments)
library(Hmisc)
library(rworldmap)
library(cluster)
library(stringr)
library(lubridate)
library(Lmoments)
library(gamlss)
library(extRemes)
library(fitdistrplus)
library(evd)
library(nleqslv)
library(XLConnect)
library(openxlsx)
library(trend)
library(Kendall)
library(lmomRFA)
add_legend <- function(...) {
opar <- par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0),
mar=c(0, 0, 0, 0), new=TRUE)
on.exit(par(opar))
plot(0, 0, type='n', bty='n', xaxt='n', yaxt='n')
legend(...)
}
rep.row<-function(x,n){
matrix(rep(x,each=n),nrow=n)
}
pop.var <- function(x) var(x) * (length(x)-1) / length(x)
pop.sd <- function(x) sqrt(pop.var(x))
texty = c("comm","nthu","thun","trop")
colors = c("#e41a1c", "#377eb8", "#4daf4a",
"#984ea3", "#ff7f00", "#a65628",
"#f781bf", "#999999")
}
load("C:/Users/Jai/Box Sync/Wind/Florida/temp.RData")
clu
for (clu in clu.f:(clu.f+1)){
clusters = pam(kdata.sca[,c(2,3,7)], clu)
# Plotting
cl = 1
while(cl<length(unique(clusters$clustering))+1){
cl.index = which(as.numeric(clusters$clustering)==cl)
file = paste0("station_matrix_",
kdata.sca[cl.index,1],
"_update.xlsx")
dm.table = cbind(kdata[cl.index,1],
k_n[cl.index],
cov[cl.index,3],
lmom[cl.index,8:11])
colnames(dm.table)[1:3] = c("id", "n", "mean")
dm.test = regtst(dm.table[,1:6],5000)
is.dm.ex =
length(which(as.numeric(dm.test$D)>dm.test$Dcrit[1]))
n.good.fit = length(which(abs(dm.test$Z)<1.64))
# Check if there are discordant stations
if(is.dm.ex>0){
file_r = paste0("station_matrix_",
dm.sta,
"_update.xlsx")
file = file[which(!(file %in% file_r))]
}
yearr = as.data.frame(
matrix(, nrow = , ncol = 2))
f = 1
## Identify beginning and ending years
while(f<length(file)+1){
data_g = read.xlsx(paste(dir,"/Lower_48/",file[f],sep=""))
# Delete unnecessary rows until first data
data_gg = data_g[(grep("^Date",
data_g[,1])+1):length(data_g[,1]),]
# assign data_gg column names as data_g's row 6
# because it has column names aligned correctly
colnames(data_gg) <- data_g[6,]
# Date/Time Format originally MM/DD/YYYY HH:MM
# Distorted to Excel Serial Date (start from 1900-01-01)
# numbers while loading. Hence, these are
# converted to YYYY-MM-DD HH:MM format
data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600
+ as.POSIXct("1899-12-30 00:00"))
# Convert strings into numbers
data_gg[,2] = as.numeric(data_gg[,2])
data_gg[,3] = as.numeric(data_gg[,3])
# Read storm type
# Read storm type
if(pv==2){
# Non-thunderstorm
data_gg <- data_gg[-c(which(data_gg[,5]!="1")),]
}
if(pv==3){
# Thunderstorm
data_gg <- data_gg[-c(which(data_gg[,7]!="1")),]
}
if(pv==4){
# Tropical Storm
data_gg <- data_gg[-c(which(data_gg[,10]!="1")),]
}
# Obtain minimum and maximum years of each station
yearr[f,1] = min(unique(year(data_gg[,1])))
yearr[f,2] = max(unique(year(data_gg[,1])))
f = f + 1
}
super_data = as.data.frame(
matrix(, nrow = length(min(yearr[,1]):max(yearr[,2])),
ncol = 2*length(file)))
super_data[,seq(2,length(file)*2,2)] =
min(yearr[,1]):max(yearr[,2])
f = 1
# Place max accordingly
while(f<length(file)+1){
data_g = read.xlsx(paste(dir,"/Lower_48/",file[f],sep=""))
# Delete unnecessary rows until first data
data_gg = data_g[(grep("^Date",
data_g[,1])+1):length(data_g[,1]),]
# assign data_gg column names as data_g's row 6
# because it has column names aligned correctly
colnames(data_gg) <- data_g[6,]
# Date/Time Format originally MM/DD/YYYY HH:MM
# Distorted to Excel Serial Date (start from 1900-01-01)
# numbers while loading. Hence, these are
# converted to YYYY-MM-DD HH:MM format
data_gg[,1] <-as.POSIXct(as.numeric(data_gg[,1])*24*3600
+ as.POSIXct("1899-12-30 00:00"))
# Convert strings into numbers
data_gg[,2] = as.numeric(data_gg[,2])
data_gg[,3] = as.numeric(data_gg[,3])
# Read storm type
if(pv==2){
# Non-thunderstorm
data_gg <- data_gg[-c(which(data_gg[,5]!="1")),]
}
if(pv==3){
# Thunderstorm
data_gg <- data_gg[-c(which(data_gg[,7]!="1")),]
}
if(pv==4){
# Tropical Storm
data_gg <- data_gg[-c(which(data_gg[,10]!="1")),]
}
# Annual
{
i = 1
j = 1
k = 1
# no_YEARS: how many data points are there?
no_YEARS = length(unique(year(data_gg[,1])))
year_a = as.data.frame(matrix(, nrow = no_YEARS, ncol = 2))
data_gg[,14] = year(data_gg[,1])
while(k<(no_YEARS)){
# Identify which data row is the last one
# from all of the year n
while(data_gg[i + j - 1,14] == data_gg[i + j,14]){
j = j + 1
}
j = i + j - 1
if(m == 1){
# Mean
year[k] = sum(data_gg[i:j,3])/length(i:j)
type = "Mean"
}else if(m == 2){
# Median
year[k] = median(data_gg[i:j,3])
type = "Median"
}else if(m == 3){
# Coefficient of Variation
year[k] = sd(data_gg[i:j,3])/
(sum(data_gg[i:j,3])/length(i:j))
type = "Coefficient of Variation"
}else if(m == 4){
# Maximum
year_a[k,1] = max(data_gg[i:j,3])
year_a[k,2] = as.character(data_gg[i,1])
year_a[k,3] = year(data_gg[i,1])
type = "Maximum"
}else if(m == 5){
# Frequency
year[k] = length(i:j)
type = "Frequency"
}else{
# L-Moments
L = Lmoments(data_gg[i:j,3], na.rm=TRUE)
lcv = L[2]/L[1] # L-cv
lsk = L[3]/L[2] # L-Skew
lku = L[4]/L[2] # L-Kurtosis
l1 = fit$mle[1]/(1+fit$mle[2])
l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))
t2[k,1] = l2/l1
t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])
if(n == 1){
year[k] = lcv
type = "L-CV"
}else if(n == 2){
year[k] = lsk
type = "L-Skewness"
}else{
year[k] = lku
type = "L-Kurtosis"
}
}
# i variable is added by 1
# to move on to the next year
i = j + 1
j = 1
k = k + 1
if(k == (no_YEARS)){
last = length(data_gg[,3])
if(m == 1){
year[k] = sum(data_gg[i:last,3])/length(i:last)
}else if(m == 2){
year[k] = median(data_gg[i:last,3])
}else if(m == 3){
year[k] = sd(data_gg[i:last,3])/
(sum(data_gg[i:last,3])/length(i:last))
}else if(m == 4){
year_a[k,1] = max(data_gg[i:last,3])
year_a[k,2] = as.character(data_gg[last,1])
year_a[k,3] = year(data_gg[last,1])
}else if(m == 5){
year[k] = length(i:last)
}else{
L = Lmoments(data_gg[i:last,3], na.rm=TRUE)
lcv = L[2]/L[1] # L-cv
lsk = L[3]/L[2] # L-Skew
lku = L[4]/L[2] # L-Kurtosis
l1 = fit$mle[1]/(1+fit$mle[2])
l2 = fit$mle[1]/((1+fit$mle[2])*(2+fit$mle[2]))
t2[k,1] = l2/l1
t3[k,1] = (1-fit$mle[2])/(3+fit$mle[2])
if(n == 1){
year[k] = lcv
}else if(n == 2){
year[k] = lsk
}else{
year[k] = lku
}
}
k = k + 1
}
}
used_data = year_a[,1]
used_date = year_a[,2]
}
# Obtain desired data
super_data[,2*(f-1)+1] =
ifelse(!is.na(match(super_data[,2*(f)],year_a[,3])),
year_a[,1],NA)
f = f + 1
}
# Obtain max from every row, ignoring NA
cus.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)
if(dim(super_data)[2]<3){
super_data = super_data[which(!is.na(super_data[,1])),]
yearr_t = super_data[,2]
used_data = super_data[,1]
}else{
# Superstation values
used_data = apply(super_data[,seq(1,length(file)*2-1,2)],1, cus.max)
yearr_t = min(yearr[,1]):max(yearr[,2])
}
# Remove data points that are NA (not recorded)
# by indexing together
t = yearr_t
t[which(is.na(used_data))] = NA
used_data <- used_data[!is.na(used_data)]
t <- t[!is.na(t)]
# Significance
# Linear Regression Model
sta.lm = lm(log(used_data)~t)
# P-Values for Intercept and Trend Coefficients
t1 = sta.lm$coefficients[2]/
summary(sta.lm)$coefficients[2,2]
alp = pt(t1,df = length(used_data)-2, lower.tail = F)
t2 = qt(1-alp,df = length(used_data)-2, lower.tail = T) -
1/sqrt((1/(cor(log(used_data),t)^2))-1)*sqrt(length(used_data))
# Log transformation and linear regression
ln_used_data = log(used_data)
lm.year = lm(ln_used_data~t)
m.year = lm(used_data~t)
sta.data = m.year$coefficients[1]+m.year$coefficients[2]*t
rho = cor(ln_used_data,t)
# Conditional Moments of y
mu_y.w = lm.year$coefficients[1] +
lm.year$coefficients[2]*t
vr_y.w = (sd(ln_used_data)^2)*(1-rho^2)
sk_y.w = skewness(ln_used_data) -
skewness(t)*(lm.year$coefficients[2]^3)
# Conditional Moments of x
mu_x.w = exp(mu_y.w + (vr_y.w)/2)
vr_x.w = exp(2*mu_y.w + vr_y.w)*(exp(vr_y.w)-1)
cv_x.w = sqrt(exp(vr_y.w)-1)
sk_x.w = 3*cv_x.w + cv_x.w^3
# Stationary
mean.sta = mean(ln_used_data)
sd.sta = sqrt(sum((ln_used_data -
mean(ln_used_data))^2)/
length(ln_used_data))
YR = c(10,20,30)
LLL = as.data.frame(matrix(, nrow = 4, ncol = 9))
t_i = 1
while(t_i<4){
t_non = c(tail(t,1),2020,2030)
# Nonstationary - Mean
mean.hom = unname(lm.year$coefficients[1] +
lm.year$coefficients[2]*t_non[t_i])
sd.hom = unname(sqrt(var(ln_used_data)-
lm.year$coefficients[2]^2*
var(t)))
# Nonstationary - Mean + Cv
res = (((lm.year$residuals)^(2))^(1/3))
lm.res = lm(res~t)
mean.het = mean.hom
sd.het = sqrt((lm.res$coefficients[1]+
lm.res$coefficients[2]*tail(t,1))^3 +
3*(summary(lm.res)$sigma^2)*
(lm.res$coefficients[1]+
lm.res$coefficients[2]*t_non[t_i]))
# Generate return period data points
rp = seq(1.01,1700, length.out = 10001)
if(dis=="ln2"){
ppp_sta = exp(mean.sta + qnorm(1-(1/rp))*sd.sta)
ppp_hom = exp(mean.hom + qnorm(1-(1/rp))*sd.hom)
ppp_het = exp(mean.het + qnorm(1-(1/rp))*sd.het)
}
if(dis=="gev"){
source(paste0(dir_fl,"gev.R"))
}
if(dis=="pe3"){
source(paste0(dir_fl,"pe3.R"))
}
if(dis=="gno"){
source(paste0(dir_fl,"gno.R"))
}
if(t_i==1){
png(filename=paste0(dir,"/plots/",texty[pv],
"/cLLL",clu,"_",cl,".png"),
w = 800, h = 600)
par(mfrow=c(1,2))
plot(t,ln_used_data, xlab = "Year",
ylab = "ln (Peak Wind Gust)",
xaxt = "n",
yaxt = "n",
cex.lab = 1.1)
abline(lm(ln_used_data~t))
axis(side=1,
at = c(1980,1990,2000,2010),
cex.axis = 1.5)
axis(side=2,
cex.axis = 1.5)
plot(t, res, xlab = "Year",
ylab = expression(epsilon^{2/3}),
xaxt = "n",
yaxt = "n",
cex.lab = 1.1)
abline(lm(res~t))
axis(side=1,
at = c(1980,1990,2000,2010),
cex.axis = 1.5)
axis(side=2,
cex.axis = 1.5)
dev.off()
par(mar = c(4,4,2,2))
png(filename=paste0(dir,"/plots/",texty[pv],
"/lll",clu,"_",cl,".png"),
w = 800, h = 600)
plot(rp,ppp_sta, log = "x", typ="n",
xlim = c(1,3500), ylim = c(40,150),
xlab = "Return Period",
ylab = "Peak wind gust (mph)",
xaxt = "n",
yaxt = "n",
cex.lab = 1.5)
axis(side=1, labels = T,
at=c(1, 10, 50, 100,300,700,1700),
cex.axis = 1.5)
axis(side=2, labels = T,
at=seq(40,240,10),
cex.axis = 1)
abline(v=c(1, 10, 50, 100, 300,700, 1700),
h=seq(40,240,10),
col="gray", lty=3)
legend("bottomright",
c("Stationary",
"Trend in Mean",
"Trend in Mean + CV"),
lty = c(1,2,3),
lwd = 2,
cex = 1.5)
lines(rp, ppp_sta, col = colors[cl], lwd = 2)
}
lines(rp, ppp_hom, col = colors[cl], lwd = 1+t_i, lty = 2)
lines(rp, ppp_het, col = colors[cl], lwd = 1+t_i, lty = 3)
text(x= 1700, y= max(ppp_hom), cex = 1,
pos = 4, labels = paste(t_non[t_i], "HOM"))
text(x= 1700, y= max(ppp_het), cex = 1,
pos = 4, labels = paste(t_non[t_i], "HET"))
LLL[1,t_i] = ppp_sta[which.min(abs(rp-100))]
LLL[2,t_i] = ppp_sta[which.min(abs(rp-300))]
LLL[3,t_i] = ppp_sta[which.min(abs(rp-700))]
LLL[4,t_i] = ppp_sta[which.min(abs(rp-1700))]
LLL[1,t_i + 3] = ppp_hom[which.min(abs(rp-100))]
LLL[2,t_i + 3] = ppp_hom[which.min(abs(rp-300))]
LLL[3,t_i + 3] = ppp_hom[which.min(abs(rp-700))]
LLL[4,t_i + 3] = ppp_hom[which.min(abs(rp-1700))]
LLL[1,t_i + 6] = ppp_het[which.min(abs(rp-100))]
LLL[2,t_i + 6] = ppp_het[which.min(abs(rp-300))]
LLL[3,t_i + 6] = ppp_het[which.min(abs(rp-700))]
LLL[4,t_i + 6] = ppp_het[which.min(abs(rp-1700))]
if(t_i==3){
assign(paste("LLL",clu,cl,YR[t_i], sep = "_"),
LLL)
}
t_i = t_i + 1
}
dev.off()
cl = cl + 1
}
}
